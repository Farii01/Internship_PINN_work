{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farii01/Internship_PINN_work/blob/main/Updated_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyE0odYT4N5r",
        "outputId": "2b28ffdd-fb83-480b-a3ab-d5491e1913f3"
      },
      "id": "WyE0odYT4N5r",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git\n",
            "  Cloning https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git to /tmp/pip-req-build-jrpxbtl4\n",
            "  Running command git clone --filter=blob:none --quiet https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git /tmp/pip-req-build-jrpxbtl4\n",
            "  Resolved https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git to commit 87d5eab82dbae0c55a8d93113cd4ac6db38a0bf0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (0.24.1)\n",
            "Requirement already satisfied: cdflib in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (1.3.4)\n",
            "Requirement already satisfied: cmocean in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (3.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (2.1.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy->pygeodyntools==1.0.0) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pygeodyntools==1.0.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pygeotools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy"
      ],
      "metadata": {
        "id": "DrYzazJN4Pn6"
      },
      "id": "DrYzazJN4Pn6",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/COVOBS-x2_400reals.hdf5\"\n",
        "\n",
        "pygeo = pygeotools.pygeotools()\n",
        "model_name=  \"COVOBS-x2_400reals.hdf5\"\n",
        "pygeo.loadModel(\n",
        "    modelName=\"COVOBS-x2_400reals.hdf5\",\n",
        "    modelType=\"covobs_hdf5\",\n",
        "    modelPath=model_path\n",
        ")\n",
        "\n",
        "pygeo.isLoaded(\"COVOBS-x2_400reals.hdf5\")  # Should return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53TdxT8P4SBC",
        "outputId": "2fd8657e-c617-4c54-823e-e4601fc6c55f"
      },
      "id": "53TdxT8P4SBC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygeotools was initialized with `verbose=True`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6397c23f",
      "metadata": {
        "id": "6397c23f"
      },
      "source": [
        "### Retrieving the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c703ff4",
      "metadata": {
        "id": "9c703ff4"
      },
      "outputs": [],
      "source": [
        "# Setting the grid\n",
        "pygeo.setGrid(\"1deg\")\n",
        "\n",
        "# Creating the context\n",
        "context = {\n",
        "    \"lmax\": 13,\n",
        "    \"r\": pygeo.constants[\"rCore\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbefdba0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbefdba0",
        "outputId": "9969f005-8e56-40bf-8381-2a83e3299515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "# Computing the MF and SV\n",
        "MF = pygeo.addMeasure(\"COVOBS-x2_400reals.hdf5\", \"MF\", context)\n",
        "SV = pygeo.addMeasure(\"COVOBS-x2_400reals.hdf5\", \"SV\", context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ec93eb1b",
      "metadata": {
        "id": "ec93eb1b"
      },
      "outputs": [],
      "source": [
        "# Retrieving the grid\n",
        "_, (thetas, phis) = pygeo.getCurrentGrid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54866c66",
      "metadata": {
        "id": "54866c66"
      },
      "source": [
        "## Generating the patches for input in PINN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we cut the Earth into small square patches, train a separate neural network on each patch, then stitch the results together.\n"
      ],
      "metadata": {
        "id": "_EhA1-3KFOW_"
      },
      "id": "_EhA1-3KFOW_"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a797e596",
      "metadata": {
        "id": "a797e596"
      },
      "outputs": [],
      "source": [
        "def generate_patches(theta_size=20, phi_size=20, overlap=5):# 20 rows longitude, 20 cols latitude\n",
        "    patches = []\n",
        "# without overlap\n",
        "    dtheta = theta_size - overlap\n",
        "    dphi = phi_size - overlap\n",
        "\n",
        "#going from pole to pole (0 to 180 in colatitude) by stepping in 15° chunks (because dtheta = 15)\n",
        "    for i1 in range(0, 180 - theta_size + 1, dtheta):\n",
        "        i2 = i1 + theta_size\n",
        "        for j1 in range(0, 360 - phi_size + 1, dphi):\n",
        "            j2 = j1 + phi_size\n",
        "\n",
        "            patches.append((i1, i2, j1, j2))\n",
        "\n",
        "    return patches\n",
        "'''\n",
        "This loop creates a full list of (i1, i2, j1, j2) — patches of size 20°×20°, sliding across the whole globe, overlapping 5°.\n",
        "\n",
        "Each patch will be used to train a small PINN, then all will be stitched together into one global map.\n",
        "'''\n",
        "# How to use?\n",
        "patches = generate_patches(theta_size=20, phi_size=20, overlap=5)\n",
        "\n",
        "for i1, i2, j1, j2 in patches:\n",
        "    ... # For each patch, we have to solve the inverse problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b6f158c",
      "metadata": {
        "id": "3b6f158c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d0b343df-6bd6-4b10-a58e-ff8d6de687b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#Take a small patch (20° × 20°)\\ni1, i2 = 60, 80\\nj1, j2 = 330, 350\\n\\n# Computing the derivative wrt theta and phi of the MF\\ndBrdth = numpy.gradient(Br, thetas, axis=0)[i1:i2,j1:j2,...]\\ndBrdph = numpy.gradient(Br, phis, axis=1)[i1:i2,j1:j2,...]\\n\\nBr = Br[i1:i2,j1:j2,...]\\ndBrdt = dBrdt[i1:i2,j1:j2,...]\\n\\n# Slicing the grid\\nthetas_bis = thetas[i1:i2]\\nphis_bis = phis[j1:j2]\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Selecting the data\n",
        "Br = pygeo.selectFromMeasure(\"COVOBS-x2_400reals.hdf5\", MF, options={\"component\": \"r\", \"time\": 2020})\n",
        "dBrdt = pygeo.selectFromMeasure(\"COVOBS-x2_400reals.hdf5\", SV, options={\"component\": \"r\", \"time\": 2020})\n",
        "'''\n",
        "#Take a small patch (20° × 20°)\n",
        "i1, i2 = 60, 80\n",
        "j1, j2 = 330, 350\n",
        "\n",
        "# Computing the derivative wrt theta and phi of the MF\n",
        "dBrdth = numpy.gradient(Br, thetas, axis=0)[i1:i2,j1:j2,...]\n",
        "dBrdph = numpy.gradient(Br, phis, axis=1)[i1:i2,j1:j2,...]\n",
        "\n",
        "Br = Br[i1:i2,j1:j2,...]\n",
        "dBrdt = dBrdt[i1:i2,j1:j2,...]\n",
        "\n",
        "# Slicing the grid\n",
        "thetas_bis = thetas[i1:i2]\n",
        "phis_bis = phis[j1:j2]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e84ad3",
      "metadata": {
        "id": "09e84ad3"
      },
      "source": [
        "### Creating the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d20a3a9e",
      "metadata": {
        "id": "d20a3a9e"
      },
      "outputs": [],
      "source": [
        "node_inputs = 2\n",
        "node_outputs = 2\n",
        "node_layer = 64\n",
        "hidden_layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "120de866",
      "metadata": {
        "id": "120de866"
      },
      "outputs": [],
      "source": [
        "#  Defining the NN\n",
        "# For now, it has one hidden layer with 32 nodes\n",
        "# The activation functions are TANH\n",
        "class CoreFlowPINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoreFlowPINN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(node_inputs, node_layer))\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(nn.Linear(node_layer, node_layer))\n",
        "            layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(node_layer, node_outputs))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9cc552a1",
      "metadata": {
        "id": "9cc552a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b4cd6c07-d204-4aba-f4aa-d9dc9737fed3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Shaping everything for the NN\\n\\n# Creating the grid\\nthetas_grid, phis_grid = numpy.meshgrid(thetas_bis, phis_bis, indexing=\"ij\")\\n\\n# Creating the (flatten) grid points\\nthetas_flatten = thetas_grid.flatten()\\nphis_flatten = phis_grid.flatten()\\n\\n# Creating grid tensors to feed the NN\\nthetas_nn = torch.tensor(thetas_flatten[:, None], dtype=torch.float32, requires_grad=True)\\nphis_nn = torch.tensor(phis_flatten[:, None], dtype=torch.float32, requires_grad=True)\\n\\n# Creating tensors for the MF and SV\\nBr_nn = torch.tensor(Br.flatten()[:, None], dtype=torch.float32)\\ndBrdt_nn = torch.tensor(dBrdt.flatten()[:, None], dtype=torch.float32)\\ndBrdth_nn = torch.tensor(dBrdth.flatten()[:, None], dtype=torch.float32)\\ndBrdph_nn = torch.tensor(dBrdph.flatten()[:, None], dtype=torch.float32)\\n\\n# Ravel the angles\\ninputs = torch.cat([thetas_nn, phis_nn], dim=1)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''\n",
        "# Shaping everything for the NN\n",
        "\n",
        "# Creating the grid\n",
        "thetas_grid, phis_grid = numpy.meshgrid(thetas_bis, phis_bis, indexing=\"ij\")\n",
        "\n",
        "# Creating the (flatten) grid points\n",
        "thetas_flatten = thetas_grid.flatten()\n",
        "phis_flatten = phis_grid.flatten()\n",
        "\n",
        "# Creating grid tensors to feed the NN\n",
        "thetas_nn = torch.tensor(thetas_flatten[:, None], dtype=torch.float32, requires_grad=True)\n",
        "phis_nn = torch.tensor(phis_flatten[:, None], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# Creating tensors for the MF and SV\n",
        "Br_nn = torch.tensor(Br.flatten()[:, None], dtype=torch.float32)\n",
        "dBrdt_nn = torch.tensor(dBrdt.flatten()[:, None], dtype=torch.float32)\n",
        "dBrdth_nn = torch.tensor(dBrdth.flatten()[:, None], dtype=torch.float32)\n",
        "dBrdph_nn = torch.tensor(dBrdph.flatten()[:, None], dtype=torch.float32)\n",
        "\n",
        "# Ravel the angles\n",
        "inputs = torch.cat([thetas_nn, phis_nn], dim=1)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of patches: {len(patches)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONM5SM_4ct37",
        "outputId": "d5afb5f0-09dc-465f-beeb-d7b041765e0a"
      },
      "id": "ONM5SM_4ct37",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a604454",
      "metadata": {
        "id": "5a604454"
      },
      "source": [
        "### Adding the physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7189ba3d",
      "metadata": {
        "id": "7189ba3d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We want to solve the radial induction equation at the CMB\n",
        "    dBr / dt + ∇h • (Uh Br) = 0\n",
        "\n",
        "with some quasi-geostrophic condition (meaning the flow will partly align along the axis of rotation, making columns-like flow)\n",
        "    ∇h • (Uh cos(θ)) = 0\n",
        "\n",
        "As the flow is incompressible, it is subject to ∇ • U = 0. As a result, it admits a unique toroidal-poloidal decomposition:\n",
        "    Uh = ∇ x (r T) + ∇ (r S)\n",
        "\n",
        "Thus, instead of directly predicting uθ and uφ, it appears clever to predict T and S as it already enforces the incompressibility condition.\n",
        "\n",
        "In spherical coordinates, one has\n",
        "    uθ = -(dT/dφ) / sin(θ) + dS/dθ\n",
        "    uφ = dT/dθ + (dS/dφ) / sin(θ)\n",
        "\"\"\"\n",
        "\n",
        "r = torch.tensor(pygeo.constants[\"rCore\"]) # placing ourselves at the CMB\n",
        "\n",
        "def compute_loss(model, inputs, thetas_nn, phis_nn, Br_nn, dBrdt_nn, dBrdth_nn, dBrdph_nn):\n",
        "\n",
        "    # Retrieving the predicted flow\n",
        "    u_pred = model(inputs)\n",
        "\n",
        "    # Retrieving the toroidal and poloidal components\n",
        "\n",
        "    T = u_pred[:, 0:1]\n",
        "    S = u_pred[:, 1:2]\n",
        "\n",
        "    # First derivatives of T and S\n",
        "    dT_dth = torch.autograd.grad(T, thetas_nn, grad_outputs=torch.ones_like(T), create_graph=True, retain_graph=True)[0]\n",
        "    dT_dph = torch.autograd.grad(T, phis_nn, grad_outputs=torch.ones_like(T), create_graph=True, retain_graph=True)[0]\n",
        "    dS_dth = torch.autograd.grad(S, thetas_nn, grad_outputs=torch.ones_like(S), create_graph=True, retain_graph=True)[0]\n",
        "    dS_dph = torch.autograd.grad(S, phis_nn, grad_outputs=torch.ones_like(S), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L1\n",
        "    \"\"\"\n",
        "    # Computing the L1² loss function\n",
        "    # L1² = || dBr / dt + ∇h • (Uh Br) ||²\n",
        "    # ∇h • (Uh Br) = (∇h • Uh) Br + Uh • (∇h Br)\n",
        "\n",
        "    sin_th = torch.sin(thetas_nn)\n",
        "    cos_th = torch.cos(thetas_nn)\n",
        "    tan_th = torch.tan(thetas_nn)\n",
        "\n",
        "    # We are defining u_th and u_ph with T and S\n",
        "    u_th = -dT_dph / sin_th + dS_dth\n",
        "    u_ph = dT_dth + dS_dph / sin_th\n",
        "\n",
        "    # Computing ∇h • Uh\n",
        "    u_th_sin_th = u_th * sin_th\n",
        "    d_u_th_sin_th_dth = torch.autograd.grad(u_th_sin_th, thetas_nn, grad_outputs=torch.ones_like(u_th_sin_th), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    d_u_ph_dph = torch.autograd.grad(u_ph, phis_nn, grad_outputs=torch.ones_like(u_ph), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    divH_uH = (1 / (r * sin_th)) * (d_u_th_sin_th_dth + d_u_ph_dph)\n",
        "    # divH_uH = u_th * tan_th / r\n",
        "\n",
        "    # Computing ∇h Br\n",
        "    # The derivatives are provided as they are not the NN variables but inputs\n",
        "    gradH_Br_th = (1 / r) * dBrdth_nn\n",
        "    gradH_Br_ph = (1 / (r * sin_th)) * dBrdph_nn\n",
        "\n",
        "    # Wrapping the induction equation\n",
        "    L1 = dBrdt_nn + Br_nn * divH_uH + u_th * gradH_Br_th + u_ph * gradH_Br_ph\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L2\n",
        "    \"\"\"\n",
        "    L2 = divH_uH - u_th * tan_th / r\n",
        "\n",
        "    return L1, L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42f4fa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42f4fa3",
        "outputId": "d2d7916c-5f64-422e-c880-10ca8eea41f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 1.327360e+06 | L2 Loss: 1.058399e-10 | Total Loss: 1.327360e+06\n",
            "Epoch  500 | L1 Loss: 6.835668e+02 | L2 Loss: 1.178818e-05 | Total Loss: 6.835786e+02\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 1.289237e+06 | L2 Loss: 2.340753e-10 | Total Loss: 1.289237e+06\n",
            "Epoch  500 | L1 Loss: 7.785200e+02 | L2 Loss: 1.123165e-05 | Total Loss: 7.785312e+02\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 1.288331e+06 | L2 Loss: 5.249894e-10 | Total Loss: 1.288331e+06\n",
            "Epoch  500 | L1 Loss: 5.944352e+02 | L2 Loss: 8.806851e-06 | Total Loss: 5.944440e+02\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 1.407890e+06 | L2 Loss: 1.438577e-09 | Total Loss: 1.407890e+06\n",
            "Epoch  500 | L1 Loss: 6.158219e+02 | L2 Loss: 1.022188e-05 | Total Loss: 6.158321e+02\n",
            "\n",
            "=== Training run 5/5 ===\n",
            "Epoch    0 | L1 Loss: 1.329882e+06 | L2 Loss: 1.014578e-11 | Total Loss: 1.329882e+06\n",
            "Epoch  500 | L1 Loss: 6.654930e+02 | L2 Loss: 7.135322e-06 | Total Loss: 6.655001e+02\n",
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 1.218409e+07 | L2 Loss: 4.351480e-11 | Total Loss: 1.218409e+07\n",
            "Epoch  500 | L1 Loss: 2.267131e+04 | L2 Loss: 2.952807e-05 | Total Loss: 2.267134e+04\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 1.282652e+07 | L2 Loss: 4.574293e-10 | Total Loss: 1.282652e+07\n",
            "Epoch  500 | L1 Loss: 2.373554e+04 | L2 Loss: 2.984704e-05 | Total Loss: 2.373556e+04\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 1.177047e+07 | L2 Loss: 8.987376e-10 | Total Loss: 1.177047e+07\n",
            "Epoch  500 | L1 Loss: 2.010453e+04 | L2 Loss: 2.973879e-05 | Total Loss: 2.010456e+04\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 1.213483e+07 | L2 Loss: 4.136536e-11 | Total Loss: 1.213483e+07\n",
            "Epoch  500 | L1 Loss: 2.221134e+04 | L2 Loss: 3.008965e-05 | Total Loss: 2.221137e+04\n",
            "\n",
            "=== Training run 5/5 ===\n",
            "Epoch    0 | L1 Loss: 1.297643e+07 | L2 Loss: 2.373843e-10 | Total Loss: 1.297643e+07\n",
            "Epoch  500 | L1 Loss: 2.199261e+04 | L2 Loss: 2.959162e-05 | Total Loss: 2.199264e+04\n",
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 1.635954e+07 | L2 Loss: 2.325752e-09 | Total Loss: 1.635954e+07\n",
            "Epoch  500 | L1 Loss: 3.329657e+04 | L2 Loss: 2.584204e-05 | Total Loss: 3.329660e+04\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 1.661941e+07 | L2 Loss: 3.787718e-09 | Total Loss: 1.661941e+07\n",
            "Epoch  500 | L1 Loss: 3.713870e+04 | L2 Loss: 1.930167e-05 | Total Loss: 3.713872e+04\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 1.647427e+07 | L2 Loss: 1.944889e-10 | Total Loss: 1.647427e+07\n",
            "Epoch  500 | L1 Loss: 4.899966e+04 | L2 Loss: 2.105351e-05 | Total Loss: 4.899968e+04\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 1.615075e+07 | L2 Loss: 7.059610e-11 | Total Loss: 1.615075e+07\n",
            "Epoch  500 | L1 Loss: 1.465971e+04 | L2 Loss: 2.495100e-05 | Total Loss: 1.465974e+04\n",
            "\n",
            "=== Training run 5/5 ===\n",
            "Epoch    0 | L1 Loss: 1.665976e+07 | L2 Loss: 4.580887e-11 | Total Loss: 1.665976e+07\n",
            "Epoch  500 | L1 Loss: 1.274151e+04 | L2 Loss: 1.570757e-05 | Total Loss: 1.274153e+04\n",
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 3.239654e+07 | L2 Loss: 5.847326e-10 | Total Loss: 3.239654e+07\n",
            "Epoch  500 | L1 Loss: 2.651853e+04 | L2 Loss: 4.958272e-05 | Total Loss: 2.651858e+04\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 3.289240e+07 | L2 Loss: 1.249914e-09 | Total Loss: 3.289240e+07\n",
            "Epoch  500 | L1 Loss: 2.693542e+04 | L2 Loss: 4.889012e-05 | Total Loss: 2.693547e+04\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 3.256856e+07 | L2 Loss: 5.337244e-10 | Total Loss: 3.256856e+07\n",
            "Epoch  500 | L1 Loss: 2.596431e+04 | L2 Loss: 4.983062e-05 | Total Loss: 2.596437e+04\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 3.201249e+07 | L2 Loss: 4.812192e-09 | Total Loss: 3.201249e+07\n",
            "Epoch  500 | L1 Loss: 2.648182e+04 | L2 Loss: 4.634105e-05 | Total Loss: 2.648187e+04\n",
            "\n",
            "=== Training run 5/5 ===\n",
            "Epoch    0 | L1 Loss: 3.226088e+07 | L2 Loss: 2.024717e-10 | Total Loss: 3.226088e+07\n",
            "Epoch  500 | L1 Loss: 2.749149e+04 | L2 Loss: 4.967182e-05 | Total Loss: 2.749154e+04\n",
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 7.914834e+07 | L2 Loss: 1.736709e-09 | Total Loss: 7.914834e+07\n",
            "Epoch  500 | L1 Loss: 5.777747e+04 | L2 Loss: 5.933784e-05 | Total Loss: 5.777753e+04\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 7.926657e+07 | L2 Loss: 6.259354e-10 | Total Loss: 7.926657e+07\n",
            "Epoch  500 | L1 Loss: 6.035363e+04 | L2 Loss: 5.417936e-05 | Total Loss: 6.035368e+04\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 7.923210e+07 | L2 Loss: 8.098803e-09 | Total Loss: 7.923210e+07\n",
            "Epoch  500 | L1 Loss: 7.551125e+04 | L2 Loss: 5.660254e-05 | Total Loss: 7.551130e+04\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 7.801089e+07 | L2 Loss: 1.245552e-09 | Total Loss: 7.801089e+07\n",
            "Epoch  500 | L1 Loss: 7.251328e+04 | L2 Loss: 5.866620e-05 | Total Loss: 7.251334e+04\n",
            "\n",
            "=== Training run 5/5 ===\n",
            "Epoch    0 | L1 Loss: 7.750877e+07 | L2 Loss: 7.255833e-12 | Total Loss: 7.750877e+07\n",
            "Epoch  500 | L1 Loss: 6.250769e+04 | L2 Loss: 5.395737e-05 | Total Loss: 6.250775e+04\n",
            "\n",
            "=== Training run 1/5 ===\n",
            "Epoch    0 | L1 Loss: 4.266292e+07 | L2 Loss: 9.898214e-09 | Total Loss: 4.266292e+07\n",
            "Epoch  500 | L1 Loss: 1.099951e+05 | L2 Loss: 7.765327e-05 | Total Loss: 1.099952e+05\n",
            "\n",
            "=== Training run 2/5 ===\n",
            "Epoch    0 | L1 Loss: 4.214890e+07 | L2 Loss: 7.814125e-10 | Total Loss: 4.214890e+07\n",
            "Epoch  500 | L1 Loss: 7.697085e+04 | L2 Loss: 7.691654e-05 | Total Loss: 7.697093e+04\n",
            "\n",
            "=== Training run 3/5 ===\n",
            "Epoch    0 | L1 Loss: 4.269963e+07 | L2 Loss: 1.445755e-09 | Total Loss: 4.269963e+07\n",
            "Epoch  500 | L1 Loss: 8.032765e+04 | L2 Loss: 7.836052e-05 | Total Loss: 8.032773e+04\n",
            "\n",
            "=== Training run 4/5 ===\n",
            "Epoch    0 | L1 Loss: 4.257558e+07 | L2 Loss: 6.635115e-09 | Total Loss: 4.257558e+07\n",
            "Epoch  500 | L1 Loss: 9.538907e+04 | L2 Loss: 7.852007e-05 | Total Loss: 9.538915e+04\n"
          ]
        }
      ],
      "source": [
        "# I moved everything to the training loop so that for each patch in every iteration, the model receives the correct,\n",
        "# patch-specific input like thetas_nn, phis_nn, Br_nn, dBrdt_nn, dBrdth_nn, dBrdph_nn\n",
        "\n",
        "# Training over all patches\n",
        "num_realizations = 5 # 5 time on each patch\n",
        "epochs = 1000\n",
        "λ = 1000\n",
        "\n",
        "loss_grids = []\n",
        "patch_centers = []\n",
        "\n",
        "for i1, i2, j1, j2 in patches:\n",
        "    if i1 < 10 or i2 > 170 or (i1 <= 90 <= i2):\n",
        "        continue  # skip poles and equator\n",
        "\n",
        "    Br_patch = Br[i1:i2, j1:j2, ...]\n",
        "    dBrdt_patch = dBrdt[i1:i2, j1:j2, ...]\n",
        "    dBrdth = numpy.gradient(Br, thetas, axis=0)[i1:i2, j1:j2, ...]\n",
        "    dBrdph = numpy.gradient(Br, phis, axis=1)[i1:i2, j1:j2, ...]\n",
        "\n",
        "    thetas_bis = thetas[i1:i2]\n",
        "    phis_bis = phis[j1:j2]\n",
        "\n",
        "    thetas_grid, phis_grid = numpy.meshgrid(thetas_bis, phis_bis, indexing=\"ij\")\n",
        "    thetas_flat = thetas_grid.flatten()\n",
        "    phis_flat = phis_grid.flatten()\n",
        "\n",
        "    thetas_nn = torch.tensor(thetas_flat[:, None], dtype=torch.float32, requires_grad=True)\n",
        "    phis_nn = torch.tensor(phis_flat[:, None], dtype=torch.float32, requires_grad=True)\n",
        "    Br_nn = torch.tensor(Br_patch.flatten()[:, None], dtype=torch.float32)\n",
        "    dBrdt_nn = torch.tensor(dBrdt_patch.flatten()[:, None], dtype=torch.float32)\n",
        "    dBrdth_nn = torch.tensor(dBrdth.flatten()[:, None], dtype=torch.float32)\n",
        "    dBrdph_nn = torch.tensor(dBrdph.flatten()[:, None], dtype=torch.float32)\n",
        "\n",
        "    inputs = torch.cat([thetas_nn, phis_nn], dim=1)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for run in range(num_realizations):\n",
        "        print(f\"\\n=== Training run {run + 1}/{num_realizations} ===\")\n",
        "        model = CoreFlowPINN()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            L1, L2 = compute_loss(model, inputs, thetas_nn, phis_nn, Br_nn, dBrdt_nn, dBrdth_nn, dBrdph_nn)\n",
        "            L1_loss = (L1**2).mean()\n",
        "            L2_loss = (L2**2).mean()\n",
        "            Loss = L1_loss + λ * L2_loss\n",
        "            Loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if epoch % 500 == 0:\n",
        "                print(f\"Epoch {epoch:4d} | L1 Loss: {L1_loss.item():12.6e} | L2 Loss: {L2_loss.item():12.6e} | Total Loss: {Loss.item():12.6e}\", flush=True)\n",
        "'''\n",
        "\n",
        "Later for graphs\n",
        "                L1_final, _ = compute_loss(model, inputs, thetas_nn, phis_nn, Br_nn, dBrdt_nn, dBrdth_nn, dBrdph_nn)\n",
        "                L1_squared = (L1_final**2).detach().cpu().numpy().flatten()\n",
        "                loss_history.append(L1_squared)\n",
        "\n",
        "    mean_loss_map = numpy.mean(loss_history, axis=0)\n",
        "    loss_grid = mean_loss_map.reshape(len(thetas_bis), len(phis_bis))\n",
        "\n",
        "    loss_grids.append(loss_grid)\n",
        "    patch_centers.append(((i1 + i2) // 2, (j1 + j2) // 2))\n",
        "'''\n",
        "print(\" Done computing for all patches.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
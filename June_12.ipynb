{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farii01/Internship_PINN_work/blob/main/June_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "WyE0odYT4N5r",
      "metadata": {
        "id": "WyE0odYT4N5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c48c20-7595-4023-d09a-084533e45e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git\n",
            "  Cloning https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git to /tmp/pip-req-build-xw0ly_ia\n",
            "  Running command git clone --filter=blob:none --quiet https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git /tmp/pip-req-build-xw0ly_ia\n",
            "  Resolved https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git to commit 87d5eab82dbae0c55a8d93113cd4ac6db38a0bf0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (0.24.1)\n",
            "Requirement already satisfied: cdflib in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (1.3.4)\n",
            "Requirement already satisfied: cmocean in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (3.14.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.11/dist-packages (from pygeodyntools==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (2.1.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy->pygeodyntools==1.0.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pygeodyntools==1.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy->pygeodyntools==1.0.0) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->pygeodyntools==1.0.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://gricad-gitlab.univ-grenoble-alpes.fr/claveaur/pygeotools.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "DrYzazJN4Pn6",
      "metadata": {
        "id": "DrYzazJN4Pn6"
      },
      "outputs": [],
      "source": [
        "import pygeotools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53TdxT8P4SBC",
      "metadata": {
        "id": "53TdxT8P4SBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089bfd61-1b34-4ed6-c710-c71bbb52dc77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygeotools was initialized with `verbose=True`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model_path = \"/content/COVOBS-x2_400reals.hdf5\"\n",
        "\n",
        "pygeo = pygeotools.pygeotools()\n",
        "model_name=  \"COVOBS-x2_400reals.hdf5\"\n",
        "pygeo.loadModel(\n",
        "    modelName=\"COVOBS\",\n",
        "    modelType=\"covobs_hdf5\",\n",
        "    modelPath=\"COVOBS-x2_400reals.hdf5\"\n",
        ")\n",
        "\n",
        "\n",
        "pygeo.isLoaded(\"COVOBS\")  # Should return True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6397c23f",
      "metadata": {
        "id": "6397c23f"
      },
      "source": [
        "### Retrieving the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c703ff4",
      "metadata": {
        "id": "9c703ff4"
      },
      "outputs": [],
      "source": [
        "# Setting the grid\n",
        "pygeo.setGrid(\"1deg\")\n",
        "\n",
        "# Creating the context\n",
        "context = {\n",
        "    \"lmax\": 13,\n",
        "    \"r\": pygeo.constants[\"rCore\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cbefdba0",
      "metadata": {
        "id": "cbefdba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad24d98-7d0e-4b74-9011-71f2b45aaffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "# Computing the MF and SV\n",
        "MF = pygeo.addMeasure(\"COVOBS\", \"MF\", context)\n",
        "SV = pygeo.addMeasure(\"COVOBS\", \"SV\", context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ec93eb1b",
      "metadata": {
        "id": "ec93eb1b"
      },
      "outputs": [],
      "source": [
        "# Retrieving the grid\n",
        "_, (thetas, phis) = pygeo.getCurrentGrid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54866c66",
      "metadata": {
        "id": "54866c66"
      },
      "source": [
        "## Generating the patches for input in PINN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_EhA1-3KFOW_",
      "metadata": {
        "id": "_EhA1-3KFOW_"
      },
      "source": [
        "we cut the Earth into small square patches, train a separate neural network on each patch, then stitch the results together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a797e596",
      "metadata": {
        "id": "a797e596"
      },
      "outputs": [],
      "source": [
        "def generate_patches(theta_size=20, phi_size=20, overlap=5):# 20 rows longitude, 20 cols latitude\n",
        "    patches = []\n",
        "# without overlap\n",
        "    dtheta = theta_size - overlap\n",
        "    dphi = phi_size - overlap\n",
        "\n",
        "#going from pole to pole (0 to 180 in colatitude) by stepping in 15° chunks (because dtheta = 15)\n",
        "    for i1 in range(0, 180 - theta_size + 1, dtheta):\n",
        "        i2 = i1 + theta_size\n",
        "        for j1 in range(0, 360 - phi_size + 1, dphi):\n",
        "            j2 = j1 + phi_size\n",
        "\n",
        "            patches.append((i1, i2, j1, j2))\n",
        "\n",
        "    return patches\n",
        "'''\n",
        "This loop creates a full list of (i1, i2, j1, j2) — patches of size 20°×20°, sliding across the whole globe, overlapping 5°.\n",
        "\n",
        "Each patch will be used to train a small PINN, then all will be stitched together into one global map.\n",
        "'''\n",
        "# How to use?\n",
        "patches = generate_patches(theta_size=20, phi_size=20, overlap=5)\n",
        "\n",
        "for i1, i2, j1, j2 in patches:\n",
        "    ... # For each patch, we have to solve the inverse problem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Br = pygeo.selectFromMeasure(\"COVOBS\", MF, options={\"component\": \"r\", \"time\": 2020})\n",
        "dBrdt = pygeo.selectFromMeasure(\"COVOBS\", SV, options={\"component\": \"r\", \"time\": 2020})"
      ],
      "metadata": {
        "id": "aFpBjR6Tt9ND"
      },
      "id": "aFpBjR6Tt9ND",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_yVqXaykwfGr"
      },
      "id": "_yVqXaykwfGr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The mean of Br across all realizations = Br_obs\n",
        "\n",
        "The standard deviation across realizations = σᵢ\n"
      ],
      "metadata": {
        "id": "JADwH8cLvrjP"
      },
      "id": "JADwH8cLvrjP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Br once for all patches\n",
        "Br_full = pygeo.selectFromMeasure(\n",
        "    modelName=\"COVOBS\",\n",
        "    measure=MF,\n",
        "    options={\n",
        "        \"components\": (0,),   # radial component Br\n",
        "        \"reals\": (0, 400)     # all 400 realizations\n",
        "    }\n",
        ")\n",
        "\n",
        "# Drop radius dimension (CMB)\n",
        "Br_all = Br_full[0, ...]  # shape: [180, 360, 400]\n",
        "\n",
        "# Loop over each patch\n",
        "for patch_idx, (i1, i2, j1, j2) in enumerate(patches, start=1):\n",
        "\n",
        "    if i1 < 10 or i2 > 170 or (i1 <= 90 <= i2):\n",
        "        continue  # skip equator and near-pole patches\n",
        "\n",
        "    if (i2 - i1) == 0 or (j2 - j1) == 0:\n",
        "        print(f\" Skipping empty patch: i1={i1}, i2={i2}, j1={j1}, j2={j2}\")\n",
        "        continue\n",
        "\n",
        "    if i2 > Br_all.shape[0] or j2 > Br_all.shape[1]:\n",
        "        print(f\" Skipping out-of-bound patch: i2={i2}, j2={j2}, Br_all shape={Br_all.shape}\")\n",
        "        continue\n",
        "\n",
        "    Br_patch_all = Br_all[i1:i2, j1:j2, :]  # shape: [ni, nj, 400]\n",
        "\n",
        "    if Br_patch_all.shape[0] == 0 or Br_patch_all.shape[1] == 0:\n",
        "        print(f\"Empty patch shape: {Br_patch_all.shape} for i=({i1},{i2}) j=({j1},{j2})\")\n",
        "        continue\n",
        "\n",
        "    # Mean and std across realizations\n",
        "    Br_obs_patch = torch.mean(torch.tensor(Br_patch_all), dim=-1, keepdim=True)\n",
        "    sigma_i_patch = torch.std(torch.tensor(Br_patch_all), dim=-1, keepdim=True)\n",
        "\n",
        "    # Flatten\n",
        "    Br_obs = Br_obs_patch.reshape(-1, 1)\n",
        "    sigma_i = sigma_i_patch.reshape(-1, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "1e3h7zbFwapL"
      },
      "id": "1e3h7zbFwapL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "09e84ad3",
      "metadata": {
        "id": "09e84ad3"
      },
      "source": [
        "### Creating the NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d20a3a9e",
      "metadata": {
        "id": "d20a3a9e"
      },
      "outputs": [],
      "source": [
        "node_inputs = 2\n",
        "node_outputs = 3\n",
        "\n",
        "node_layer = 64\n",
        "hidden_layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "120de866",
      "metadata": {
        "id": "120de866"
      },
      "outputs": [],
      "source": [
        "#  Defining the NN\n",
        "# For now, it has one hidden layer with 32 nodes\n",
        "# The activation functions are TANH\n",
        "class CoreFlowPINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoreFlowPINN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(node_inputs, node_layer))\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(nn.Linear(node_layer, node_layer))\n",
        "            layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(node_layer, node_outputs))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ONM5SM_4ct37",
      "metadata": {
        "id": "ONM5SM_4ct37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2842b0-0f3f-4404-de62-2fb0b1d1dc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 253\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of patches: {len(patches)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a604454",
      "metadata": {
        "id": "5a604454"
      },
      "source": [
        "### Adding the physics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7189ba3d",
      "metadata": {
        "id": "7189ba3d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We want to solve the radial induction equation at the CMB\n",
        "    dBr / dt + ∇h • (Uh Br) = 0\n",
        "\n",
        "with some quasi-geostrophic condition (meaning the flow will partly align along the axis of rotation, making columns-like flow)\n",
        "    ∇h • (Uh cos(θ)) = 0\n",
        "\n",
        "As the flow is incompressible, it is subject to ∇ • U = 0. As a result, it admits a unique toroidal-poloidal decomposition:\n",
        "    Uh = ∇ x (r T) + ∇ (r S)\n",
        "\n",
        "Thus, instead of directly predicting uθ and uφ, it appears clever to predict T and S as it already enforces the incompressibility condition.\n",
        "\n",
        "In spherical coordinates, one has\n",
        "    uθ = -(dT/dφ) / sin(θ) + dS/dθ\n",
        "    uφ = dT/dθ + (dS/dφ) / sin(θ)\n",
        "\"\"\"\n",
        "\n",
        "r = torch.tensor(pygeo.constants[\"rCore\"]) # placing ourselves at the CMB\n",
        "\n",
        "def compute_loss(model, inputs, thetas_nn, phis_nn, dBrdt_nn, Br_obs, sigma_i):\n",
        "\n",
        "\n",
        "\n",
        "    # Retrieving the predicted flow\n",
        "    u_pred = model(inputs)\n",
        "\n",
        "    # Retrieving the toroidal and poloidal components\n",
        "\n",
        "    T = u_pred[:, 0:1]\n",
        "    S = u_pred[:, 1:2]\n",
        "    Br_nn = u_pred[:, 2:3]\n",
        "\n",
        "\n",
        "    dBrdth_nn = torch.autograd.grad(Br_nn, thetas_nn, grad_outputs=torch.ones_like(Br_nn), create_graph=True)[0]\n",
        "    dBrdph_nn = torch.autograd.grad(Br_nn, phis_nn, grad_outputs=torch.ones_like(Br_nn), create_graph=True)[0]\n",
        "\n",
        "\n",
        "    # First derivatives of T and S\n",
        "    dT_dth = torch.autograd.grad(T, thetas_nn, grad_outputs=torch.ones_like(T), create_graph=True, retain_graph=True)[0]\n",
        "    dT_dph = torch.autograd.grad(T, phis_nn, grad_outputs=torch.ones_like(T), create_graph=True, retain_graph=True)[0]\n",
        "    dS_dth = torch.autograd.grad(S, thetas_nn, grad_outputs=torch.ones_like(S), create_graph=True, retain_graph=True)[0]\n",
        "    dS_dph = torch.autograd.grad(S, phis_nn, grad_outputs=torch.ones_like(S), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L1\n",
        "    \"\"\"\n",
        "    # Computing the L1² loss function\n",
        "    # L1² = || dBr / dt + ∇h • (Uh Br) ||²\n",
        "    # ∇h • (Uh Br) = (∇h • Uh) Br + Uh • (∇h Br)\n",
        "\n",
        "    sin_th = torch.sin(thetas_nn)\n",
        "    cos_th = torch.cos(thetas_nn)\n",
        "    tan_th = torch.tan(thetas_nn)\n",
        "\n",
        "    # We are defining u_th and u_ph with T and S\n",
        "    u_th = -dT_dph / sin_th + dS_dth\n",
        "    u_ph = dT_dth + dS_dph / sin_th\n",
        "\n",
        "    # Computing ∇h • Uh\n",
        "    u_th_sin_th = u_th * sin_th\n",
        "    d_u_th_sin_th_dth = torch.autograd.grad(u_th_sin_th, thetas_nn, grad_outputs=torch.ones_like(u_th_sin_th), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    d_u_ph_dph = torch.autograd.grad(u_ph, phis_nn, grad_outputs=torch.ones_like(u_ph), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    divH_uH = (1 / (r * sin_th)) * (d_u_th_sin_th_dth + d_u_ph_dph)\n",
        "    # divH_uH = u_th * tan_th / r\n",
        "\n",
        "    # Computing ∇h Br\n",
        "    # The derivatives are provided as they are not the NN variables but inputs\n",
        "    gradH_Br_th = (1 / r) * dBrdth_nn\n",
        "    gradH_Br_ph = (1 / (r * sin_th)) * dBrdph_nn\n",
        "\n",
        "    # Wrapping the induction equation\n",
        "    L1 = dBrdt_nn + Br_nn * divH_uH + u_th * gradH_Br_th + u_ph * gradH_Br_ph\n",
        "\n",
        "    L1_loss = (L1**2).mean()\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L2\n",
        "    \"\"\"\n",
        "    L2 = divH_uH - u_th * tan_th / r\n",
        "    L2_loss = (L2**2).mean()\n",
        "    L2_loss_λ = λ * L2_loss\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L3\n",
        "    \"\"\"\n",
        "    # L3 = (1/N) * Σ_i [ (Br_nn - Br_obs) / sigma_i ]²\n",
        "\n",
        "    Br_residual = Br_nn - Br_obs\n",
        "\n",
        "    L3 = torch.mean((Br_residual / sigma_i) ** 2)\n",
        "\n",
        "    L3_loss = (L3**2).mean()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computing L4 —  div(B) = 0\n",
        "    ∇h • Br = 1/(r sinθ) ∂(Br sinθ)/∂θ + 1/(r sinθ) ∂Br/∂φ\n",
        "    \"\"\"\n",
        "\n",
        "    Br_sin_th = Br_nn * sin_th\n",
        "\n",
        "    dBr_sin_th_dth = torch.autograd.grad(Br_sin_th, thetas_nn, grad_outputs=torch.ones_like(Br_sin_th), create_graph=True, retain_graph=True)[0]\n",
        "    dBr_dph = torch.autograd.grad(Br_nn, phis_nn, grad_outputs=torch.ones_like(Br_nn), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    div_B = (1 / (r * sin_th)) * (dBr_sin_th_dth + dBr_dph)\n",
        "    L4 = torch.mean(div_B ** 2)\n",
        "    μ = 0\n",
        "\n",
        "\n",
        "    L4_Loss= μ * L4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return L1_loss, L2_loss_λ, L3_loss, L4_Loss, u_th, u_ph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Br_obs.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLDGDZj0xN_D",
        "outputId": "5c0ce746-c89f-40b1-ef7d-b13e89e85690"
      },
      "id": "QLDGDZj0xN_D",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42f4fa3",
      "metadata": {
        "id": "c42f4fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc6abe2-adab-4eb5-c244-7f44616af31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of times each patch will run: 5\n",
            "Number of epochs: 100\n",
            "Number of patches: 253 but we will consider : 40\n",
            "Processing from patch 24 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 24\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.366782e+06 | L2 Loss: 3.168665e-08 | Total Loss: 1.366782e+06\n",
            "Epoch   10 | L1 Loss: 1.366782e+06 | L2 Loss: 5.996321e-07 | Total Loss: 1.366782e+06\n",
            "Epoch   15 | L1 Loss: 1.366782e+06 | L2 Loss: 4.095199e-06 | Total Loss: 1.366782e+06\n",
            "Epoch   20 | L1 Loss: 1.366782e+06 | L2 Loss: 8.434676e-05 | Total Loss: 1.366782e+06\n",
            "Epoch   25 | L1 Loss: 1.366780e+06 | L2 Loss: 4.092939e-03 | Total Loss: 1.366780e+06\n",
            "Epoch   30 | L1 Loss: 1.366769e+06 | L2 Loss: 7.676635e-02 | Total Loss: 1.366769e+06\n",
            "Epoch   35 | L1 Loss: 1.366718e+06 | L2 Loss: 9.811825e-01 | Total Loss: 1.366719e+06\n",
            "Epoch   40 | L1 Loss: 1.366558e+06 | L2 Loss: 8.287808e+00 | Total Loss: 1.366566e+06\n",
            "Epoch   45 | L1 Loss: 1.366227e+06 | L2 Loss: 5.318104e+01 | Total Loss: 1.366280e+06\n",
            "Epoch   50 | L1 Loss: 1.365717e+06 | L2 Loss: 2.296201e+02 | Total Loss: 1.365946e+06\n",
            "Epoch   55 | L1 Loss: 1.365164e+06 | L2 Loss: 4.917616e+02 | Total Loss: 1.365656e+06\n",
            "Epoch   60 | L1 Loss: 1.364564e+06 | L2 Loss: 7.322151e+02 | Total Loss: 1.365296e+06\n",
            "Epoch   65 | L1 Loss: 1.364120e+06 | L2 Loss: 7.026207e+02 | Total Loss: 1.364823e+06\n",
            "Epoch   70 | L1 Loss: 1.363526e+06 | L2 Loss: 7.336801e+02 | Total Loss: 1.364260e+06\n",
            "Epoch   75 | L1 Loss: 1.362599e+06 | L2 Loss: 8.097603e+02 | Total Loss: 1.363409e+06\n",
            "Epoch   80 | L1 Loss: 1.361273e+06 | L2 Loss: 7.647059e+02 | Total Loss: 1.362038e+06\n",
            "Epoch   85 | L1 Loss: 1.359286e+06 | L2 Loss: 8.500693e+02 | Total Loss: 1.360136e+06\n",
            "Epoch   90 | L1 Loss: 1.357512e+06 | L2 Loss: 1.547293e+03 | Total Loss: 1.359059e+06\n",
            "Epoch   95 | L1 Loss: 1.353730e+06 | L2 Loss: 2.026145e+03 | Total Loss: 1.355756e+06\n",
            "\n",
            " === Training run 2/5 === Patches: 24\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.366782e+06 | L2 Loss: 3.168665e-08 | Total Loss: 1.366782e+06\n",
            "Epoch   10 | L1 Loss: 1.366782e+06 | L2 Loss: 5.996321e-07 | Total Loss: 1.366782e+06\n",
            "Epoch   15 | L1 Loss: 1.366782e+06 | L2 Loss: 4.095199e-06 | Total Loss: 1.366782e+06\n",
            "Epoch   20 | L1 Loss: 1.366782e+06 | L2 Loss: 8.434676e-05 | Total Loss: 1.366782e+06\n",
            "Epoch   25 | L1 Loss: 1.366780e+06 | L2 Loss: 4.092939e-03 | Total Loss: 1.366780e+06\n",
            "Epoch   30 | L1 Loss: 1.366769e+06 | L2 Loss: 7.676635e-02 | Total Loss: 1.366769e+06\n",
            "Epoch   35 | L1 Loss: 1.366718e+06 | L2 Loss: 9.811825e-01 | Total Loss: 1.366719e+06\n",
            "Epoch   40 | L1 Loss: 1.366558e+06 | L2 Loss: 8.287808e+00 | Total Loss: 1.366566e+06\n",
            "Epoch   45 | L1 Loss: 1.366227e+06 | L2 Loss: 5.318104e+01 | Total Loss: 1.366280e+06\n",
            "Epoch   50 | L1 Loss: 1.365717e+06 | L2 Loss: 2.296201e+02 | Total Loss: 1.365946e+06\n",
            "Epoch   55 | L1 Loss: 1.365164e+06 | L2 Loss: 4.917616e+02 | Total Loss: 1.365656e+06\n",
            "Epoch   60 | L1 Loss: 1.364564e+06 | L2 Loss: 7.322151e+02 | Total Loss: 1.365296e+06\n",
            "Epoch   65 | L1 Loss: 1.364120e+06 | L2 Loss: 7.026207e+02 | Total Loss: 1.364823e+06\n",
            "Epoch   70 | L1 Loss: 1.363526e+06 | L2 Loss: 7.336801e+02 | Total Loss: 1.364260e+06\n",
            "Epoch   75 | L1 Loss: 1.362599e+06 | L2 Loss: 8.097603e+02 | Total Loss: 1.363409e+06\n",
            "Epoch   80 | L1 Loss: 1.361273e+06 | L2 Loss: 7.647059e+02 | Total Loss: 1.362038e+06\n",
            "Epoch   85 | L1 Loss: 1.359286e+06 | L2 Loss: 8.500693e+02 | Total Loss: 1.360136e+06\n",
            "Epoch   90 | L1 Loss: 1.357512e+06 | L2 Loss: 1.547293e+03 | Total Loss: 1.359059e+06\n",
            "Epoch   95 | L1 Loss: 1.353730e+06 | L2 Loss: 2.026145e+03 | Total Loss: 1.355756e+06\n",
            "\n",
            " === Training run 3/5 === Patches: 24\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.366782e+06 | L2 Loss: 3.168665e-08 | Total Loss: 1.366782e+06\n",
            "Epoch   10 | L1 Loss: 1.366782e+06 | L2 Loss: 5.996321e-07 | Total Loss: 1.366782e+06\n",
            "Epoch   15 | L1 Loss: 1.366782e+06 | L2 Loss: 4.095199e-06 | Total Loss: 1.366782e+06\n",
            "Epoch   20 | L1 Loss: 1.366782e+06 | L2 Loss: 8.434676e-05 | Total Loss: 1.366782e+06\n",
            "Epoch   25 | L1 Loss: 1.366780e+06 | L2 Loss: 4.092939e-03 | Total Loss: 1.366780e+06\n",
            "Epoch   30 | L1 Loss: 1.366769e+06 | L2 Loss: 7.676635e-02 | Total Loss: 1.366769e+06\n",
            "Epoch   35 | L1 Loss: 1.366718e+06 | L2 Loss: 9.811825e-01 | Total Loss: 1.366719e+06\n",
            "Epoch   40 | L1 Loss: 1.366558e+06 | L2 Loss: 8.287808e+00 | Total Loss: 1.366566e+06\n",
            "Epoch   45 | L1 Loss: 1.366227e+06 | L2 Loss: 5.318104e+01 | Total Loss: 1.366280e+06\n",
            "Epoch   50 | L1 Loss: 1.365717e+06 | L2 Loss: 2.296201e+02 | Total Loss: 1.365946e+06\n",
            "Epoch   55 | L1 Loss: 1.365164e+06 | L2 Loss: 4.917616e+02 | Total Loss: 1.365656e+06\n",
            "Epoch   60 | L1 Loss: 1.364564e+06 | L2 Loss: 7.322151e+02 | Total Loss: 1.365296e+06\n",
            "Epoch   65 | L1 Loss: 1.364120e+06 | L2 Loss: 7.026207e+02 | Total Loss: 1.364823e+06\n",
            "Epoch   70 | L1 Loss: 1.363526e+06 | L2 Loss: 7.336801e+02 | Total Loss: 1.364260e+06\n",
            "Epoch   75 | L1 Loss: 1.362599e+06 | L2 Loss: 8.097603e+02 | Total Loss: 1.363409e+06\n",
            "Epoch   80 | L1 Loss: 1.361273e+06 | L2 Loss: 7.647059e+02 | Total Loss: 1.362038e+06\n",
            "Epoch   85 | L1 Loss: 1.359286e+06 | L2 Loss: 8.500693e+02 | Total Loss: 1.360136e+06\n",
            "Epoch   90 | L1 Loss: 1.357512e+06 | L2 Loss: 1.547293e+03 | Total Loss: 1.359059e+06\n",
            "Epoch   95 | L1 Loss: 1.353730e+06 | L2 Loss: 2.026145e+03 | Total Loss: 1.355756e+06\n",
            "\n",
            " === Training run 4/5 === Patches: 24\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.366782e+06 | L2 Loss: 3.168665e-08 | Total Loss: 1.366782e+06\n",
            "Epoch   10 | L1 Loss: 1.366782e+06 | L2 Loss: 5.996321e-07 | Total Loss: 1.366782e+06\n",
            "Epoch   15 | L1 Loss: 1.366782e+06 | L2 Loss: 4.095199e-06 | Total Loss: 1.366782e+06\n",
            "Epoch   20 | L1 Loss: 1.366782e+06 | L2 Loss: 8.434676e-05 | Total Loss: 1.366782e+06\n",
            "Epoch   25 | L1 Loss: 1.366780e+06 | L2 Loss: 4.092939e-03 | Total Loss: 1.366780e+06\n",
            "Epoch   30 | L1 Loss: 1.366769e+06 | L2 Loss: 7.676635e-02 | Total Loss: 1.366769e+06\n",
            "Epoch   35 | L1 Loss: 1.366718e+06 | L2 Loss: 9.811825e-01 | Total Loss: 1.366719e+06\n",
            "Epoch   40 | L1 Loss: 1.366558e+06 | L2 Loss: 8.287808e+00 | Total Loss: 1.366566e+06\n",
            "Epoch   45 | L1 Loss: 1.366227e+06 | L2 Loss: 5.318104e+01 | Total Loss: 1.366280e+06\n",
            "Epoch   50 | L1 Loss: 1.365717e+06 | L2 Loss: 2.296201e+02 | Total Loss: 1.365946e+06\n",
            "Epoch   55 | L1 Loss: 1.365164e+06 | L2 Loss: 4.917616e+02 | Total Loss: 1.365656e+06\n",
            "Epoch   60 | L1 Loss: 1.364564e+06 | L2 Loss: 7.322151e+02 | Total Loss: 1.365296e+06\n",
            "Epoch   65 | L1 Loss: 1.364120e+06 | L2 Loss: 7.026207e+02 | Total Loss: 1.364823e+06\n",
            "Epoch   70 | L1 Loss: 1.363526e+06 | L2 Loss: 7.336801e+02 | Total Loss: 1.364260e+06\n",
            "Epoch   75 | L1 Loss: 1.362599e+06 | L2 Loss: 8.097603e+02 | Total Loss: 1.363409e+06\n",
            "Epoch   80 | L1 Loss: 1.361273e+06 | L2 Loss: 7.647059e+02 | Total Loss: 1.362038e+06\n",
            "Epoch   85 | L1 Loss: 1.359286e+06 | L2 Loss: 8.500693e+02 | Total Loss: 1.360136e+06\n",
            "Epoch   90 | L1 Loss: 1.357512e+06 | L2 Loss: 1.547293e+03 | Total Loss: 1.359059e+06\n",
            "Epoch   95 | L1 Loss: 1.353730e+06 | L2 Loss: 2.026145e+03 | Total Loss: 1.355756e+06\n",
            "\n",
            " === Training run 5/5 === Patches: 24\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.366782e+06 | L2 Loss: 3.168665e-08 | Total Loss: 1.366782e+06\n",
            "Epoch   10 | L1 Loss: 1.366782e+06 | L2 Loss: 5.996321e-07 | Total Loss: 1.366782e+06\n",
            "Epoch   15 | L1 Loss: 1.366782e+06 | L2 Loss: 4.095199e-06 | Total Loss: 1.366782e+06\n",
            "Epoch   20 | L1 Loss: 1.366782e+06 | L2 Loss: 8.434676e-05 | Total Loss: 1.366782e+06\n",
            "Epoch   25 | L1 Loss: 1.366780e+06 | L2 Loss: 4.092939e-03 | Total Loss: 1.366780e+06\n",
            "Epoch   30 | L1 Loss: 1.366769e+06 | L2 Loss: 7.676635e-02 | Total Loss: 1.366769e+06\n",
            "Epoch   35 | L1 Loss: 1.366718e+06 | L2 Loss: 9.811825e-01 | Total Loss: 1.366719e+06\n",
            "Epoch   40 | L1 Loss: 1.366558e+06 | L2 Loss: 8.287808e+00 | Total Loss: 1.366566e+06\n",
            "Epoch   45 | L1 Loss: 1.366227e+06 | L2 Loss: 5.318104e+01 | Total Loss: 1.366280e+06\n",
            "Epoch   50 | L1 Loss: 1.365717e+06 | L2 Loss: 2.296201e+02 | Total Loss: 1.365946e+06\n",
            "Epoch   55 | L1 Loss: 1.365164e+06 | L2 Loss: 4.917616e+02 | Total Loss: 1.365656e+06\n",
            "Epoch   60 | L1 Loss: 1.364564e+06 | L2 Loss: 7.322151e+02 | Total Loss: 1.365296e+06\n",
            "Epoch   65 | L1 Loss: 1.364120e+06 | L2 Loss: 7.026207e+02 | Total Loss: 1.364823e+06\n",
            "Epoch   70 | L1 Loss: 1.363526e+06 | L2 Loss: 7.336801e+02 | Total Loss: 1.364260e+06\n",
            "Epoch   75 | L1 Loss: 1.362599e+06 | L2 Loss: 8.097603e+02 | Total Loss: 1.363409e+06\n",
            "Epoch   80 | L1 Loss: 1.361273e+06 | L2 Loss: 7.647059e+02 | Total Loss: 1.362038e+06\n",
            "Epoch   85 | L1 Loss: 1.359286e+06 | L2 Loss: 8.500693e+02 | Total Loss: 1.360136e+06\n",
            "Epoch   90 | L1 Loss: 1.357512e+06 | L2 Loss: 1.547293e+03 | Total Loss: 1.359059e+06\n",
            "Epoch   95 | L1 Loss: 1.353730e+06 | L2 Loss: 2.026145e+03 | Total Loss: 1.355756e+06\n",
            "Processing from patch 25 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 25\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.223975e+07 | L2 Loss: 8.768761e-08 | Total Loss: 1.223975e+07\n",
            "Epoch   10 | L1 Loss: 1.223975e+07 | L2 Loss: 2.532533e-06 | Total Loss: 1.223975e+07\n",
            "Epoch   15 | L1 Loss: 1.223975e+07 | L2 Loss: 3.157276e-05 | Total Loss: 1.223975e+07\n",
            "Epoch   20 | L1 Loss: 1.223975e+07 | L2 Loss: 2.687969e-04 | Total Loss: 1.223975e+07\n",
            "Epoch   25 | L1 Loss: 1.223974e+07 | L2 Loss: 2.804672e-03 | Total Loss: 1.223974e+07\n",
            "Epoch   30 | L1 Loss: 1.223970e+07 | L2 Loss: 7.209397e-02 | Total Loss: 1.223970e+07\n",
            "Epoch   35 | L1 Loss: 1.223958e+07 | L2 Loss: 1.399765e+00 | Total Loss: 1.223958e+07\n",
            "Epoch   40 | L1 Loss: 1.223935e+07 | L2 Loss: 1.514256e+01 | Total Loss: 1.223936e+07\n",
            "Epoch   45 | L1 Loss: 1.223904e+07 | L2 Loss: 6.441276e+01 | Total Loss: 1.223910e+07\n",
            "Epoch   50 | L1 Loss: 1.223854e+07 | L2 Loss: 1.529123e+02 | Total Loss: 1.223869e+07\n",
            "Epoch   55 | L1 Loss: 1.223790e+07 | L2 Loss: 2.623229e+02 | Total Loss: 1.223816e+07\n",
            "Epoch   60 | L1 Loss: 1.223720e+07 | L2 Loss: 2.903193e+02 | Total Loss: 1.223749e+07\n",
            "Epoch   65 | L1 Loss: 1.223639e+07 | L2 Loss: 1.665588e+02 | Total Loss: 1.223656e+07\n",
            "Epoch   70 | L1 Loss: 1.223528e+07 | L2 Loss: 1.277762e+02 | Total Loss: 1.223540e+07\n",
            "Epoch   75 | L1 Loss: 1.223366e+07 | L2 Loss: 3.001003e+02 | Total Loss: 1.223396e+07\n",
            "Epoch   80 | L1 Loss: 1.223165e+07 | L2 Loss: 4.333597e+02 | Total Loss: 1.223208e+07\n",
            "Epoch   85 | L1 Loss: 1.222912e+07 | L2 Loss: 5.732418e+02 | Total Loss: 1.222969e+07\n",
            "Epoch   90 | L1 Loss: 1.222619e+07 | L2 Loss: 6.616786e+02 | Total Loss: 1.222686e+07\n",
            "Epoch   95 | L1 Loss: 1.222286e+07 | L2 Loss: 8.899980e+02 | Total Loss: 1.222375e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 25\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.223975e+07 | L2 Loss: 8.768761e-08 | Total Loss: 1.223975e+07\n",
            "Epoch   10 | L1 Loss: 1.223975e+07 | L2 Loss: 2.532533e-06 | Total Loss: 1.223975e+07\n",
            "Epoch   15 | L1 Loss: 1.223975e+07 | L2 Loss: 3.157276e-05 | Total Loss: 1.223975e+07\n",
            "Epoch   20 | L1 Loss: 1.223975e+07 | L2 Loss: 2.687969e-04 | Total Loss: 1.223975e+07\n",
            "Epoch   25 | L1 Loss: 1.223974e+07 | L2 Loss: 2.804672e-03 | Total Loss: 1.223974e+07\n",
            "Epoch   30 | L1 Loss: 1.223970e+07 | L2 Loss: 7.209397e-02 | Total Loss: 1.223970e+07\n",
            "Epoch   35 | L1 Loss: 1.223958e+07 | L2 Loss: 1.399765e+00 | Total Loss: 1.223958e+07\n",
            "Epoch   40 | L1 Loss: 1.223935e+07 | L2 Loss: 1.514256e+01 | Total Loss: 1.223936e+07\n",
            "Epoch   45 | L1 Loss: 1.223904e+07 | L2 Loss: 6.441276e+01 | Total Loss: 1.223910e+07\n",
            "Epoch   50 | L1 Loss: 1.223854e+07 | L2 Loss: 1.529123e+02 | Total Loss: 1.223869e+07\n",
            "Epoch   55 | L1 Loss: 1.223790e+07 | L2 Loss: 2.623229e+02 | Total Loss: 1.223816e+07\n",
            "Epoch   60 | L1 Loss: 1.223720e+07 | L2 Loss: 2.903193e+02 | Total Loss: 1.223749e+07\n",
            "Epoch   65 | L1 Loss: 1.223639e+07 | L2 Loss: 1.665588e+02 | Total Loss: 1.223656e+07\n",
            "Epoch   70 | L1 Loss: 1.223528e+07 | L2 Loss: 1.277762e+02 | Total Loss: 1.223540e+07\n",
            "Epoch   75 | L1 Loss: 1.223366e+07 | L2 Loss: 3.001003e+02 | Total Loss: 1.223396e+07\n",
            "Epoch   80 | L1 Loss: 1.223165e+07 | L2 Loss: 4.333597e+02 | Total Loss: 1.223208e+07\n",
            "Epoch   85 | L1 Loss: 1.222912e+07 | L2 Loss: 5.732418e+02 | Total Loss: 1.222969e+07\n",
            "Epoch   90 | L1 Loss: 1.222619e+07 | L2 Loss: 6.616786e+02 | Total Loss: 1.222686e+07\n",
            "Epoch   95 | L1 Loss: 1.222286e+07 | L2 Loss: 8.899980e+02 | Total Loss: 1.222375e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 25\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.223975e+07 | L2 Loss: 8.768761e-08 | Total Loss: 1.223975e+07\n",
            "Epoch   10 | L1 Loss: 1.223975e+07 | L2 Loss: 2.532533e-06 | Total Loss: 1.223975e+07\n",
            "Epoch   15 | L1 Loss: 1.223975e+07 | L2 Loss: 3.157276e-05 | Total Loss: 1.223975e+07\n",
            "Epoch   20 | L1 Loss: 1.223975e+07 | L2 Loss: 2.687969e-04 | Total Loss: 1.223975e+07\n",
            "Epoch   25 | L1 Loss: 1.223974e+07 | L2 Loss: 2.804672e-03 | Total Loss: 1.223974e+07\n",
            "Epoch   30 | L1 Loss: 1.223970e+07 | L2 Loss: 7.209397e-02 | Total Loss: 1.223970e+07\n",
            "Epoch   35 | L1 Loss: 1.223958e+07 | L2 Loss: 1.399765e+00 | Total Loss: 1.223958e+07\n",
            "Epoch   40 | L1 Loss: 1.223935e+07 | L2 Loss: 1.514256e+01 | Total Loss: 1.223936e+07\n",
            "Epoch   45 | L1 Loss: 1.223904e+07 | L2 Loss: 6.441276e+01 | Total Loss: 1.223910e+07\n",
            "Epoch   50 | L1 Loss: 1.223854e+07 | L2 Loss: 1.529123e+02 | Total Loss: 1.223869e+07\n",
            "Epoch   55 | L1 Loss: 1.223790e+07 | L2 Loss: 2.623229e+02 | Total Loss: 1.223816e+07\n",
            "Epoch   60 | L1 Loss: 1.223720e+07 | L2 Loss: 2.903193e+02 | Total Loss: 1.223749e+07\n",
            "Epoch   65 | L1 Loss: 1.223639e+07 | L2 Loss: 1.665588e+02 | Total Loss: 1.223656e+07\n",
            "Epoch   70 | L1 Loss: 1.223528e+07 | L2 Loss: 1.277762e+02 | Total Loss: 1.223540e+07\n",
            "Epoch   75 | L1 Loss: 1.223366e+07 | L2 Loss: 3.001003e+02 | Total Loss: 1.223396e+07\n",
            "Epoch   80 | L1 Loss: 1.223165e+07 | L2 Loss: 4.333597e+02 | Total Loss: 1.223208e+07\n",
            "Epoch   85 | L1 Loss: 1.222912e+07 | L2 Loss: 5.732418e+02 | Total Loss: 1.222969e+07\n",
            "Epoch   90 | L1 Loss: 1.222619e+07 | L2 Loss: 6.616786e+02 | Total Loss: 1.222686e+07\n",
            "Epoch   95 | L1 Loss: 1.222286e+07 | L2 Loss: 8.899980e+02 | Total Loss: 1.222375e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 25\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.223975e+07 | L2 Loss: 8.768761e-08 | Total Loss: 1.223975e+07\n",
            "Epoch   10 | L1 Loss: 1.223975e+07 | L2 Loss: 2.532533e-06 | Total Loss: 1.223975e+07\n",
            "Epoch   15 | L1 Loss: 1.223975e+07 | L2 Loss: 3.157276e-05 | Total Loss: 1.223975e+07\n",
            "Epoch   20 | L1 Loss: 1.223975e+07 | L2 Loss: 2.687969e-04 | Total Loss: 1.223975e+07\n",
            "Epoch   25 | L1 Loss: 1.223974e+07 | L2 Loss: 2.804672e-03 | Total Loss: 1.223974e+07\n",
            "Epoch   30 | L1 Loss: 1.223970e+07 | L2 Loss: 7.209397e-02 | Total Loss: 1.223970e+07\n",
            "Epoch   35 | L1 Loss: 1.223958e+07 | L2 Loss: 1.399765e+00 | Total Loss: 1.223958e+07\n",
            "Epoch   40 | L1 Loss: 1.223935e+07 | L2 Loss: 1.514256e+01 | Total Loss: 1.223936e+07\n",
            "Epoch   45 | L1 Loss: 1.223904e+07 | L2 Loss: 6.441276e+01 | Total Loss: 1.223910e+07\n",
            "Epoch   50 | L1 Loss: 1.223854e+07 | L2 Loss: 1.529123e+02 | Total Loss: 1.223869e+07\n",
            "Epoch   55 | L1 Loss: 1.223790e+07 | L2 Loss: 2.623229e+02 | Total Loss: 1.223816e+07\n",
            "Epoch   60 | L1 Loss: 1.223720e+07 | L2 Loss: 2.903193e+02 | Total Loss: 1.223749e+07\n",
            "Epoch   65 | L1 Loss: 1.223639e+07 | L2 Loss: 1.665588e+02 | Total Loss: 1.223656e+07\n",
            "Epoch   70 | L1 Loss: 1.223528e+07 | L2 Loss: 1.277762e+02 | Total Loss: 1.223540e+07\n",
            "Epoch   75 | L1 Loss: 1.223366e+07 | L2 Loss: 3.001003e+02 | Total Loss: 1.223396e+07\n",
            "Epoch   80 | L1 Loss: 1.223165e+07 | L2 Loss: 4.333597e+02 | Total Loss: 1.223208e+07\n",
            "Epoch   85 | L1 Loss: 1.222912e+07 | L2 Loss: 5.732418e+02 | Total Loss: 1.222969e+07\n",
            "Epoch   90 | L1 Loss: 1.222619e+07 | L2 Loss: 6.616786e+02 | Total Loss: 1.222686e+07\n",
            "Epoch   95 | L1 Loss: 1.222286e+07 | L2 Loss: 8.899980e+02 | Total Loss: 1.222375e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 25\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.223975e+07 | L2 Loss: 8.768761e-08 | Total Loss: 1.223975e+07\n",
            "Epoch   10 | L1 Loss: 1.223975e+07 | L2 Loss: 2.532533e-06 | Total Loss: 1.223975e+07\n",
            "Epoch   15 | L1 Loss: 1.223975e+07 | L2 Loss: 3.157276e-05 | Total Loss: 1.223975e+07\n",
            "Epoch   20 | L1 Loss: 1.223975e+07 | L2 Loss: 2.687969e-04 | Total Loss: 1.223975e+07\n",
            "Epoch   25 | L1 Loss: 1.223974e+07 | L2 Loss: 2.804672e-03 | Total Loss: 1.223974e+07\n",
            "Epoch   30 | L1 Loss: 1.223970e+07 | L2 Loss: 7.209397e-02 | Total Loss: 1.223970e+07\n",
            "Epoch   35 | L1 Loss: 1.223958e+07 | L2 Loss: 1.399765e+00 | Total Loss: 1.223958e+07\n",
            "Epoch   40 | L1 Loss: 1.223935e+07 | L2 Loss: 1.514256e+01 | Total Loss: 1.223936e+07\n",
            "Epoch   45 | L1 Loss: 1.223904e+07 | L2 Loss: 6.441276e+01 | Total Loss: 1.223910e+07\n",
            "Epoch   50 | L1 Loss: 1.223854e+07 | L2 Loss: 1.529123e+02 | Total Loss: 1.223869e+07\n",
            "Epoch   55 | L1 Loss: 1.223790e+07 | L2 Loss: 2.623229e+02 | Total Loss: 1.223816e+07\n",
            "Epoch   60 | L1 Loss: 1.223720e+07 | L2 Loss: 2.903193e+02 | Total Loss: 1.223749e+07\n",
            "Epoch   65 | L1 Loss: 1.223639e+07 | L2 Loss: 1.665588e+02 | Total Loss: 1.223656e+07\n",
            "Epoch   70 | L1 Loss: 1.223528e+07 | L2 Loss: 1.277762e+02 | Total Loss: 1.223540e+07\n",
            "Epoch   75 | L1 Loss: 1.223366e+07 | L2 Loss: 3.001003e+02 | Total Loss: 1.223396e+07\n",
            "Epoch   80 | L1 Loss: 1.223165e+07 | L2 Loss: 4.333597e+02 | Total Loss: 1.223208e+07\n",
            "Epoch   85 | L1 Loss: 1.222912e+07 | L2 Loss: 5.732418e+02 | Total Loss: 1.222969e+07\n",
            "Epoch   90 | L1 Loss: 1.222619e+07 | L2 Loss: 6.616786e+02 | Total Loss: 1.222686e+07\n",
            "Epoch   95 | L1 Loss: 1.222286e+07 | L2 Loss: 8.899980e+02 | Total Loss: 1.222375e+07\n",
            "Processing from patch 26 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 26\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.634854e+07 | L2 Loss: 4.724905e-08 | Total Loss: 1.634854e+07\n",
            "Epoch   10 | L1 Loss: 1.634854e+07 | L2 Loss: 2.273321e-06 | Total Loss: 1.634854e+07\n",
            "Epoch   15 | L1 Loss: 1.634854e+07 | L2 Loss: 3.408124e-05 | Total Loss: 1.634854e+07\n",
            "Epoch   20 | L1 Loss: 1.634854e+07 | L2 Loss: 3.293953e-04 | Total Loss: 1.634854e+07\n",
            "Epoch   25 | L1 Loss: 1.634854e+07 | L2 Loss: 2.589440e-03 | Total Loss: 1.634854e+07\n",
            "Epoch   30 | L1 Loss: 1.634852e+07 | L2 Loss: 1.991602e-02 | Total Loss: 1.634852e+07\n",
            "Epoch   35 | L1 Loss: 1.634845e+07 | L2 Loss: 1.452709e-01 | Total Loss: 1.634845e+07\n",
            "Epoch   40 | L1 Loss: 1.634819e+07 | L2 Loss: 1.034981e+00 | Total Loss: 1.634819e+07\n",
            "Epoch   45 | L1 Loss: 1.634760e+07 | L2 Loss: 7.549334e+00 | Total Loss: 1.634761e+07\n",
            "Epoch   50 | L1 Loss: 1.634683e+07 | L2 Loss: 4.668616e+01 | Total Loss: 1.634688e+07\n",
            "Epoch   55 | L1 Loss: 1.634620e+07 | L2 Loss: 1.534587e+02 | Total Loss: 1.634635e+07\n",
            "Epoch   60 | L1 Loss: 1.634547e+07 | L2 Loss: 1.833487e+02 | Total Loss: 1.634565e+07\n",
            "Epoch   65 | L1 Loss: 1.634456e+07 | L2 Loss: 1.686150e+02 | Total Loss: 1.634473e+07\n",
            "Epoch   70 | L1 Loss: 1.634362e+07 | L2 Loss: 1.512332e+02 | Total Loss: 1.634378e+07\n",
            "Epoch   75 | L1 Loss: 1.634236e+07 | L2 Loss: 2.042879e+02 | Total Loss: 1.634257e+07\n",
            "Epoch   80 | L1 Loss: 1.634071e+07 | L2 Loss: 2.859317e+02 | Total Loss: 1.634100e+07\n",
            "Epoch   85 | L1 Loss: 1.633878e+07 | L2 Loss: 3.280912e+02 | Total Loss: 1.633911e+07\n",
            "Epoch   90 | L1 Loss: 1.633615e+07 | L2 Loss: 4.864561e+02 | Total Loss: 1.633664e+07\n",
            "Epoch   95 | L1 Loss: 1.633276e+07 | L2 Loss: 7.597880e+02 | Total Loss: 1.633352e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 26\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.634854e+07 | L2 Loss: 4.724905e-08 | Total Loss: 1.634854e+07\n",
            "Epoch   10 | L1 Loss: 1.634854e+07 | L2 Loss: 2.273321e-06 | Total Loss: 1.634854e+07\n",
            "Epoch   15 | L1 Loss: 1.634854e+07 | L2 Loss: 3.408124e-05 | Total Loss: 1.634854e+07\n",
            "Epoch   20 | L1 Loss: 1.634854e+07 | L2 Loss: 3.293953e-04 | Total Loss: 1.634854e+07\n",
            "Epoch   25 | L1 Loss: 1.634854e+07 | L2 Loss: 2.589440e-03 | Total Loss: 1.634854e+07\n",
            "Epoch   30 | L1 Loss: 1.634852e+07 | L2 Loss: 1.991602e-02 | Total Loss: 1.634852e+07\n",
            "Epoch   35 | L1 Loss: 1.634845e+07 | L2 Loss: 1.452709e-01 | Total Loss: 1.634845e+07\n",
            "Epoch   40 | L1 Loss: 1.634819e+07 | L2 Loss: 1.034981e+00 | Total Loss: 1.634819e+07\n",
            "Epoch   45 | L1 Loss: 1.634760e+07 | L2 Loss: 7.549334e+00 | Total Loss: 1.634761e+07\n",
            "Epoch   50 | L1 Loss: 1.634683e+07 | L2 Loss: 4.668616e+01 | Total Loss: 1.634688e+07\n",
            "Epoch   55 | L1 Loss: 1.634620e+07 | L2 Loss: 1.534587e+02 | Total Loss: 1.634635e+07\n",
            "Epoch   60 | L1 Loss: 1.634547e+07 | L2 Loss: 1.833487e+02 | Total Loss: 1.634565e+07\n",
            "Epoch   65 | L1 Loss: 1.634456e+07 | L2 Loss: 1.686150e+02 | Total Loss: 1.634473e+07\n",
            "Epoch   70 | L1 Loss: 1.634362e+07 | L2 Loss: 1.512332e+02 | Total Loss: 1.634378e+07\n",
            "Epoch   75 | L1 Loss: 1.634236e+07 | L2 Loss: 2.042879e+02 | Total Loss: 1.634257e+07\n",
            "Epoch   80 | L1 Loss: 1.634071e+07 | L2 Loss: 2.859317e+02 | Total Loss: 1.634100e+07\n",
            "Epoch   85 | L1 Loss: 1.633878e+07 | L2 Loss: 3.280912e+02 | Total Loss: 1.633911e+07\n",
            "Epoch   90 | L1 Loss: 1.633615e+07 | L2 Loss: 4.864561e+02 | Total Loss: 1.633664e+07\n",
            "Epoch   95 | L1 Loss: 1.633276e+07 | L2 Loss: 7.597880e+02 | Total Loss: 1.633352e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 26\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.634854e+07 | L2 Loss: 4.724905e-08 | Total Loss: 1.634854e+07\n",
            "Epoch   10 | L1 Loss: 1.634854e+07 | L2 Loss: 2.273321e-06 | Total Loss: 1.634854e+07\n",
            "Epoch   15 | L1 Loss: 1.634854e+07 | L2 Loss: 3.408124e-05 | Total Loss: 1.634854e+07\n",
            "Epoch   20 | L1 Loss: 1.634854e+07 | L2 Loss: 3.293953e-04 | Total Loss: 1.634854e+07\n",
            "Epoch   25 | L1 Loss: 1.634854e+07 | L2 Loss: 2.589440e-03 | Total Loss: 1.634854e+07\n",
            "Epoch   30 | L1 Loss: 1.634852e+07 | L2 Loss: 1.991602e-02 | Total Loss: 1.634852e+07\n",
            "Epoch   35 | L1 Loss: 1.634845e+07 | L2 Loss: 1.452709e-01 | Total Loss: 1.634845e+07\n",
            "Epoch   40 | L1 Loss: 1.634819e+07 | L2 Loss: 1.034981e+00 | Total Loss: 1.634819e+07\n",
            "Epoch   45 | L1 Loss: 1.634760e+07 | L2 Loss: 7.549334e+00 | Total Loss: 1.634761e+07\n",
            "Epoch   50 | L1 Loss: 1.634683e+07 | L2 Loss: 4.668616e+01 | Total Loss: 1.634688e+07\n",
            "Epoch   55 | L1 Loss: 1.634620e+07 | L2 Loss: 1.534587e+02 | Total Loss: 1.634635e+07\n",
            "Epoch   60 | L1 Loss: 1.634547e+07 | L2 Loss: 1.833487e+02 | Total Loss: 1.634565e+07\n",
            "Epoch   65 | L1 Loss: 1.634456e+07 | L2 Loss: 1.686150e+02 | Total Loss: 1.634473e+07\n",
            "Epoch   70 | L1 Loss: 1.634362e+07 | L2 Loss: 1.512332e+02 | Total Loss: 1.634378e+07\n",
            "Epoch   75 | L1 Loss: 1.634236e+07 | L2 Loss: 2.042879e+02 | Total Loss: 1.634257e+07\n",
            "Epoch   80 | L1 Loss: 1.634071e+07 | L2 Loss: 2.859317e+02 | Total Loss: 1.634100e+07\n",
            "Epoch   85 | L1 Loss: 1.633878e+07 | L2 Loss: 3.280912e+02 | Total Loss: 1.633911e+07\n",
            "Epoch   90 | L1 Loss: 1.633615e+07 | L2 Loss: 4.864561e+02 | Total Loss: 1.633664e+07\n",
            "Epoch   95 | L1 Loss: 1.633276e+07 | L2 Loss: 7.597880e+02 | Total Loss: 1.633352e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 26\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.634854e+07 | L2 Loss: 4.724905e-08 | Total Loss: 1.634854e+07\n",
            "Epoch   10 | L1 Loss: 1.634854e+07 | L2 Loss: 2.273321e-06 | Total Loss: 1.634854e+07\n",
            "Epoch   15 | L1 Loss: 1.634854e+07 | L2 Loss: 3.408124e-05 | Total Loss: 1.634854e+07\n",
            "Epoch   20 | L1 Loss: 1.634854e+07 | L2 Loss: 3.293953e-04 | Total Loss: 1.634854e+07\n",
            "Epoch   25 | L1 Loss: 1.634854e+07 | L2 Loss: 2.589440e-03 | Total Loss: 1.634854e+07\n",
            "Epoch   30 | L1 Loss: 1.634852e+07 | L2 Loss: 1.991602e-02 | Total Loss: 1.634852e+07\n",
            "Epoch   35 | L1 Loss: 1.634845e+07 | L2 Loss: 1.452709e-01 | Total Loss: 1.634845e+07\n",
            "Epoch   40 | L1 Loss: 1.634819e+07 | L2 Loss: 1.034981e+00 | Total Loss: 1.634819e+07\n",
            "Epoch   45 | L1 Loss: 1.634760e+07 | L2 Loss: 7.549334e+00 | Total Loss: 1.634761e+07\n",
            "Epoch   50 | L1 Loss: 1.634683e+07 | L2 Loss: 4.668616e+01 | Total Loss: 1.634688e+07\n",
            "Epoch   55 | L1 Loss: 1.634620e+07 | L2 Loss: 1.534587e+02 | Total Loss: 1.634635e+07\n",
            "Epoch   60 | L1 Loss: 1.634547e+07 | L2 Loss: 1.833487e+02 | Total Loss: 1.634565e+07\n",
            "Epoch   65 | L1 Loss: 1.634456e+07 | L2 Loss: 1.686150e+02 | Total Loss: 1.634473e+07\n",
            "Epoch   70 | L1 Loss: 1.634362e+07 | L2 Loss: 1.512332e+02 | Total Loss: 1.634378e+07\n",
            "Epoch   75 | L1 Loss: 1.634236e+07 | L2 Loss: 2.042879e+02 | Total Loss: 1.634257e+07\n",
            "Epoch   80 | L1 Loss: 1.634071e+07 | L2 Loss: 2.859317e+02 | Total Loss: 1.634100e+07\n",
            "Epoch   85 | L1 Loss: 1.633878e+07 | L2 Loss: 3.280912e+02 | Total Loss: 1.633911e+07\n",
            "Epoch   90 | L1 Loss: 1.633615e+07 | L2 Loss: 4.864561e+02 | Total Loss: 1.633664e+07\n",
            "Epoch   95 | L1 Loss: 1.633276e+07 | L2 Loss: 7.597880e+02 | Total Loss: 1.633352e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 26\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.634854e+07 | L2 Loss: 4.724905e-08 | Total Loss: 1.634854e+07\n",
            "Epoch   10 | L1 Loss: 1.634854e+07 | L2 Loss: 2.273321e-06 | Total Loss: 1.634854e+07\n",
            "Epoch   15 | L1 Loss: 1.634854e+07 | L2 Loss: 3.408124e-05 | Total Loss: 1.634854e+07\n",
            "Epoch   20 | L1 Loss: 1.634854e+07 | L2 Loss: 3.293953e-04 | Total Loss: 1.634854e+07\n",
            "Epoch   25 | L1 Loss: 1.634854e+07 | L2 Loss: 2.589440e-03 | Total Loss: 1.634854e+07\n",
            "Epoch   30 | L1 Loss: 1.634852e+07 | L2 Loss: 1.991602e-02 | Total Loss: 1.634852e+07\n",
            "Epoch   35 | L1 Loss: 1.634845e+07 | L2 Loss: 1.452709e-01 | Total Loss: 1.634845e+07\n",
            "Epoch   40 | L1 Loss: 1.634819e+07 | L2 Loss: 1.034981e+00 | Total Loss: 1.634819e+07\n",
            "Epoch   45 | L1 Loss: 1.634760e+07 | L2 Loss: 7.549334e+00 | Total Loss: 1.634761e+07\n",
            "Epoch   50 | L1 Loss: 1.634683e+07 | L2 Loss: 4.668616e+01 | Total Loss: 1.634688e+07\n",
            "Epoch   55 | L1 Loss: 1.634620e+07 | L2 Loss: 1.534587e+02 | Total Loss: 1.634635e+07\n",
            "Epoch   60 | L1 Loss: 1.634547e+07 | L2 Loss: 1.833487e+02 | Total Loss: 1.634565e+07\n",
            "Epoch   65 | L1 Loss: 1.634456e+07 | L2 Loss: 1.686150e+02 | Total Loss: 1.634473e+07\n",
            "Epoch   70 | L1 Loss: 1.634362e+07 | L2 Loss: 1.512332e+02 | Total Loss: 1.634378e+07\n",
            "Epoch   75 | L1 Loss: 1.634236e+07 | L2 Loss: 2.042879e+02 | Total Loss: 1.634257e+07\n",
            "Epoch   80 | L1 Loss: 1.634071e+07 | L2 Loss: 2.859317e+02 | Total Loss: 1.634100e+07\n",
            "Epoch   85 | L1 Loss: 1.633878e+07 | L2 Loss: 3.280912e+02 | Total Loss: 1.633911e+07\n",
            "Epoch   90 | L1 Loss: 1.633615e+07 | L2 Loss: 4.864561e+02 | Total Loss: 1.633664e+07\n",
            "Epoch   95 | L1 Loss: 1.633276e+07 | L2 Loss: 7.597880e+02 | Total Loss: 1.633352e+07\n",
            "Processing from patch 27 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 27\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.278774e+07 | L2 Loss: 9.533773e-10 | Total Loss: 3.278774e+07\n",
            "Epoch   10 | L1 Loss: 3.278774e+07 | L2 Loss: 4.983758e-07 | Total Loss: 3.278774e+07\n",
            "Epoch   15 | L1 Loss: 3.278774e+07 | L2 Loss: 8.136009e-06 | Total Loss: 3.278774e+07\n",
            "Epoch   20 | L1 Loss: 3.278774e+07 | L2 Loss: 6.408005e-05 | Total Loss: 3.278774e+07\n",
            "Epoch   25 | L1 Loss: 3.278774e+07 | L2 Loss: 3.935263e-04 | Total Loss: 3.278774e+07\n",
            "Epoch   30 | L1 Loss: 3.278773e+07 | L2 Loss: 2.376642e-03 | Total Loss: 3.278773e+07\n",
            "Epoch   35 | L1 Loss: 3.278770e+07 | L2 Loss: 1.525350e-02 | Total Loss: 3.278770e+07\n",
            "Epoch   40 | L1 Loss: 3.278762e+07 | L2 Loss: 1.031420e-01 | Total Loss: 3.278762e+07\n",
            "Epoch   45 | L1 Loss: 3.278739e+07 | L2 Loss: 6.046060e-01 | Total Loss: 3.278739e+07\n",
            "Epoch   50 | L1 Loss: 3.278681e+07 | L2 Loss: 3.590771e+00 | Total Loss: 3.278681e+07\n",
            "Epoch   55 | L1 Loss: 3.278551e+07 | L2 Loss: 1.741552e+01 | Total Loss: 3.278553e+07\n",
            "Epoch   60 | L1 Loss: 3.278308e+07 | L2 Loss: 6.529072e+01 | Total Loss: 3.278315e+07\n",
            "Epoch   65 | L1 Loss: 3.277955e+07 | L2 Loss: 1.789494e+02 | Total Loss: 3.277973e+07\n",
            "Epoch   70 | L1 Loss: 3.277552e+07 | L2 Loss: 4.736273e+02 | Total Loss: 3.277600e+07\n",
            "Epoch   75 | L1 Loss: 3.277103e+07 | L2 Loss: 1.271844e+03 | Total Loss: 3.277230e+07\n",
            "Epoch   80 | L1 Loss: 3.276609e+07 | L2 Loss: 2.331338e+03 | Total Loss: 3.276842e+07\n",
            "Epoch   85 | L1 Loss: 3.276073e+07 | L2 Loss: 2.910027e+03 | Total Loss: 3.276364e+07\n",
            "Epoch   90 | L1 Loss: 3.275466e+07 | L2 Loss: 3.330161e+03 | Total Loss: 3.275799e+07\n",
            "Epoch   95 | L1 Loss: 3.274701e+07 | L2 Loss: 4.501435e+03 | Total Loss: 3.275151e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 27\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.278774e+07 | L2 Loss: 9.533773e-10 | Total Loss: 3.278774e+07\n",
            "Epoch   10 | L1 Loss: 3.278774e+07 | L2 Loss: 4.983758e-07 | Total Loss: 3.278774e+07\n",
            "Epoch   15 | L1 Loss: 3.278774e+07 | L2 Loss: 8.136009e-06 | Total Loss: 3.278774e+07\n",
            "Epoch   20 | L1 Loss: 3.278774e+07 | L2 Loss: 6.408005e-05 | Total Loss: 3.278774e+07\n",
            "Epoch   25 | L1 Loss: 3.278774e+07 | L2 Loss: 3.935263e-04 | Total Loss: 3.278774e+07\n",
            "Epoch   30 | L1 Loss: 3.278773e+07 | L2 Loss: 2.376642e-03 | Total Loss: 3.278773e+07\n",
            "Epoch   35 | L1 Loss: 3.278770e+07 | L2 Loss: 1.525350e-02 | Total Loss: 3.278770e+07\n",
            "Epoch   40 | L1 Loss: 3.278762e+07 | L2 Loss: 1.031420e-01 | Total Loss: 3.278762e+07\n",
            "Epoch   45 | L1 Loss: 3.278739e+07 | L2 Loss: 6.046060e-01 | Total Loss: 3.278739e+07\n",
            "Epoch   50 | L1 Loss: 3.278681e+07 | L2 Loss: 3.590771e+00 | Total Loss: 3.278681e+07\n",
            "Epoch   55 | L1 Loss: 3.278551e+07 | L2 Loss: 1.741552e+01 | Total Loss: 3.278553e+07\n",
            "Epoch   60 | L1 Loss: 3.278308e+07 | L2 Loss: 6.529072e+01 | Total Loss: 3.278315e+07\n",
            "Epoch   65 | L1 Loss: 3.277955e+07 | L2 Loss: 1.789494e+02 | Total Loss: 3.277973e+07\n",
            "Epoch   70 | L1 Loss: 3.277552e+07 | L2 Loss: 4.736273e+02 | Total Loss: 3.277600e+07\n",
            "Epoch   75 | L1 Loss: 3.277103e+07 | L2 Loss: 1.271844e+03 | Total Loss: 3.277230e+07\n",
            "Epoch   80 | L1 Loss: 3.276609e+07 | L2 Loss: 2.331338e+03 | Total Loss: 3.276842e+07\n",
            "Epoch   85 | L1 Loss: 3.276073e+07 | L2 Loss: 2.910027e+03 | Total Loss: 3.276364e+07\n",
            "Epoch   90 | L1 Loss: 3.275466e+07 | L2 Loss: 3.330161e+03 | Total Loss: 3.275799e+07\n",
            "Epoch   95 | L1 Loss: 3.274701e+07 | L2 Loss: 4.501435e+03 | Total Loss: 3.275151e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 27\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.278774e+07 | L2 Loss: 9.533773e-10 | Total Loss: 3.278774e+07\n",
            "Epoch   10 | L1 Loss: 3.278774e+07 | L2 Loss: 4.983758e-07 | Total Loss: 3.278774e+07\n",
            "Epoch   15 | L1 Loss: 3.278774e+07 | L2 Loss: 8.136009e-06 | Total Loss: 3.278774e+07\n",
            "Epoch   20 | L1 Loss: 3.278774e+07 | L2 Loss: 6.408005e-05 | Total Loss: 3.278774e+07\n",
            "Epoch   25 | L1 Loss: 3.278774e+07 | L2 Loss: 3.935263e-04 | Total Loss: 3.278774e+07\n",
            "Epoch   30 | L1 Loss: 3.278773e+07 | L2 Loss: 2.376642e-03 | Total Loss: 3.278773e+07\n",
            "Epoch   35 | L1 Loss: 3.278770e+07 | L2 Loss: 1.525350e-02 | Total Loss: 3.278770e+07\n",
            "Epoch   40 | L1 Loss: 3.278762e+07 | L2 Loss: 1.031420e-01 | Total Loss: 3.278762e+07\n",
            "Epoch   45 | L1 Loss: 3.278739e+07 | L2 Loss: 6.046060e-01 | Total Loss: 3.278739e+07\n",
            "Epoch   50 | L1 Loss: 3.278681e+07 | L2 Loss: 3.590771e+00 | Total Loss: 3.278681e+07\n",
            "Epoch   55 | L1 Loss: 3.278551e+07 | L2 Loss: 1.741552e+01 | Total Loss: 3.278553e+07\n",
            "Epoch   60 | L1 Loss: 3.278308e+07 | L2 Loss: 6.529072e+01 | Total Loss: 3.278315e+07\n",
            "Epoch   65 | L1 Loss: 3.277955e+07 | L2 Loss: 1.789494e+02 | Total Loss: 3.277973e+07\n",
            "Epoch   70 | L1 Loss: 3.277552e+07 | L2 Loss: 4.736273e+02 | Total Loss: 3.277600e+07\n",
            "Epoch   75 | L1 Loss: 3.277103e+07 | L2 Loss: 1.271844e+03 | Total Loss: 3.277230e+07\n",
            "Epoch   80 | L1 Loss: 3.276609e+07 | L2 Loss: 2.331338e+03 | Total Loss: 3.276842e+07\n",
            "Epoch   85 | L1 Loss: 3.276073e+07 | L2 Loss: 2.910027e+03 | Total Loss: 3.276364e+07\n",
            "Epoch   90 | L1 Loss: 3.275466e+07 | L2 Loss: 3.330161e+03 | Total Loss: 3.275799e+07\n",
            "Epoch   95 | L1 Loss: 3.274701e+07 | L2 Loss: 4.501435e+03 | Total Loss: 3.275151e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 27\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.278774e+07 | L2 Loss: 9.533773e-10 | Total Loss: 3.278774e+07\n",
            "Epoch   10 | L1 Loss: 3.278774e+07 | L2 Loss: 4.983758e-07 | Total Loss: 3.278774e+07\n",
            "Epoch   15 | L1 Loss: 3.278774e+07 | L2 Loss: 8.136009e-06 | Total Loss: 3.278774e+07\n",
            "Epoch   20 | L1 Loss: 3.278774e+07 | L2 Loss: 6.408005e-05 | Total Loss: 3.278774e+07\n",
            "Epoch   25 | L1 Loss: 3.278774e+07 | L2 Loss: 3.935263e-04 | Total Loss: 3.278774e+07\n",
            "Epoch   30 | L1 Loss: 3.278773e+07 | L2 Loss: 2.376642e-03 | Total Loss: 3.278773e+07\n",
            "Epoch   35 | L1 Loss: 3.278770e+07 | L2 Loss: 1.525350e-02 | Total Loss: 3.278770e+07\n",
            "Epoch   40 | L1 Loss: 3.278762e+07 | L2 Loss: 1.031420e-01 | Total Loss: 3.278762e+07\n",
            "Epoch   45 | L1 Loss: 3.278739e+07 | L2 Loss: 6.046060e-01 | Total Loss: 3.278739e+07\n",
            "Epoch   50 | L1 Loss: 3.278681e+07 | L2 Loss: 3.590771e+00 | Total Loss: 3.278681e+07\n",
            "Epoch   55 | L1 Loss: 3.278551e+07 | L2 Loss: 1.741552e+01 | Total Loss: 3.278553e+07\n",
            "Epoch   60 | L1 Loss: 3.278308e+07 | L2 Loss: 6.529072e+01 | Total Loss: 3.278315e+07\n",
            "Epoch   65 | L1 Loss: 3.277955e+07 | L2 Loss: 1.789494e+02 | Total Loss: 3.277973e+07\n",
            "Epoch   70 | L1 Loss: 3.277552e+07 | L2 Loss: 4.736273e+02 | Total Loss: 3.277600e+07\n",
            "Epoch   75 | L1 Loss: 3.277103e+07 | L2 Loss: 1.271844e+03 | Total Loss: 3.277230e+07\n",
            "Epoch   80 | L1 Loss: 3.276609e+07 | L2 Loss: 2.331338e+03 | Total Loss: 3.276842e+07\n",
            "Epoch   85 | L1 Loss: 3.276073e+07 | L2 Loss: 2.910027e+03 | Total Loss: 3.276364e+07\n",
            "Epoch   90 | L1 Loss: 3.275466e+07 | L2 Loss: 3.330161e+03 | Total Loss: 3.275799e+07\n",
            "Epoch   95 | L1 Loss: 3.274701e+07 | L2 Loss: 4.501435e+03 | Total Loss: 3.275151e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 27\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.278774e+07 | L2 Loss: 9.533773e-10 | Total Loss: 3.278774e+07\n",
            "Epoch   10 | L1 Loss: 3.278774e+07 | L2 Loss: 4.983758e-07 | Total Loss: 3.278774e+07\n",
            "Epoch   15 | L1 Loss: 3.278774e+07 | L2 Loss: 8.136009e-06 | Total Loss: 3.278774e+07\n",
            "Epoch   20 | L1 Loss: 3.278774e+07 | L2 Loss: 6.408005e-05 | Total Loss: 3.278774e+07\n",
            "Epoch   25 | L1 Loss: 3.278774e+07 | L2 Loss: 3.935263e-04 | Total Loss: 3.278774e+07\n",
            "Epoch   30 | L1 Loss: 3.278773e+07 | L2 Loss: 2.376642e-03 | Total Loss: 3.278773e+07\n",
            "Epoch   35 | L1 Loss: 3.278770e+07 | L2 Loss: 1.525350e-02 | Total Loss: 3.278770e+07\n",
            "Epoch   40 | L1 Loss: 3.278762e+07 | L2 Loss: 1.031420e-01 | Total Loss: 3.278762e+07\n",
            "Epoch   45 | L1 Loss: 3.278739e+07 | L2 Loss: 6.046060e-01 | Total Loss: 3.278739e+07\n",
            "Epoch   50 | L1 Loss: 3.278681e+07 | L2 Loss: 3.590771e+00 | Total Loss: 3.278681e+07\n",
            "Epoch   55 | L1 Loss: 3.278551e+07 | L2 Loss: 1.741552e+01 | Total Loss: 3.278553e+07\n",
            "Epoch   60 | L1 Loss: 3.278308e+07 | L2 Loss: 6.529072e+01 | Total Loss: 3.278315e+07\n",
            "Epoch   65 | L1 Loss: 3.277955e+07 | L2 Loss: 1.789494e+02 | Total Loss: 3.277973e+07\n",
            "Epoch   70 | L1 Loss: 3.277552e+07 | L2 Loss: 4.736273e+02 | Total Loss: 3.277600e+07\n",
            "Epoch   75 | L1 Loss: 3.277103e+07 | L2 Loss: 1.271844e+03 | Total Loss: 3.277230e+07\n",
            "Epoch   80 | L1 Loss: 3.276609e+07 | L2 Loss: 2.331338e+03 | Total Loss: 3.276842e+07\n",
            "Epoch   85 | L1 Loss: 3.276073e+07 | L2 Loss: 2.910027e+03 | Total Loss: 3.276364e+07\n",
            "Epoch   90 | L1 Loss: 3.275466e+07 | L2 Loss: 3.330161e+03 | Total Loss: 3.275799e+07\n",
            "Epoch   95 | L1 Loss: 3.274701e+07 | L2 Loss: 4.501435e+03 | Total Loss: 3.275151e+07\n",
            "Processing from patch 28 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 28\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 7.786711e+07 | L2 Loss: 4.990202e-10 | Total Loss: 7.786711e+07\n",
            "Epoch   10 | L1 Loss: 7.786711e+07 | L2 Loss: 5.073202e-08 | Total Loss: 7.786711e+07\n",
            "Epoch   15 | L1 Loss: 7.786711e+07 | L2 Loss: 2.912599e-06 | Total Loss: 7.786711e+07\n",
            "Epoch   20 | L1 Loss: 7.786711e+07 | L2 Loss: 3.476221e-05 | Total Loss: 7.786711e+07\n",
            "Epoch   25 | L1 Loss: 7.786711e+07 | L2 Loss: 3.049212e-04 | Total Loss: 7.786711e+07\n",
            "Epoch   30 | L1 Loss: 7.786707e+07 | L2 Loss: 3.281093e-03 | Total Loss: 7.786707e+07\n",
            "Epoch   35 | L1 Loss: 7.786692e+07 | L2 Loss: 3.749951e-02 | Total Loss: 7.786692e+07\n",
            "Epoch   40 | L1 Loss: 7.786640e+07 | L2 Loss: 5.045846e-01 | Total Loss: 7.786640e+07\n",
            "Epoch   45 | L1 Loss: 7.786522e+07 | L2 Loss: 5.561854e+00 | Total Loss: 7.786523e+07\n",
            "Epoch   50 | L1 Loss: 7.786369e+07 | L2 Loss: 3.933566e+01 | Total Loss: 7.786373e+07\n",
            "Epoch   55 | L1 Loss: 7.786214e+07 | L2 Loss: 1.203184e+02 | Total Loss: 7.786226e+07\n",
            "Epoch   60 | L1 Loss: 7.786000e+07 | L2 Loss: 1.449611e+02 | Total Loss: 7.786014e+07\n",
            "Epoch   65 | L1 Loss: 7.785774e+07 | L2 Loss: 1.076958e+02 | Total Loss: 7.785785e+07\n",
            "Epoch   70 | L1 Loss: 7.785443e+07 | L2 Loss: 1.822568e+02 | Total Loss: 7.785462e+07\n",
            "Epoch   75 | L1 Loss: 7.785058e+07 | L2 Loss: 2.473439e+02 | Total Loss: 7.785083e+07\n",
            "Epoch   80 | L1 Loss: 7.784550e+07 | L2 Loss: 2.337511e+02 | Total Loss: 7.784573e+07\n",
            "Epoch   85 | L1 Loss: 7.783804e+07 | L2 Loss: 4.290233e+02 | Total Loss: 7.783847e+07\n",
            "Epoch   90 | L1 Loss: 7.782863e+07 | L2 Loss: 8.513624e+02 | Total Loss: 7.782948e+07\n",
            "Epoch   95 | L1 Loss: 7.781525e+07 | L2 Loss: 2.027874e+03 | Total Loss: 7.781727e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 28\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 7.786711e+07 | L2 Loss: 4.990202e-10 | Total Loss: 7.786711e+07\n",
            "Epoch   10 | L1 Loss: 7.786711e+07 | L2 Loss: 5.073202e-08 | Total Loss: 7.786711e+07\n",
            "Epoch   15 | L1 Loss: 7.786711e+07 | L2 Loss: 2.912599e-06 | Total Loss: 7.786711e+07\n",
            "Epoch   20 | L1 Loss: 7.786711e+07 | L2 Loss: 3.476221e-05 | Total Loss: 7.786711e+07\n",
            "Epoch   25 | L1 Loss: 7.786711e+07 | L2 Loss: 3.049212e-04 | Total Loss: 7.786711e+07\n",
            "Epoch   30 | L1 Loss: 7.786707e+07 | L2 Loss: 3.281093e-03 | Total Loss: 7.786707e+07\n",
            "Epoch   35 | L1 Loss: 7.786692e+07 | L2 Loss: 3.749951e-02 | Total Loss: 7.786692e+07\n",
            "Epoch   40 | L1 Loss: 7.786640e+07 | L2 Loss: 5.045846e-01 | Total Loss: 7.786640e+07\n",
            "Epoch   45 | L1 Loss: 7.786522e+07 | L2 Loss: 5.561854e+00 | Total Loss: 7.786523e+07\n",
            "Epoch   50 | L1 Loss: 7.786369e+07 | L2 Loss: 3.933566e+01 | Total Loss: 7.786373e+07\n",
            "Epoch   55 | L1 Loss: 7.786214e+07 | L2 Loss: 1.203184e+02 | Total Loss: 7.786226e+07\n",
            "Epoch   60 | L1 Loss: 7.786000e+07 | L2 Loss: 1.449611e+02 | Total Loss: 7.786014e+07\n",
            "Epoch   65 | L1 Loss: 7.785774e+07 | L2 Loss: 1.076958e+02 | Total Loss: 7.785785e+07\n",
            "Epoch   70 | L1 Loss: 7.785443e+07 | L2 Loss: 1.822568e+02 | Total Loss: 7.785462e+07\n",
            "Epoch   75 | L1 Loss: 7.785058e+07 | L2 Loss: 2.473439e+02 | Total Loss: 7.785083e+07\n",
            "Epoch   80 | L1 Loss: 7.784550e+07 | L2 Loss: 2.337511e+02 | Total Loss: 7.784573e+07\n",
            "Epoch   85 | L1 Loss: 7.783804e+07 | L2 Loss: 4.290233e+02 | Total Loss: 7.783847e+07\n",
            "Epoch   90 | L1 Loss: 7.782863e+07 | L2 Loss: 8.513624e+02 | Total Loss: 7.782948e+07\n",
            "Epoch   95 | L1 Loss: 7.781525e+07 | L2 Loss: 2.027874e+03 | Total Loss: 7.781727e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 28\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 7.786711e+07 | L2 Loss: 4.990202e-10 | Total Loss: 7.786711e+07\n",
            "Epoch   10 | L1 Loss: 7.786711e+07 | L2 Loss: 5.073202e-08 | Total Loss: 7.786711e+07\n",
            "Epoch   15 | L1 Loss: 7.786711e+07 | L2 Loss: 2.912599e-06 | Total Loss: 7.786711e+07\n",
            "Epoch   20 | L1 Loss: 7.786711e+07 | L2 Loss: 3.476221e-05 | Total Loss: 7.786711e+07\n",
            "Epoch   25 | L1 Loss: 7.786711e+07 | L2 Loss: 3.049212e-04 | Total Loss: 7.786711e+07\n",
            "Epoch   30 | L1 Loss: 7.786707e+07 | L2 Loss: 3.281093e-03 | Total Loss: 7.786707e+07\n",
            "Epoch   35 | L1 Loss: 7.786692e+07 | L2 Loss: 3.749951e-02 | Total Loss: 7.786692e+07\n",
            "Epoch   40 | L1 Loss: 7.786640e+07 | L2 Loss: 5.045846e-01 | Total Loss: 7.786640e+07\n",
            "Epoch   45 | L1 Loss: 7.786522e+07 | L2 Loss: 5.561854e+00 | Total Loss: 7.786523e+07\n",
            "Epoch   50 | L1 Loss: 7.786369e+07 | L2 Loss: 3.933566e+01 | Total Loss: 7.786373e+07\n",
            "Epoch   55 | L1 Loss: 7.786214e+07 | L2 Loss: 1.203184e+02 | Total Loss: 7.786226e+07\n",
            "Epoch   60 | L1 Loss: 7.786000e+07 | L2 Loss: 1.449611e+02 | Total Loss: 7.786014e+07\n",
            "Epoch   65 | L1 Loss: 7.785774e+07 | L2 Loss: 1.076958e+02 | Total Loss: 7.785785e+07\n",
            "Epoch   70 | L1 Loss: 7.785443e+07 | L2 Loss: 1.822568e+02 | Total Loss: 7.785462e+07\n",
            "Epoch   75 | L1 Loss: 7.785058e+07 | L2 Loss: 2.473439e+02 | Total Loss: 7.785083e+07\n",
            "Epoch   80 | L1 Loss: 7.784550e+07 | L2 Loss: 2.337511e+02 | Total Loss: 7.784573e+07\n",
            "Epoch   85 | L1 Loss: 7.783804e+07 | L2 Loss: 4.290233e+02 | Total Loss: 7.783847e+07\n",
            "Epoch   90 | L1 Loss: 7.782863e+07 | L2 Loss: 8.513624e+02 | Total Loss: 7.782948e+07\n",
            "Epoch   95 | L1 Loss: 7.781525e+07 | L2 Loss: 2.027874e+03 | Total Loss: 7.781727e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 28\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 7.786711e+07 | L2 Loss: 4.990202e-10 | Total Loss: 7.786711e+07\n",
            "Epoch   10 | L1 Loss: 7.786711e+07 | L2 Loss: 5.073202e-08 | Total Loss: 7.786711e+07\n",
            "Epoch   15 | L1 Loss: 7.786711e+07 | L2 Loss: 2.912599e-06 | Total Loss: 7.786711e+07\n",
            "Epoch   20 | L1 Loss: 7.786711e+07 | L2 Loss: 3.476221e-05 | Total Loss: 7.786711e+07\n",
            "Epoch   25 | L1 Loss: 7.786711e+07 | L2 Loss: 3.049212e-04 | Total Loss: 7.786711e+07\n",
            "Epoch   30 | L1 Loss: 7.786707e+07 | L2 Loss: 3.281093e-03 | Total Loss: 7.786707e+07\n",
            "Epoch   35 | L1 Loss: 7.786692e+07 | L2 Loss: 3.749951e-02 | Total Loss: 7.786692e+07\n",
            "Epoch   40 | L1 Loss: 7.786640e+07 | L2 Loss: 5.045846e-01 | Total Loss: 7.786640e+07\n",
            "Epoch   45 | L1 Loss: 7.786522e+07 | L2 Loss: 5.561854e+00 | Total Loss: 7.786523e+07\n",
            "Epoch   50 | L1 Loss: 7.786369e+07 | L2 Loss: 3.933566e+01 | Total Loss: 7.786373e+07\n",
            "Epoch   55 | L1 Loss: 7.786214e+07 | L2 Loss: 1.203184e+02 | Total Loss: 7.786226e+07\n",
            "Epoch   60 | L1 Loss: 7.786000e+07 | L2 Loss: 1.449611e+02 | Total Loss: 7.786014e+07\n",
            "Epoch   65 | L1 Loss: 7.785774e+07 | L2 Loss: 1.076958e+02 | Total Loss: 7.785785e+07\n",
            "Epoch   70 | L1 Loss: 7.785443e+07 | L2 Loss: 1.822568e+02 | Total Loss: 7.785462e+07\n",
            "Epoch   75 | L1 Loss: 7.785058e+07 | L2 Loss: 2.473439e+02 | Total Loss: 7.785083e+07\n",
            "Epoch   80 | L1 Loss: 7.784550e+07 | L2 Loss: 2.337511e+02 | Total Loss: 7.784573e+07\n",
            "Epoch   85 | L1 Loss: 7.783804e+07 | L2 Loss: 4.290233e+02 | Total Loss: 7.783847e+07\n",
            "Epoch   90 | L1 Loss: 7.782863e+07 | L2 Loss: 8.513624e+02 | Total Loss: 7.782948e+07\n",
            "Epoch   95 | L1 Loss: 7.781525e+07 | L2 Loss: 2.027874e+03 | Total Loss: 7.781727e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 28\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 7.786711e+07 | L2 Loss: 4.990202e-10 | Total Loss: 7.786711e+07\n",
            "Epoch   10 | L1 Loss: 7.786711e+07 | L2 Loss: 5.073202e-08 | Total Loss: 7.786711e+07\n",
            "Epoch   15 | L1 Loss: 7.786711e+07 | L2 Loss: 2.912599e-06 | Total Loss: 7.786711e+07\n",
            "Epoch   20 | L1 Loss: 7.786711e+07 | L2 Loss: 3.476221e-05 | Total Loss: 7.786711e+07\n",
            "Epoch   25 | L1 Loss: 7.786711e+07 | L2 Loss: 3.049212e-04 | Total Loss: 7.786711e+07\n",
            "Epoch   30 | L1 Loss: 7.786707e+07 | L2 Loss: 3.281093e-03 | Total Loss: 7.786707e+07\n",
            "Epoch   35 | L1 Loss: 7.786692e+07 | L2 Loss: 3.749951e-02 | Total Loss: 7.786692e+07\n",
            "Epoch   40 | L1 Loss: 7.786640e+07 | L2 Loss: 5.045846e-01 | Total Loss: 7.786640e+07\n",
            "Epoch   45 | L1 Loss: 7.786522e+07 | L2 Loss: 5.561854e+00 | Total Loss: 7.786523e+07\n",
            "Epoch   50 | L1 Loss: 7.786369e+07 | L2 Loss: 3.933566e+01 | Total Loss: 7.786373e+07\n",
            "Epoch   55 | L1 Loss: 7.786214e+07 | L2 Loss: 1.203184e+02 | Total Loss: 7.786226e+07\n",
            "Epoch   60 | L1 Loss: 7.786000e+07 | L2 Loss: 1.449611e+02 | Total Loss: 7.786014e+07\n",
            "Epoch   65 | L1 Loss: 7.785774e+07 | L2 Loss: 1.076958e+02 | Total Loss: 7.785785e+07\n",
            "Epoch   70 | L1 Loss: 7.785443e+07 | L2 Loss: 1.822568e+02 | Total Loss: 7.785462e+07\n",
            "Epoch   75 | L1 Loss: 7.785058e+07 | L2 Loss: 2.473439e+02 | Total Loss: 7.785083e+07\n",
            "Epoch   80 | L1 Loss: 7.784550e+07 | L2 Loss: 2.337511e+02 | Total Loss: 7.784573e+07\n",
            "Epoch   85 | L1 Loss: 7.783804e+07 | L2 Loss: 4.290233e+02 | Total Loss: 7.783847e+07\n",
            "Epoch   90 | L1 Loss: 7.782863e+07 | L2 Loss: 8.513624e+02 | Total Loss: 7.782948e+07\n",
            "Epoch   95 | L1 Loss: 7.781525e+07 | L2 Loss: 2.027874e+03 | Total Loss: 7.781727e+07\n",
            "Processing from patch 29 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 29\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 4.231956e+07 | L2 Loss: 7.438260e-08 | Total Loss: 4.231956e+07\n",
            "Epoch   10 | L1 Loss: 4.231956e+07 | L2 Loss: 2.214096e-06 | Total Loss: 4.231956e+07\n",
            "Epoch   15 | L1 Loss: 4.231956e+07 | L2 Loss: 2.467248e-05 | Total Loss: 4.231956e+07\n",
            "Epoch   20 | L1 Loss: 4.231956e+07 | L2 Loss: 2.174167e-04 | Total Loss: 4.231956e+07\n",
            "Epoch   25 | L1 Loss: 4.231956e+07 | L2 Loss: 1.645243e-03 | Total Loss: 4.231956e+07\n",
            "Epoch   30 | L1 Loss: 4.231955e+07 | L2 Loss: 1.120152e-02 | Total Loss: 4.231955e+07\n",
            "Epoch   35 | L1 Loss: 4.231949e+07 | L2 Loss: 7.327598e-02 | Total Loss: 4.231949e+07\n",
            "Epoch   40 | L1 Loss: 4.231928e+07 | L2 Loss: 4.552563e-01 | Total Loss: 4.231928e+07\n",
            "Epoch   45 | L1 Loss: 4.231874e+07 | L2 Loss: 2.610103e+00 | Total Loss: 4.231875e+07\n",
            "Epoch   50 | L1 Loss: 4.231784e+07 | L2 Loss: 1.292347e+01 | Total Loss: 4.231785e+07\n",
            "Epoch   55 | L1 Loss: 4.231666e+07 | L2 Loss: 5.019071e+01 | Total Loss: 4.231671e+07\n",
            "Epoch   60 | L1 Loss: 4.231501e+07 | L2 Loss: 1.434963e+02 | Total Loss: 4.231516e+07\n",
            "Epoch   65 | L1 Loss: 4.231256e+07 | L2 Loss: 2.047392e+02 | Total Loss: 4.231277e+07\n",
            "Epoch   70 | L1 Loss: 4.230942e+07 | L2 Loss: 2.198469e+02 | Total Loss: 4.230964e+07\n",
            "Epoch   75 | L1 Loss: 4.230538e+07 | L2 Loss: 3.082718e+02 | Total Loss: 4.230569e+07\n",
            "Epoch   80 | L1 Loss: 4.229985e+07 | L2 Loss: 5.650050e+02 | Total Loss: 4.230042e+07\n",
            "Epoch   85 | L1 Loss: 4.229354e+07 | L2 Loss: 1.014528e+03 | Total Loss: 4.229456e+07\n",
            "Epoch   90 | L1 Loss: 4.228670e+07 | L2 Loss: 2.313790e+03 | Total Loss: 4.228901e+07\n",
            "Epoch   95 | L1 Loss: 4.227580e+07 | L2 Loss: 2.473742e+03 | Total Loss: 4.227827e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 29\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 4.231956e+07 | L2 Loss: 7.438260e-08 | Total Loss: 4.231956e+07\n",
            "Epoch   10 | L1 Loss: 4.231956e+07 | L2 Loss: 2.214096e-06 | Total Loss: 4.231956e+07\n",
            "Epoch   15 | L1 Loss: 4.231956e+07 | L2 Loss: 2.467248e-05 | Total Loss: 4.231956e+07\n",
            "Epoch   20 | L1 Loss: 4.231956e+07 | L2 Loss: 2.174167e-04 | Total Loss: 4.231956e+07\n",
            "Epoch   25 | L1 Loss: 4.231956e+07 | L2 Loss: 1.645243e-03 | Total Loss: 4.231956e+07\n",
            "Epoch   30 | L1 Loss: 4.231955e+07 | L2 Loss: 1.120152e-02 | Total Loss: 4.231955e+07\n",
            "Epoch   35 | L1 Loss: 4.231949e+07 | L2 Loss: 7.327598e-02 | Total Loss: 4.231949e+07\n",
            "Epoch   40 | L1 Loss: 4.231928e+07 | L2 Loss: 4.552563e-01 | Total Loss: 4.231928e+07\n",
            "Epoch   45 | L1 Loss: 4.231874e+07 | L2 Loss: 2.610103e+00 | Total Loss: 4.231875e+07\n",
            "Epoch   50 | L1 Loss: 4.231784e+07 | L2 Loss: 1.292347e+01 | Total Loss: 4.231785e+07\n",
            "Epoch   55 | L1 Loss: 4.231666e+07 | L2 Loss: 5.019071e+01 | Total Loss: 4.231671e+07\n",
            "Epoch   60 | L1 Loss: 4.231501e+07 | L2 Loss: 1.434963e+02 | Total Loss: 4.231516e+07\n",
            "Epoch   65 | L1 Loss: 4.231256e+07 | L2 Loss: 2.047392e+02 | Total Loss: 4.231277e+07\n",
            "Epoch   70 | L1 Loss: 4.230942e+07 | L2 Loss: 2.198469e+02 | Total Loss: 4.230964e+07\n",
            "Epoch   75 | L1 Loss: 4.230538e+07 | L2 Loss: 3.082718e+02 | Total Loss: 4.230569e+07\n",
            "Epoch   80 | L1 Loss: 4.229985e+07 | L2 Loss: 5.650050e+02 | Total Loss: 4.230042e+07\n",
            "Epoch   85 | L1 Loss: 4.229354e+07 | L2 Loss: 1.014528e+03 | Total Loss: 4.229456e+07\n",
            "Epoch   90 | L1 Loss: 4.228670e+07 | L2 Loss: 2.313790e+03 | Total Loss: 4.228901e+07\n",
            "Epoch   95 | L1 Loss: 4.227580e+07 | L2 Loss: 2.473742e+03 | Total Loss: 4.227827e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 29\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 4.231956e+07 | L2 Loss: 7.438260e-08 | Total Loss: 4.231956e+07\n",
            "Epoch   10 | L1 Loss: 4.231956e+07 | L2 Loss: 2.214096e-06 | Total Loss: 4.231956e+07\n",
            "Epoch   15 | L1 Loss: 4.231956e+07 | L2 Loss: 2.467248e-05 | Total Loss: 4.231956e+07\n",
            "Epoch   20 | L1 Loss: 4.231956e+07 | L2 Loss: 2.174167e-04 | Total Loss: 4.231956e+07\n",
            "Epoch   25 | L1 Loss: 4.231956e+07 | L2 Loss: 1.645243e-03 | Total Loss: 4.231956e+07\n",
            "Epoch   30 | L1 Loss: 4.231955e+07 | L2 Loss: 1.120152e-02 | Total Loss: 4.231955e+07\n",
            "Epoch   35 | L1 Loss: 4.231949e+07 | L2 Loss: 7.327598e-02 | Total Loss: 4.231949e+07\n",
            "Epoch   40 | L1 Loss: 4.231928e+07 | L2 Loss: 4.552563e-01 | Total Loss: 4.231928e+07\n",
            "Epoch   45 | L1 Loss: 4.231874e+07 | L2 Loss: 2.610103e+00 | Total Loss: 4.231875e+07\n",
            "Epoch   50 | L1 Loss: 4.231784e+07 | L2 Loss: 1.292347e+01 | Total Loss: 4.231785e+07\n",
            "Epoch   55 | L1 Loss: 4.231666e+07 | L2 Loss: 5.019071e+01 | Total Loss: 4.231671e+07\n",
            "Epoch   60 | L1 Loss: 4.231501e+07 | L2 Loss: 1.434963e+02 | Total Loss: 4.231516e+07\n",
            "Epoch   65 | L1 Loss: 4.231256e+07 | L2 Loss: 2.047392e+02 | Total Loss: 4.231277e+07\n",
            "Epoch   70 | L1 Loss: 4.230942e+07 | L2 Loss: 2.198469e+02 | Total Loss: 4.230964e+07\n",
            "Epoch   75 | L1 Loss: 4.230538e+07 | L2 Loss: 3.082718e+02 | Total Loss: 4.230569e+07\n",
            "Epoch   80 | L1 Loss: 4.229985e+07 | L2 Loss: 5.650050e+02 | Total Loss: 4.230042e+07\n",
            "Epoch   85 | L1 Loss: 4.229354e+07 | L2 Loss: 1.014528e+03 | Total Loss: 4.229456e+07\n",
            "Epoch   90 | L1 Loss: 4.228670e+07 | L2 Loss: 2.313790e+03 | Total Loss: 4.228901e+07\n",
            "Epoch   95 | L1 Loss: 4.227580e+07 | L2 Loss: 2.473742e+03 | Total Loss: 4.227827e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 29\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 4.231956e+07 | L2 Loss: 7.438260e-08 | Total Loss: 4.231956e+07\n",
            "Epoch   10 | L1 Loss: 4.231956e+07 | L2 Loss: 2.214096e-06 | Total Loss: 4.231956e+07\n",
            "Epoch   15 | L1 Loss: 4.231956e+07 | L2 Loss: 2.467248e-05 | Total Loss: 4.231956e+07\n",
            "Epoch   20 | L1 Loss: 4.231956e+07 | L2 Loss: 2.174167e-04 | Total Loss: 4.231956e+07\n",
            "Epoch   25 | L1 Loss: 4.231956e+07 | L2 Loss: 1.645243e-03 | Total Loss: 4.231956e+07\n",
            "Epoch   30 | L1 Loss: 4.231955e+07 | L2 Loss: 1.120152e-02 | Total Loss: 4.231955e+07\n",
            "Epoch   35 | L1 Loss: 4.231949e+07 | L2 Loss: 7.327598e-02 | Total Loss: 4.231949e+07\n",
            "Epoch   40 | L1 Loss: 4.231928e+07 | L2 Loss: 4.552563e-01 | Total Loss: 4.231928e+07\n",
            "Epoch   45 | L1 Loss: 4.231874e+07 | L2 Loss: 2.610103e+00 | Total Loss: 4.231875e+07\n",
            "Epoch   50 | L1 Loss: 4.231784e+07 | L2 Loss: 1.292347e+01 | Total Loss: 4.231785e+07\n",
            "Epoch   55 | L1 Loss: 4.231666e+07 | L2 Loss: 5.019071e+01 | Total Loss: 4.231671e+07\n",
            "Epoch   60 | L1 Loss: 4.231501e+07 | L2 Loss: 1.434963e+02 | Total Loss: 4.231516e+07\n",
            "Epoch   65 | L1 Loss: 4.231256e+07 | L2 Loss: 2.047392e+02 | Total Loss: 4.231277e+07\n",
            "Epoch   70 | L1 Loss: 4.230942e+07 | L2 Loss: 2.198469e+02 | Total Loss: 4.230964e+07\n",
            "Epoch   75 | L1 Loss: 4.230538e+07 | L2 Loss: 3.082718e+02 | Total Loss: 4.230569e+07\n",
            "Epoch   80 | L1 Loss: 4.229985e+07 | L2 Loss: 5.650050e+02 | Total Loss: 4.230042e+07\n",
            "Epoch   85 | L1 Loss: 4.229354e+07 | L2 Loss: 1.014528e+03 | Total Loss: 4.229456e+07\n",
            "Epoch   90 | L1 Loss: 4.228670e+07 | L2 Loss: 2.313790e+03 | Total Loss: 4.228901e+07\n",
            "Epoch   95 | L1 Loss: 4.227580e+07 | L2 Loss: 2.473742e+03 | Total Loss: 4.227827e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 29\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 4.231956e+07 | L2 Loss: 7.438260e-08 | Total Loss: 4.231956e+07\n",
            "Epoch   10 | L1 Loss: 4.231956e+07 | L2 Loss: 2.214096e-06 | Total Loss: 4.231956e+07\n",
            "Epoch   15 | L1 Loss: 4.231956e+07 | L2 Loss: 2.467248e-05 | Total Loss: 4.231956e+07\n",
            "Epoch   20 | L1 Loss: 4.231956e+07 | L2 Loss: 2.174167e-04 | Total Loss: 4.231956e+07\n",
            "Epoch   25 | L1 Loss: 4.231956e+07 | L2 Loss: 1.645243e-03 | Total Loss: 4.231956e+07\n",
            "Epoch   30 | L1 Loss: 4.231955e+07 | L2 Loss: 1.120152e-02 | Total Loss: 4.231955e+07\n",
            "Epoch   35 | L1 Loss: 4.231949e+07 | L2 Loss: 7.327598e-02 | Total Loss: 4.231949e+07\n",
            "Epoch   40 | L1 Loss: 4.231928e+07 | L2 Loss: 4.552563e-01 | Total Loss: 4.231928e+07\n",
            "Epoch   45 | L1 Loss: 4.231874e+07 | L2 Loss: 2.610103e+00 | Total Loss: 4.231875e+07\n",
            "Epoch   50 | L1 Loss: 4.231784e+07 | L2 Loss: 1.292347e+01 | Total Loss: 4.231785e+07\n",
            "Epoch   55 | L1 Loss: 4.231666e+07 | L2 Loss: 5.019071e+01 | Total Loss: 4.231671e+07\n",
            "Epoch   60 | L1 Loss: 4.231501e+07 | L2 Loss: 1.434963e+02 | Total Loss: 4.231516e+07\n",
            "Epoch   65 | L1 Loss: 4.231256e+07 | L2 Loss: 2.047392e+02 | Total Loss: 4.231277e+07\n",
            "Epoch   70 | L1 Loss: 4.230942e+07 | L2 Loss: 2.198469e+02 | Total Loss: 4.230964e+07\n",
            "Epoch   75 | L1 Loss: 4.230538e+07 | L2 Loss: 3.082718e+02 | Total Loss: 4.230569e+07\n",
            "Epoch   80 | L1 Loss: 4.229985e+07 | L2 Loss: 5.650050e+02 | Total Loss: 4.230042e+07\n",
            "Epoch   85 | L1 Loss: 4.229354e+07 | L2 Loss: 1.014528e+03 | Total Loss: 4.229456e+07\n",
            "Epoch   90 | L1 Loss: 4.228670e+07 | L2 Loss: 2.313790e+03 | Total Loss: 4.228901e+07\n",
            "Epoch   95 | L1 Loss: 4.227580e+07 | L2 Loss: 2.473742e+03 | Total Loss: 4.227827e+07\n",
            "Processing from patch 30 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 30\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.672574e+08 | L2 Loss: 1.952696e-07 | Total Loss: 1.672574e+08\n",
            "Epoch   10 | L1 Loss: 1.672574e+08 | L2 Loss: 4.077785e-06 | Total Loss: 1.672574e+08\n",
            "Epoch   15 | L1 Loss: 1.672574e+08 | L2 Loss: 2.755348e-05 | Total Loss: 1.672574e+08\n",
            "Epoch   20 | L1 Loss: 1.672574e+08 | L2 Loss: 1.437674e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   25 | L1 Loss: 1.672574e+08 | L2 Loss: 8.701842e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   30 | L1 Loss: 1.672574e+08 | L2 Loss: 6.177267e-03 | Total Loss: 1.672574e+08\n",
            "Epoch   35 | L1 Loss: 1.672573e+08 | L2 Loss: 4.150519e-02 | Total Loss: 1.672573e+08\n",
            "Epoch   40 | L1 Loss: 1.672570e+08 | L2 Loss: 2.832601e-01 | Total Loss: 1.672570e+08\n",
            "Epoch   45 | L1 Loss: 1.672562e+08 | L2 Loss: 1.658310e+00 | Total Loss: 1.672562e+08\n",
            "Epoch   50 | L1 Loss: 1.672547e+08 | L2 Loss: 7.302703e+00 | Total Loss: 1.672547e+08\n",
            "Epoch   55 | L1 Loss: 1.672522e+08 | L2 Loss: 2.917651e+01 | Total Loss: 1.672522e+08\n",
            "Epoch   60 | L1 Loss: 1.672488e+08 | L2 Loss: 5.916130e+01 | Total Loss: 1.672489e+08\n",
            "Epoch   65 | L1 Loss: 1.672444e+08 | L2 Loss: 1.360160e+02 | Total Loss: 1.672445e+08\n",
            "Epoch   70 | L1 Loss: 1.672390e+08 | L2 Loss: 2.325296e+02 | Total Loss: 1.672393e+08\n",
            "Epoch   75 | L1 Loss: 1.672328e+08 | L2 Loss: 4.511600e+02 | Total Loss: 1.672333e+08\n",
            "Epoch   80 | L1 Loss: 1.672258e+08 | L2 Loss: 7.508612e+02 | Total Loss: 1.672266e+08\n",
            "Epoch   85 | L1 Loss: 1.672179e+08 | L2 Loss: 1.194566e+03 | Total Loss: 1.672191e+08\n",
            "Epoch   90 | L1 Loss: 1.672086e+08 | L2 Loss: 2.220778e+03 | Total Loss: 1.672108e+08\n",
            "Epoch   95 | L1 Loss: 1.671987e+08 | L2 Loss: 2.761520e+03 | Total Loss: 1.672015e+08\n",
            "\n",
            " === Training run 2/5 === Patches: 30\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.672574e+08 | L2 Loss: 1.952696e-07 | Total Loss: 1.672574e+08\n",
            "Epoch   10 | L1 Loss: 1.672574e+08 | L2 Loss: 4.077785e-06 | Total Loss: 1.672574e+08\n",
            "Epoch   15 | L1 Loss: 1.672574e+08 | L2 Loss: 2.755348e-05 | Total Loss: 1.672574e+08\n",
            "Epoch   20 | L1 Loss: 1.672574e+08 | L2 Loss: 1.437674e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   25 | L1 Loss: 1.672574e+08 | L2 Loss: 8.701842e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   30 | L1 Loss: 1.672574e+08 | L2 Loss: 6.177267e-03 | Total Loss: 1.672574e+08\n",
            "Epoch   35 | L1 Loss: 1.672573e+08 | L2 Loss: 4.150519e-02 | Total Loss: 1.672573e+08\n",
            "Epoch   40 | L1 Loss: 1.672570e+08 | L2 Loss: 2.832601e-01 | Total Loss: 1.672570e+08\n",
            "Epoch   45 | L1 Loss: 1.672562e+08 | L2 Loss: 1.658310e+00 | Total Loss: 1.672562e+08\n",
            "Epoch   50 | L1 Loss: 1.672547e+08 | L2 Loss: 7.302703e+00 | Total Loss: 1.672547e+08\n",
            "Epoch   55 | L1 Loss: 1.672522e+08 | L2 Loss: 2.917651e+01 | Total Loss: 1.672522e+08\n",
            "Epoch   60 | L1 Loss: 1.672488e+08 | L2 Loss: 5.916130e+01 | Total Loss: 1.672489e+08\n",
            "Epoch   65 | L1 Loss: 1.672444e+08 | L2 Loss: 1.360160e+02 | Total Loss: 1.672445e+08\n",
            "Epoch   70 | L1 Loss: 1.672390e+08 | L2 Loss: 2.325296e+02 | Total Loss: 1.672393e+08\n",
            "Epoch   75 | L1 Loss: 1.672328e+08 | L2 Loss: 4.511600e+02 | Total Loss: 1.672333e+08\n",
            "Epoch   80 | L1 Loss: 1.672258e+08 | L2 Loss: 7.508612e+02 | Total Loss: 1.672266e+08\n",
            "Epoch   85 | L1 Loss: 1.672179e+08 | L2 Loss: 1.194566e+03 | Total Loss: 1.672191e+08\n",
            "Epoch   90 | L1 Loss: 1.672086e+08 | L2 Loss: 2.220778e+03 | Total Loss: 1.672108e+08\n",
            "Epoch   95 | L1 Loss: 1.671987e+08 | L2 Loss: 2.761520e+03 | Total Loss: 1.672015e+08\n",
            "\n",
            " === Training run 3/5 === Patches: 30\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.672574e+08 | L2 Loss: 1.952696e-07 | Total Loss: 1.672574e+08\n",
            "Epoch   10 | L1 Loss: 1.672574e+08 | L2 Loss: 4.077785e-06 | Total Loss: 1.672574e+08\n",
            "Epoch   15 | L1 Loss: 1.672574e+08 | L2 Loss: 2.755348e-05 | Total Loss: 1.672574e+08\n",
            "Epoch   20 | L1 Loss: 1.672574e+08 | L2 Loss: 1.437674e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   25 | L1 Loss: 1.672574e+08 | L2 Loss: 8.701842e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   30 | L1 Loss: 1.672574e+08 | L2 Loss: 6.177267e-03 | Total Loss: 1.672574e+08\n",
            "Epoch   35 | L1 Loss: 1.672573e+08 | L2 Loss: 4.150519e-02 | Total Loss: 1.672573e+08\n",
            "Epoch   40 | L1 Loss: 1.672570e+08 | L2 Loss: 2.832601e-01 | Total Loss: 1.672570e+08\n",
            "Epoch   45 | L1 Loss: 1.672562e+08 | L2 Loss: 1.658310e+00 | Total Loss: 1.672562e+08\n",
            "Epoch   50 | L1 Loss: 1.672547e+08 | L2 Loss: 7.302703e+00 | Total Loss: 1.672547e+08\n",
            "Epoch   55 | L1 Loss: 1.672522e+08 | L2 Loss: 2.917651e+01 | Total Loss: 1.672522e+08\n",
            "Epoch   60 | L1 Loss: 1.672488e+08 | L2 Loss: 5.916130e+01 | Total Loss: 1.672489e+08\n",
            "Epoch   65 | L1 Loss: 1.672444e+08 | L2 Loss: 1.360160e+02 | Total Loss: 1.672445e+08\n",
            "Epoch   70 | L1 Loss: 1.672390e+08 | L2 Loss: 2.325296e+02 | Total Loss: 1.672393e+08\n",
            "Epoch   75 | L1 Loss: 1.672328e+08 | L2 Loss: 4.511600e+02 | Total Loss: 1.672333e+08\n",
            "Epoch   80 | L1 Loss: 1.672258e+08 | L2 Loss: 7.508612e+02 | Total Loss: 1.672266e+08\n",
            "Epoch   85 | L1 Loss: 1.672179e+08 | L2 Loss: 1.194566e+03 | Total Loss: 1.672191e+08\n",
            "Epoch   90 | L1 Loss: 1.672086e+08 | L2 Loss: 2.220778e+03 | Total Loss: 1.672108e+08\n",
            "Epoch   95 | L1 Loss: 1.671987e+08 | L2 Loss: 2.761520e+03 | Total Loss: 1.672015e+08\n",
            "\n",
            " === Training run 4/5 === Patches: 30\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.672574e+08 | L2 Loss: 1.952696e-07 | Total Loss: 1.672574e+08\n",
            "Epoch   10 | L1 Loss: 1.672574e+08 | L2 Loss: 4.077785e-06 | Total Loss: 1.672574e+08\n",
            "Epoch   15 | L1 Loss: 1.672574e+08 | L2 Loss: 2.755348e-05 | Total Loss: 1.672574e+08\n",
            "Epoch   20 | L1 Loss: 1.672574e+08 | L2 Loss: 1.437674e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   25 | L1 Loss: 1.672574e+08 | L2 Loss: 8.701842e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   30 | L1 Loss: 1.672574e+08 | L2 Loss: 6.177267e-03 | Total Loss: 1.672574e+08\n",
            "Epoch   35 | L1 Loss: 1.672573e+08 | L2 Loss: 4.150519e-02 | Total Loss: 1.672573e+08\n",
            "Epoch   40 | L1 Loss: 1.672570e+08 | L2 Loss: 2.832601e-01 | Total Loss: 1.672570e+08\n",
            "Epoch   45 | L1 Loss: 1.672562e+08 | L2 Loss: 1.658310e+00 | Total Loss: 1.672562e+08\n",
            "Epoch   50 | L1 Loss: 1.672547e+08 | L2 Loss: 7.302703e+00 | Total Loss: 1.672547e+08\n",
            "Epoch   55 | L1 Loss: 1.672522e+08 | L2 Loss: 2.917651e+01 | Total Loss: 1.672522e+08\n",
            "Epoch   60 | L1 Loss: 1.672488e+08 | L2 Loss: 5.916130e+01 | Total Loss: 1.672489e+08\n",
            "Epoch   65 | L1 Loss: 1.672444e+08 | L2 Loss: 1.360160e+02 | Total Loss: 1.672445e+08\n",
            "Epoch   70 | L1 Loss: 1.672390e+08 | L2 Loss: 2.325296e+02 | Total Loss: 1.672393e+08\n",
            "Epoch   75 | L1 Loss: 1.672328e+08 | L2 Loss: 4.511600e+02 | Total Loss: 1.672333e+08\n",
            "Epoch   80 | L1 Loss: 1.672258e+08 | L2 Loss: 7.508612e+02 | Total Loss: 1.672266e+08\n",
            "Epoch   85 | L1 Loss: 1.672179e+08 | L2 Loss: 1.194566e+03 | Total Loss: 1.672191e+08\n",
            "Epoch   90 | L1 Loss: 1.672086e+08 | L2 Loss: 2.220778e+03 | Total Loss: 1.672108e+08\n",
            "Epoch   95 | L1 Loss: 1.671987e+08 | L2 Loss: 2.761520e+03 | Total Loss: 1.672015e+08\n",
            "\n",
            " === Training run 5/5 === Patches: 30\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.672574e+08 | L2 Loss: 1.952696e-07 | Total Loss: 1.672574e+08\n",
            "Epoch   10 | L1 Loss: 1.672574e+08 | L2 Loss: 4.077785e-06 | Total Loss: 1.672574e+08\n",
            "Epoch   15 | L1 Loss: 1.672574e+08 | L2 Loss: 2.755348e-05 | Total Loss: 1.672574e+08\n",
            "Epoch   20 | L1 Loss: 1.672574e+08 | L2 Loss: 1.437674e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   25 | L1 Loss: 1.672574e+08 | L2 Loss: 8.701842e-04 | Total Loss: 1.672574e+08\n",
            "Epoch   30 | L1 Loss: 1.672574e+08 | L2 Loss: 6.177267e-03 | Total Loss: 1.672574e+08\n",
            "Epoch   35 | L1 Loss: 1.672573e+08 | L2 Loss: 4.150519e-02 | Total Loss: 1.672573e+08\n",
            "Epoch   40 | L1 Loss: 1.672570e+08 | L2 Loss: 2.832601e-01 | Total Loss: 1.672570e+08\n",
            "Epoch   45 | L1 Loss: 1.672562e+08 | L2 Loss: 1.658310e+00 | Total Loss: 1.672562e+08\n",
            "Epoch   50 | L1 Loss: 1.672547e+08 | L2 Loss: 7.302703e+00 | Total Loss: 1.672547e+08\n",
            "Epoch   55 | L1 Loss: 1.672522e+08 | L2 Loss: 2.917651e+01 | Total Loss: 1.672522e+08\n",
            "Epoch   60 | L1 Loss: 1.672488e+08 | L2 Loss: 5.916130e+01 | Total Loss: 1.672489e+08\n",
            "Epoch   65 | L1 Loss: 1.672444e+08 | L2 Loss: 1.360160e+02 | Total Loss: 1.672445e+08\n",
            "Epoch   70 | L1 Loss: 1.672390e+08 | L2 Loss: 2.325296e+02 | Total Loss: 1.672393e+08\n",
            "Epoch   75 | L1 Loss: 1.672328e+08 | L2 Loss: 4.511600e+02 | Total Loss: 1.672333e+08\n",
            "Epoch   80 | L1 Loss: 1.672258e+08 | L2 Loss: 7.508612e+02 | Total Loss: 1.672266e+08\n",
            "Epoch   85 | L1 Loss: 1.672179e+08 | L2 Loss: 1.194566e+03 | Total Loss: 1.672191e+08\n",
            "Epoch   90 | L1 Loss: 1.672086e+08 | L2 Loss: 2.220778e+03 | Total Loss: 1.672108e+08\n",
            "Epoch   95 | L1 Loss: 1.671987e+08 | L2 Loss: 2.761520e+03 | Total Loss: 1.672015e+08\n",
            "Processing from patch 31 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 31\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.346107e+08 | L2 Loss: 1.531527e-09 | Total Loss: 3.346107e+08\n",
            "Epoch   10 | L1 Loss: 3.346107e+08 | L2 Loss: 3.826494e-07 | Total Loss: 3.346107e+08\n",
            "Epoch   15 | L1 Loss: 3.346107e+08 | L2 Loss: 5.021798e-06 | Total Loss: 3.346107e+08\n",
            "Epoch   20 | L1 Loss: 3.346107e+08 | L2 Loss: 4.032901e-05 | Total Loss: 3.346107e+08\n",
            "Epoch   25 | L1 Loss: 3.346107e+08 | L2 Loss: 2.855309e-04 | Total Loss: 3.346107e+08\n",
            "Epoch   30 | L1 Loss: 3.346107e+08 | L2 Loss: 1.937086e-03 | Total Loss: 3.346107e+08\n",
            "Epoch   35 | L1 Loss: 3.346106e+08 | L2 Loss: 1.335236e-02 | Total Loss: 3.346106e+08\n",
            "Epoch   40 | L1 Loss: 3.346102e+08 | L2 Loss: 9.608457e-02 | Total Loss: 3.346102e+08\n",
            "Epoch   45 | L1 Loss: 3.346092e+08 | L2 Loss: 7.984875e-01 | Total Loss: 3.346092e+08\n",
            "Epoch   50 | L1 Loss: 3.346071e+08 | L2 Loss: 5.362535e+00 | Total Loss: 3.346071e+08\n",
            "Epoch   55 | L1 Loss: 3.346040e+08 | L2 Loss: 2.732310e+01 | Total Loss: 3.346041e+08\n",
            "Epoch   60 | L1 Loss: 3.346006e+08 | L2 Loss: 8.749995e+01 | Total Loss: 3.346007e+08\n",
            "Epoch   65 | L1 Loss: 3.345966e+08 | L2 Loss: 1.792797e+02 | Total Loss: 3.345968e+08\n",
            "Epoch   70 | L1 Loss: 3.345918e+08 | L2 Loss: 2.831955e+02 | Total Loss: 3.345921e+08\n",
            "Epoch   75 | L1 Loss: 3.345853e+08 | L2 Loss: 3.919277e+02 | Total Loss: 3.345857e+08\n",
            "Epoch   80 | L1 Loss: 3.345759e+08 | L2 Loss: 5.800723e+02 | Total Loss: 3.345765e+08\n",
            "Epoch   85 | L1 Loss: 3.345615e+08 | L2 Loss: 9.171431e+02 | Total Loss: 3.345624e+08\n",
            "Epoch   90 | L1 Loss: 3.345413e+08 | L2 Loss: 1.581178e+03 | Total Loss: 3.345428e+08\n",
            "Epoch   95 | L1 Loss: 3.345323e+08 | L2 Loss: 2.046229e+03 | Total Loss: 3.345344e+08\n",
            "\n",
            " === Training run 2/5 === Patches: 31\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.346107e+08 | L2 Loss: 1.531527e-09 | Total Loss: 3.346107e+08\n",
            "Epoch   10 | L1 Loss: 3.346107e+08 | L2 Loss: 3.826494e-07 | Total Loss: 3.346107e+08\n",
            "Epoch   15 | L1 Loss: 3.346107e+08 | L2 Loss: 5.021798e-06 | Total Loss: 3.346107e+08\n",
            "Epoch   20 | L1 Loss: 3.346107e+08 | L2 Loss: 4.032901e-05 | Total Loss: 3.346107e+08\n",
            "Epoch   25 | L1 Loss: 3.346107e+08 | L2 Loss: 2.855309e-04 | Total Loss: 3.346107e+08\n",
            "Epoch   30 | L1 Loss: 3.346107e+08 | L2 Loss: 1.937086e-03 | Total Loss: 3.346107e+08\n",
            "Epoch   35 | L1 Loss: 3.346106e+08 | L2 Loss: 1.335236e-02 | Total Loss: 3.346106e+08\n",
            "Epoch   40 | L1 Loss: 3.346102e+08 | L2 Loss: 9.608457e-02 | Total Loss: 3.346102e+08\n",
            "Epoch   45 | L1 Loss: 3.346092e+08 | L2 Loss: 7.984875e-01 | Total Loss: 3.346092e+08\n",
            "Epoch   50 | L1 Loss: 3.346071e+08 | L2 Loss: 5.362535e+00 | Total Loss: 3.346071e+08\n",
            "Epoch   55 | L1 Loss: 3.346040e+08 | L2 Loss: 2.732310e+01 | Total Loss: 3.346041e+08\n",
            "Epoch   60 | L1 Loss: 3.346006e+08 | L2 Loss: 8.749995e+01 | Total Loss: 3.346007e+08\n",
            "Epoch   65 | L1 Loss: 3.345966e+08 | L2 Loss: 1.792797e+02 | Total Loss: 3.345968e+08\n",
            "Epoch   70 | L1 Loss: 3.345918e+08 | L2 Loss: 2.831955e+02 | Total Loss: 3.345921e+08\n",
            "Epoch   75 | L1 Loss: 3.345853e+08 | L2 Loss: 3.919277e+02 | Total Loss: 3.345857e+08\n",
            "Epoch   80 | L1 Loss: 3.345759e+08 | L2 Loss: 5.800723e+02 | Total Loss: 3.345765e+08\n",
            "Epoch   85 | L1 Loss: 3.345615e+08 | L2 Loss: 9.171431e+02 | Total Loss: 3.345624e+08\n",
            "Epoch   90 | L1 Loss: 3.345413e+08 | L2 Loss: 1.581178e+03 | Total Loss: 3.345428e+08\n",
            "Epoch   95 | L1 Loss: 3.345323e+08 | L2 Loss: 2.046229e+03 | Total Loss: 3.345344e+08\n",
            "\n",
            " === Training run 3/5 === Patches: 31\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.346107e+08 | L2 Loss: 1.531527e-09 | Total Loss: 3.346107e+08\n",
            "Epoch   10 | L1 Loss: 3.346107e+08 | L2 Loss: 3.826494e-07 | Total Loss: 3.346107e+08\n",
            "Epoch   15 | L1 Loss: 3.346107e+08 | L2 Loss: 5.021798e-06 | Total Loss: 3.346107e+08\n",
            "Epoch   20 | L1 Loss: 3.346107e+08 | L2 Loss: 4.032901e-05 | Total Loss: 3.346107e+08\n",
            "Epoch   25 | L1 Loss: 3.346107e+08 | L2 Loss: 2.855309e-04 | Total Loss: 3.346107e+08\n",
            "Epoch   30 | L1 Loss: 3.346107e+08 | L2 Loss: 1.937086e-03 | Total Loss: 3.346107e+08\n",
            "Epoch   35 | L1 Loss: 3.346106e+08 | L2 Loss: 1.335236e-02 | Total Loss: 3.346106e+08\n",
            "Epoch   40 | L1 Loss: 3.346102e+08 | L2 Loss: 9.608457e-02 | Total Loss: 3.346102e+08\n",
            "Epoch   45 | L1 Loss: 3.346092e+08 | L2 Loss: 7.984875e-01 | Total Loss: 3.346092e+08\n",
            "Epoch   50 | L1 Loss: 3.346071e+08 | L2 Loss: 5.362535e+00 | Total Loss: 3.346071e+08\n",
            "Epoch   55 | L1 Loss: 3.346040e+08 | L2 Loss: 2.732310e+01 | Total Loss: 3.346041e+08\n",
            "Epoch   60 | L1 Loss: 3.346006e+08 | L2 Loss: 8.749995e+01 | Total Loss: 3.346007e+08\n",
            "Epoch   65 | L1 Loss: 3.345966e+08 | L2 Loss: 1.792797e+02 | Total Loss: 3.345968e+08\n",
            "Epoch   70 | L1 Loss: 3.345918e+08 | L2 Loss: 2.831955e+02 | Total Loss: 3.345921e+08\n",
            "Epoch   75 | L1 Loss: 3.345853e+08 | L2 Loss: 3.919277e+02 | Total Loss: 3.345857e+08\n",
            "Epoch   80 | L1 Loss: 3.345759e+08 | L2 Loss: 5.800723e+02 | Total Loss: 3.345765e+08\n",
            "Epoch   85 | L1 Loss: 3.345615e+08 | L2 Loss: 9.171431e+02 | Total Loss: 3.345624e+08\n",
            "Epoch   90 | L1 Loss: 3.345413e+08 | L2 Loss: 1.581178e+03 | Total Loss: 3.345428e+08\n",
            "Epoch   95 | L1 Loss: 3.345323e+08 | L2 Loss: 2.046229e+03 | Total Loss: 3.345344e+08\n",
            "\n",
            " === Training run 4/5 === Patches: 31\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.346107e+08 | L2 Loss: 1.531527e-09 | Total Loss: 3.346107e+08\n",
            "Epoch   10 | L1 Loss: 3.346107e+08 | L2 Loss: 3.826494e-07 | Total Loss: 3.346107e+08\n",
            "Epoch   15 | L1 Loss: 3.346107e+08 | L2 Loss: 5.021798e-06 | Total Loss: 3.346107e+08\n",
            "Epoch   20 | L1 Loss: 3.346107e+08 | L2 Loss: 4.032901e-05 | Total Loss: 3.346107e+08\n",
            "Epoch   25 | L1 Loss: 3.346107e+08 | L2 Loss: 2.855309e-04 | Total Loss: 3.346107e+08\n",
            "Epoch   30 | L1 Loss: 3.346107e+08 | L2 Loss: 1.937086e-03 | Total Loss: 3.346107e+08\n",
            "Epoch   35 | L1 Loss: 3.346106e+08 | L2 Loss: 1.335236e-02 | Total Loss: 3.346106e+08\n",
            "Epoch   40 | L1 Loss: 3.346102e+08 | L2 Loss: 9.608457e-02 | Total Loss: 3.346102e+08\n",
            "Epoch   45 | L1 Loss: 3.346092e+08 | L2 Loss: 7.984875e-01 | Total Loss: 3.346092e+08\n",
            "Epoch   50 | L1 Loss: 3.346071e+08 | L2 Loss: 5.362535e+00 | Total Loss: 3.346071e+08\n",
            "Epoch   55 | L1 Loss: 3.346040e+08 | L2 Loss: 2.732310e+01 | Total Loss: 3.346041e+08\n",
            "Epoch   60 | L1 Loss: 3.346006e+08 | L2 Loss: 8.749995e+01 | Total Loss: 3.346007e+08\n",
            "Epoch   65 | L1 Loss: 3.345966e+08 | L2 Loss: 1.792797e+02 | Total Loss: 3.345968e+08\n",
            "Epoch   70 | L1 Loss: 3.345918e+08 | L2 Loss: 2.831955e+02 | Total Loss: 3.345921e+08\n",
            "Epoch   75 | L1 Loss: 3.345853e+08 | L2 Loss: 3.919277e+02 | Total Loss: 3.345857e+08\n",
            "Epoch   80 | L1 Loss: 3.345759e+08 | L2 Loss: 5.800723e+02 | Total Loss: 3.345765e+08\n",
            "Epoch   85 | L1 Loss: 3.345615e+08 | L2 Loss: 9.171431e+02 | Total Loss: 3.345624e+08\n",
            "Epoch   90 | L1 Loss: 3.345413e+08 | L2 Loss: 1.581178e+03 | Total Loss: 3.345428e+08\n",
            "Epoch   95 | L1 Loss: 3.345323e+08 | L2 Loss: 2.046229e+03 | Total Loss: 3.345344e+08\n",
            "\n",
            " === Training run 5/5 === Patches: 31\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.346107e+08 | L2 Loss: 1.531527e-09 | Total Loss: 3.346107e+08\n",
            "Epoch   10 | L1 Loss: 3.346107e+08 | L2 Loss: 3.826494e-07 | Total Loss: 3.346107e+08\n",
            "Epoch   15 | L1 Loss: 3.346107e+08 | L2 Loss: 5.021798e-06 | Total Loss: 3.346107e+08\n",
            "Epoch   20 | L1 Loss: 3.346107e+08 | L2 Loss: 4.032901e-05 | Total Loss: 3.346107e+08\n",
            "Epoch   25 | L1 Loss: 3.346107e+08 | L2 Loss: 2.855309e-04 | Total Loss: 3.346107e+08\n",
            "Epoch   30 | L1 Loss: 3.346107e+08 | L2 Loss: 1.937086e-03 | Total Loss: 3.346107e+08\n",
            "Epoch   35 | L1 Loss: 3.346106e+08 | L2 Loss: 1.335236e-02 | Total Loss: 3.346106e+08\n",
            "Epoch   40 | L1 Loss: 3.346102e+08 | L2 Loss: 9.608457e-02 | Total Loss: 3.346102e+08\n",
            "Epoch   45 | L1 Loss: 3.346092e+08 | L2 Loss: 7.984875e-01 | Total Loss: 3.346092e+08\n",
            "Epoch   50 | L1 Loss: 3.346071e+08 | L2 Loss: 5.362535e+00 | Total Loss: 3.346071e+08\n",
            "Epoch   55 | L1 Loss: 3.346040e+08 | L2 Loss: 2.732310e+01 | Total Loss: 3.346041e+08\n",
            "Epoch   60 | L1 Loss: 3.346006e+08 | L2 Loss: 8.749995e+01 | Total Loss: 3.346007e+08\n",
            "Epoch   65 | L1 Loss: 3.345966e+08 | L2 Loss: 1.792797e+02 | Total Loss: 3.345968e+08\n",
            "Epoch   70 | L1 Loss: 3.345918e+08 | L2 Loss: 2.831955e+02 | Total Loss: 3.345921e+08\n",
            "Epoch   75 | L1 Loss: 3.345853e+08 | L2 Loss: 3.919277e+02 | Total Loss: 3.345857e+08\n",
            "Epoch   80 | L1 Loss: 3.345759e+08 | L2 Loss: 5.800723e+02 | Total Loss: 3.345765e+08\n",
            "Epoch   85 | L1 Loss: 3.345615e+08 | L2 Loss: 9.171431e+02 | Total Loss: 3.345624e+08\n",
            "Epoch   90 | L1 Loss: 3.345413e+08 | L2 Loss: 1.581178e+03 | Total Loss: 3.345428e+08\n",
            "Epoch   95 | L1 Loss: 3.345323e+08 | L2 Loss: 2.046229e+03 | Total Loss: 3.345344e+08\n",
            "Processing from patch 32 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 32\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.536458e+08 | L2 Loss: 5.519427e-08 | Total Loss: 1.536458e+08\n",
            "Epoch   10 | L1 Loss: 1.536458e+08 | L2 Loss: 1.359353e-06 | Total Loss: 1.536458e+08\n",
            "Epoch   15 | L1 Loss: 1.536458e+08 | L2 Loss: 1.384467e-05 | Total Loss: 1.536458e+08\n",
            "Epoch   20 | L1 Loss: 1.536458e+08 | L2 Loss: 1.068012e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   25 | L1 Loss: 1.536458e+08 | L2 Loss: 6.848439e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   30 | L1 Loss: 1.536458e+08 | L2 Loss: 3.948580e-03 | Total Loss: 1.536458e+08\n",
            "Epoch   35 | L1 Loss: 1.536458e+08 | L2 Loss: 2.041669e-02 | Total Loss: 1.536458e+08\n",
            "Epoch   40 | L1 Loss: 1.536456e+08 | L2 Loss: 1.005105e-01 | Total Loss: 1.536456e+08\n",
            "Epoch   45 | L1 Loss: 1.536451e+08 | L2 Loss: 4.713157e-01 | Total Loss: 1.536451e+08\n",
            "Epoch   50 | L1 Loss: 1.536443e+08 | L2 Loss: 1.834149e+00 | Total Loss: 1.536443e+08\n",
            "Epoch   55 | L1 Loss: 1.536432e+08 | L2 Loss: 6.566463e+00 | Total Loss: 1.536432e+08\n",
            "Epoch   60 | L1 Loss: 1.536416e+08 | L2 Loss: 1.974233e+01 | Total Loss: 1.536417e+08\n",
            "Epoch   65 | L1 Loss: 1.536394e+08 | L2 Loss: 4.208269e+01 | Total Loss: 1.536394e+08\n",
            "Epoch   70 | L1 Loss: 1.536356e+08 | L2 Loss: 6.141566e+01 | Total Loss: 1.536357e+08\n",
            "Epoch   75 | L1 Loss: 1.536305e+08 | L2 Loss: 7.784128e+01 | Total Loss: 1.536306e+08\n",
            "Epoch   80 | L1 Loss: 1.536237e+08 | L2 Loss: 1.590444e+02 | Total Loss: 1.536239e+08\n",
            "Epoch   85 | L1 Loss: 1.536154e+08 | L2 Loss: 2.671004e+02 | Total Loss: 1.536157e+08\n",
            "Epoch   90 | L1 Loss: 1.536065e+08 | L2 Loss: 3.588015e+02 | Total Loss: 1.536068e+08\n",
            "Epoch   95 | L1 Loss: 1.535967e+08 | L2 Loss: 4.222580e+02 | Total Loss: 1.535971e+08\n",
            "\n",
            " === Training run 2/5 === Patches: 32\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.536458e+08 | L2 Loss: 5.519427e-08 | Total Loss: 1.536458e+08\n",
            "Epoch   10 | L1 Loss: 1.536458e+08 | L2 Loss: 1.359353e-06 | Total Loss: 1.536458e+08\n",
            "Epoch   15 | L1 Loss: 1.536458e+08 | L2 Loss: 1.384467e-05 | Total Loss: 1.536458e+08\n",
            "Epoch   20 | L1 Loss: 1.536458e+08 | L2 Loss: 1.068012e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   25 | L1 Loss: 1.536458e+08 | L2 Loss: 6.848439e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   30 | L1 Loss: 1.536458e+08 | L2 Loss: 3.948580e-03 | Total Loss: 1.536458e+08\n",
            "Epoch   35 | L1 Loss: 1.536458e+08 | L2 Loss: 2.041669e-02 | Total Loss: 1.536458e+08\n",
            "Epoch   40 | L1 Loss: 1.536456e+08 | L2 Loss: 1.005105e-01 | Total Loss: 1.536456e+08\n",
            "Epoch   45 | L1 Loss: 1.536451e+08 | L2 Loss: 4.713157e-01 | Total Loss: 1.536451e+08\n",
            "Epoch   50 | L1 Loss: 1.536443e+08 | L2 Loss: 1.834149e+00 | Total Loss: 1.536443e+08\n",
            "Epoch   55 | L1 Loss: 1.536432e+08 | L2 Loss: 6.566463e+00 | Total Loss: 1.536432e+08\n",
            "Epoch   60 | L1 Loss: 1.536416e+08 | L2 Loss: 1.974233e+01 | Total Loss: 1.536417e+08\n",
            "Epoch   65 | L1 Loss: 1.536394e+08 | L2 Loss: 4.208269e+01 | Total Loss: 1.536394e+08\n",
            "Epoch   70 | L1 Loss: 1.536356e+08 | L2 Loss: 6.141566e+01 | Total Loss: 1.536357e+08\n",
            "Epoch   75 | L1 Loss: 1.536305e+08 | L2 Loss: 7.784128e+01 | Total Loss: 1.536306e+08\n",
            "Epoch   80 | L1 Loss: 1.536237e+08 | L2 Loss: 1.590444e+02 | Total Loss: 1.536239e+08\n",
            "Epoch   85 | L1 Loss: 1.536154e+08 | L2 Loss: 2.671004e+02 | Total Loss: 1.536157e+08\n",
            "Epoch   90 | L1 Loss: 1.536065e+08 | L2 Loss: 3.588015e+02 | Total Loss: 1.536068e+08\n",
            "Epoch   95 | L1 Loss: 1.535967e+08 | L2 Loss: 4.222580e+02 | Total Loss: 1.535971e+08\n",
            "\n",
            " === Training run 3/5 === Patches: 32\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.536458e+08 | L2 Loss: 5.519427e-08 | Total Loss: 1.536458e+08\n",
            "Epoch   10 | L1 Loss: 1.536458e+08 | L2 Loss: 1.359353e-06 | Total Loss: 1.536458e+08\n",
            "Epoch   15 | L1 Loss: 1.536458e+08 | L2 Loss: 1.384467e-05 | Total Loss: 1.536458e+08\n",
            "Epoch   20 | L1 Loss: 1.536458e+08 | L2 Loss: 1.068012e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   25 | L1 Loss: 1.536458e+08 | L2 Loss: 6.848439e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   30 | L1 Loss: 1.536458e+08 | L2 Loss: 3.948580e-03 | Total Loss: 1.536458e+08\n",
            "Epoch   35 | L1 Loss: 1.536458e+08 | L2 Loss: 2.041669e-02 | Total Loss: 1.536458e+08\n",
            "Epoch   40 | L1 Loss: 1.536456e+08 | L2 Loss: 1.005105e-01 | Total Loss: 1.536456e+08\n",
            "Epoch   45 | L1 Loss: 1.536451e+08 | L2 Loss: 4.713157e-01 | Total Loss: 1.536451e+08\n",
            "Epoch   50 | L1 Loss: 1.536443e+08 | L2 Loss: 1.834149e+00 | Total Loss: 1.536443e+08\n",
            "Epoch   55 | L1 Loss: 1.536432e+08 | L2 Loss: 6.566463e+00 | Total Loss: 1.536432e+08\n",
            "Epoch   60 | L1 Loss: 1.536416e+08 | L2 Loss: 1.974233e+01 | Total Loss: 1.536417e+08\n",
            "Epoch   65 | L1 Loss: 1.536394e+08 | L2 Loss: 4.208269e+01 | Total Loss: 1.536394e+08\n",
            "Epoch   70 | L1 Loss: 1.536356e+08 | L2 Loss: 6.141566e+01 | Total Loss: 1.536357e+08\n",
            "Epoch   75 | L1 Loss: 1.536305e+08 | L2 Loss: 7.784128e+01 | Total Loss: 1.536306e+08\n",
            "Epoch   80 | L1 Loss: 1.536237e+08 | L2 Loss: 1.590444e+02 | Total Loss: 1.536239e+08\n",
            "Epoch   85 | L1 Loss: 1.536154e+08 | L2 Loss: 2.671004e+02 | Total Loss: 1.536157e+08\n",
            "Epoch   90 | L1 Loss: 1.536065e+08 | L2 Loss: 3.588015e+02 | Total Loss: 1.536068e+08\n",
            "Epoch   95 | L1 Loss: 1.535967e+08 | L2 Loss: 4.222580e+02 | Total Loss: 1.535971e+08\n",
            "\n",
            " === Training run 4/5 === Patches: 32\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.536458e+08 | L2 Loss: 5.519427e-08 | Total Loss: 1.536458e+08\n",
            "Epoch   10 | L1 Loss: 1.536458e+08 | L2 Loss: 1.359353e-06 | Total Loss: 1.536458e+08\n",
            "Epoch   15 | L1 Loss: 1.536458e+08 | L2 Loss: 1.384467e-05 | Total Loss: 1.536458e+08\n",
            "Epoch   20 | L1 Loss: 1.536458e+08 | L2 Loss: 1.068012e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   25 | L1 Loss: 1.536458e+08 | L2 Loss: 6.848439e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   30 | L1 Loss: 1.536458e+08 | L2 Loss: 3.948580e-03 | Total Loss: 1.536458e+08\n",
            "Epoch   35 | L1 Loss: 1.536458e+08 | L2 Loss: 2.041669e-02 | Total Loss: 1.536458e+08\n",
            "Epoch   40 | L1 Loss: 1.536456e+08 | L2 Loss: 1.005105e-01 | Total Loss: 1.536456e+08\n",
            "Epoch   45 | L1 Loss: 1.536451e+08 | L2 Loss: 4.713157e-01 | Total Loss: 1.536451e+08\n",
            "Epoch   50 | L1 Loss: 1.536443e+08 | L2 Loss: 1.834149e+00 | Total Loss: 1.536443e+08\n",
            "Epoch   55 | L1 Loss: 1.536432e+08 | L2 Loss: 6.566463e+00 | Total Loss: 1.536432e+08\n",
            "Epoch   60 | L1 Loss: 1.536416e+08 | L2 Loss: 1.974233e+01 | Total Loss: 1.536417e+08\n",
            "Epoch   65 | L1 Loss: 1.536394e+08 | L2 Loss: 4.208269e+01 | Total Loss: 1.536394e+08\n",
            "Epoch   70 | L1 Loss: 1.536356e+08 | L2 Loss: 6.141566e+01 | Total Loss: 1.536357e+08\n",
            "Epoch   75 | L1 Loss: 1.536305e+08 | L2 Loss: 7.784128e+01 | Total Loss: 1.536306e+08\n",
            "Epoch   80 | L1 Loss: 1.536237e+08 | L2 Loss: 1.590444e+02 | Total Loss: 1.536239e+08\n",
            "Epoch   85 | L1 Loss: 1.536154e+08 | L2 Loss: 2.671004e+02 | Total Loss: 1.536157e+08\n",
            "Epoch   90 | L1 Loss: 1.536065e+08 | L2 Loss: 3.588015e+02 | Total Loss: 1.536068e+08\n",
            "Epoch   95 | L1 Loss: 1.535967e+08 | L2 Loss: 4.222580e+02 | Total Loss: 1.535971e+08\n",
            "\n",
            " === Training run 5/5 === Patches: 32\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 1.536458e+08 | L2 Loss: 5.519427e-08 | Total Loss: 1.536458e+08\n",
            "Epoch   10 | L1 Loss: 1.536458e+08 | L2 Loss: 1.359353e-06 | Total Loss: 1.536458e+08\n",
            "Epoch   15 | L1 Loss: 1.536458e+08 | L2 Loss: 1.384467e-05 | Total Loss: 1.536458e+08\n",
            "Epoch   20 | L1 Loss: 1.536458e+08 | L2 Loss: 1.068012e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   25 | L1 Loss: 1.536458e+08 | L2 Loss: 6.848439e-04 | Total Loss: 1.536458e+08\n",
            "Epoch   30 | L1 Loss: 1.536458e+08 | L2 Loss: 3.948580e-03 | Total Loss: 1.536458e+08\n",
            "Epoch   35 | L1 Loss: 1.536458e+08 | L2 Loss: 2.041669e-02 | Total Loss: 1.536458e+08\n",
            "Epoch   40 | L1 Loss: 1.536456e+08 | L2 Loss: 1.005105e-01 | Total Loss: 1.536456e+08\n",
            "Epoch   45 | L1 Loss: 1.536451e+08 | L2 Loss: 4.713157e-01 | Total Loss: 1.536451e+08\n",
            "Epoch   50 | L1 Loss: 1.536443e+08 | L2 Loss: 1.834149e+00 | Total Loss: 1.536443e+08\n",
            "Epoch   55 | L1 Loss: 1.536432e+08 | L2 Loss: 6.566463e+00 | Total Loss: 1.536432e+08\n",
            "Epoch   60 | L1 Loss: 1.536416e+08 | L2 Loss: 1.974233e+01 | Total Loss: 1.536417e+08\n",
            "Epoch   65 | L1 Loss: 1.536394e+08 | L2 Loss: 4.208269e+01 | Total Loss: 1.536394e+08\n",
            "Epoch   70 | L1 Loss: 1.536356e+08 | L2 Loss: 6.141566e+01 | Total Loss: 1.536357e+08\n",
            "Epoch   75 | L1 Loss: 1.536305e+08 | L2 Loss: 7.784128e+01 | Total Loss: 1.536306e+08\n",
            "Epoch   80 | L1 Loss: 1.536237e+08 | L2 Loss: 1.590444e+02 | Total Loss: 1.536239e+08\n",
            "Epoch   85 | L1 Loss: 1.536154e+08 | L2 Loss: 2.671004e+02 | Total Loss: 1.536157e+08\n",
            "Epoch   90 | L1 Loss: 1.536065e+08 | L2 Loss: 3.588015e+02 | Total Loss: 1.536068e+08\n",
            "Epoch   95 | L1 Loss: 1.535967e+08 | L2 Loss: 4.222580e+02 | Total Loss: 1.535971e+08\n",
            "Processing from patch 33 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 33\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 2.972776e+08 | L2 Loss: 1.042685e-07 | Total Loss: 2.972776e+08\n",
            "Epoch   10 | L1 Loss: 2.972776e+08 | L2 Loss: 2.016516e-06 | Total Loss: 2.972776e+08\n",
            "Epoch   15 | L1 Loss: 2.972776e+08 | L2 Loss: 1.400403e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   20 | L1 Loss: 2.972776e+08 | L2 Loss: 8.682287e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   25 | L1 Loss: 2.972776e+08 | L2 Loss: 5.902774e-04 | Total Loss: 2.972776e+08\n",
            "Epoch   30 | L1 Loss: 2.972776e+08 | L2 Loss: 3.463862e-03 | Total Loss: 2.972776e+08\n",
            "Epoch   35 | L1 Loss: 2.972774e+08 | L2 Loss: 2.218103e-02 | Total Loss: 2.972774e+08\n",
            "Epoch   40 | L1 Loss: 2.972771e+08 | L2 Loss: 1.368625e-01 | Total Loss: 2.972771e+08\n",
            "Epoch   45 | L1 Loss: 2.972763e+08 | L2 Loss: 5.924568e-01 | Total Loss: 2.972763e+08\n",
            "Epoch   50 | L1 Loss: 2.972749e+08 | L2 Loss: 2.898727e+00 | Total Loss: 2.972749e+08\n",
            "Epoch   55 | L1 Loss: 2.972725e+08 | L2 Loss: 7.225563e+00 | Total Loss: 2.972725e+08\n",
            "Epoch   60 | L1 Loss: 2.972685e+08 | L2 Loss: 2.531584e+01 | Total Loss: 2.972685e+08\n",
            "Epoch   65 | L1 Loss: 2.972625e+08 | L2 Loss: 6.821921e+01 | Total Loss: 2.972626e+08\n",
            "Epoch   70 | L1 Loss: 2.972540e+08 | L2 Loss: 1.609245e+02 | Total Loss: 2.972542e+08\n",
            "Epoch   75 | L1 Loss: 2.972428e+08 | L2 Loss: 2.972014e+02 | Total Loss: 2.972431e+08\n",
            "Epoch   80 | L1 Loss: 2.972298e+08 | L2 Loss: 4.237312e+02 | Total Loss: 2.972302e+08\n",
            "Epoch   85 | L1 Loss: 2.972159e+08 | L2 Loss: 6.409291e+02 | Total Loss: 2.972165e+08\n",
            "Epoch   90 | L1 Loss: 2.972021e+08 | L2 Loss: 9.381834e+02 | Total Loss: 2.972030e+08\n",
            "Epoch   95 | L1 Loss: 2.971846e+08 | L2 Loss: 1.457230e+03 | Total Loss: 2.971861e+08\n",
            "\n",
            " === Training run 2/5 === Patches: 33\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 2.972776e+08 | L2 Loss: 1.042685e-07 | Total Loss: 2.972776e+08\n",
            "Epoch   10 | L1 Loss: 2.972776e+08 | L2 Loss: 2.016516e-06 | Total Loss: 2.972776e+08\n",
            "Epoch   15 | L1 Loss: 2.972776e+08 | L2 Loss: 1.400403e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   20 | L1 Loss: 2.972776e+08 | L2 Loss: 8.682287e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   25 | L1 Loss: 2.972776e+08 | L2 Loss: 5.902774e-04 | Total Loss: 2.972776e+08\n",
            "Epoch   30 | L1 Loss: 2.972776e+08 | L2 Loss: 3.463862e-03 | Total Loss: 2.972776e+08\n",
            "Epoch   35 | L1 Loss: 2.972774e+08 | L2 Loss: 2.218103e-02 | Total Loss: 2.972774e+08\n",
            "Epoch   40 | L1 Loss: 2.972771e+08 | L2 Loss: 1.368625e-01 | Total Loss: 2.972771e+08\n",
            "Epoch   45 | L1 Loss: 2.972763e+08 | L2 Loss: 5.924568e-01 | Total Loss: 2.972763e+08\n",
            "Epoch   50 | L1 Loss: 2.972749e+08 | L2 Loss: 2.898727e+00 | Total Loss: 2.972749e+08\n",
            "Epoch   55 | L1 Loss: 2.972725e+08 | L2 Loss: 7.225563e+00 | Total Loss: 2.972725e+08\n",
            "Epoch   60 | L1 Loss: 2.972685e+08 | L2 Loss: 2.531584e+01 | Total Loss: 2.972685e+08\n",
            "Epoch   65 | L1 Loss: 2.972625e+08 | L2 Loss: 6.821921e+01 | Total Loss: 2.972626e+08\n",
            "Epoch   70 | L1 Loss: 2.972540e+08 | L2 Loss: 1.609245e+02 | Total Loss: 2.972542e+08\n",
            "Epoch   75 | L1 Loss: 2.972428e+08 | L2 Loss: 2.972014e+02 | Total Loss: 2.972431e+08\n",
            "Epoch   80 | L1 Loss: 2.972298e+08 | L2 Loss: 4.237312e+02 | Total Loss: 2.972302e+08\n",
            "Epoch   85 | L1 Loss: 2.972159e+08 | L2 Loss: 6.409291e+02 | Total Loss: 2.972165e+08\n",
            "Epoch   90 | L1 Loss: 2.972021e+08 | L2 Loss: 9.381834e+02 | Total Loss: 2.972030e+08\n",
            "Epoch   95 | L1 Loss: 2.971846e+08 | L2 Loss: 1.457230e+03 | Total Loss: 2.971861e+08\n",
            "\n",
            " === Training run 3/5 === Patches: 33\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 2.972776e+08 | L2 Loss: 1.042685e-07 | Total Loss: 2.972776e+08\n",
            "Epoch   10 | L1 Loss: 2.972776e+08 | L2 Loss: 2.016516e-06 | Total Loss: 2.972776e+08\n",
            "Epoch   15 | L1 Loss: 2.972776e+08 | L2 Loss: 1.400403e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   20 | L1 Loss: 2.972776e+08 | L2 Loss: 8.682287e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   25 | L1 Loss: 2.972776e+08 | L2 Loss: 5.902774e-04 | Total Loss: 2.972776e+08\n",
            "Epoch   30 | L1 Loss: 2.972776e+08 | L2 Loss: 3.463862e-03 | Total Loss: 2.972776e+08\n",
            "Epoch   35 | L1 Loss: 2.972774e+08 | L2 Loss: 2.218103e-02 | Total Loss: 2.972774e+08\n",
            "Epoch   40 | L1 Loss: 2.972771e+08 | L2 Loss: 1.368625e-01 | Total Loss: 2.972771e+08\n",
            "Epoch   45 | L1 Loss: 2.972763e+08 | L2 Loss: 5.924568e-01 | Total Loss: 2.972763e+08\n",
            "Epoch   50 | L1 Loss: 2.972749e+08 | L2 Loss: 2.898727e+00 | Total Loss: 2.972749e+08\n",
            "Epoch   55 | L1 Loss: 2.972725e+08 | L2 Loss: 7.225563e+00 | Total Loss: 2.972725e+08\n",
            "Epoch   60 | L1 Loss: 2.972685e+08 | L2 Loss: 2.531584e+01 | Total Loss: 2.972685e+08\n",
            "Epoch   65 | L1 Loss: 2.972625e+08 | L2 Loss: 6.821921e+01 | Total Loss: 2.972626e+08\n",
            "Epoch   70 | L1 Loss: 2.972540e+08 | L2 Loss: 1.609245e+02 | Total Loss: 2.972542e+08\n",
            "Epoch   75 | L1 Loss: 2.972428e+08 | L2 Loss: 2.972014e+02 | Total Loss: 2.972431e+08\n",
            "Epoch   80 | L1 Loss: 2.972298e+08 | L2 Loss: 4.237312e+02 | Total Loss: 2.972302e+08\n",
            "Epoch   85 | L1 Loss: 2.972159e+08 | L2 Loss: 6.409291e+02 | Total Loss: 2.972165e+08\n",
            "Epoch   90 | L1 Loss: 2.972021e+08 | L2 Loss: 9.381834e+02 | Total Loss: 2.972030e+08\n",
            "Epoch   95 | L1 Loss: 2.971846e+08 | L2 Loss: 1.457230e+03 | Total Loss: 2.971861e+08\n",
            "\n",
            " === Training run 4/5 === Patches: 33\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 2.972776e+08 | L2 Loss: 1.042685e-07 | Total Loss: 2.972776e+08\n",
            "Epoch   10 | L1 Loss: 2.972776e+08 | L2 Loss: 2.016516e-06 | Total Loss: 2.972776e+08\n",
            "Epoch   15 | L1 Loss: 2.972776e+08 | L2 Loss: 1.400403e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   20 | L1 Loss: 2.972776e+08 | L2 Loss: 8.682287e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   25 | L1 Loss: 2.972776e+08 | L2 Loss: 5.902774e-04 | Total Loss: 2.972776e+08\n",
            "Epoch   30 | L1 Loss: 2.972776e+08 | L2 Loss: 3.463862e-03 | Total Loss: 2.972776e+08\n",
            "Epoch   35 | L1 Loss: 2.972774e+08 | L2 Loss: 2.218103e-02 | Total Loss: 2.972774e+08\n",
            "Epoch   40 | L1 Loss: 2.972771e+08 | L2 Loss: 1.368625e-01 | Total Loss: 2.972771e+08\n",
            "Epoch   45 | L1 Loss: 2.972763e+08 | L2 Loss: 5.924568e-01 | Total Loss: 2.972763e+08\n",
            "Epoch   50 | L1 Loss: 2.972749e+08 | L2 Loss: 2.898727e+00 | Total Loss: 2.972749e+08\n",
            "Epoch   55 | L1 Loss: 2.972725e+08 | L2 Loss: 7.225563e+00 | Total Loss: 2.972725e+08\n",
            "Epoch   60 | L1 Loss: 2.972685e+08 | L2 Loss: 2.531584e+01 | Total Loss: 2.972685e+08\n",
            "Epoch   65 | L1 Loss: 2.972625e+08 | L2 Loss: 6.821921e+01 | Total Loss: 2.972626e+08\n",
            "Epoch   70 | L1 Loss: 2.972540e+08 | L2 Loss: 1.609245e+02 | Total Loss: 2.972542e+08\n",
            "Epoch   75 | L1 Loss: 2.972428e+08 | L2 Loss: 2.972014e+02 | Total Loss: 2.972431e+08\n",
            "Epoch   80 | L1 Loss: 2.972298e+08 | L2 Loss: 4.237312e+02 | Total Loss: 2.972302e+08\n",
            "Epoch   85 | L1 Loss: 2.972159e+08 | L2 Loss: 6.409291e+02 | Total Loss: 2.972165e+08\n",
            "Epoch   90 | L1 Loss: 2.972021e+08 | L2 Loss: 9.381834e+02 | Total Loss: 2.972030e+08\n",
            "Epoch   95 | L1 Loss: 2.971846e+08 | L2 Loss: 1.457230e+03 | Total Loss: 2.971861e+08\n",
            "\n",
            " === Training run 5/5 === Patches: 33\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 2.972776e+08 | L2 Loss: 1.042685e-07 | Total Loss: 2.972776e+08\n",
            "Epoch   10 | L1 Loss: 2.972776e+08 | L2 Loss: 2.016516e-06 | Total Loss: 2.972776e+08\n",
            "Epoch   15 | L1 Loss: 2.972776e+08 | L2 Loss: 1.400403e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   20 | L1 Loss: 2.972776e+08 | L2 Loss: 8.682287e-05 | Total Loss: 2.972776e+08\n",
            "Epoch   25 | L1 Loss: 2.972776e+08 | L2 Loss: 5.902774e-04 | Total Loss: 2.972776e+08\n",
            "Epoch   30 | L1 Loss: 2.972776e+08 | L2 Loss: 3.463862e-03 | Total Loss: 2.972776e+08\n",
            "Epoch   35 | L1 Loss: 2.972774e+08 | L2 Loss: 2.218103e-02 | Total Loss: 2.972774e+08\n",
            "Epoch   40 | L1 Loss: 2.972771e+08 | L2 Loss: 1.368625e-01 | Total Loss: 2.972771e+08\n",
            "Epoch   45 | L1 Loss: 2.972763e+08 | L2 Loss: 5.924568e-01 | Total Loss: 2.972763e+08\n",
            "Epoch   50 | L1 Loss: 2.972749e+08 | L2 Loss: 2.898727e+00 | Total Loss: 2.972749e+08\n",
            "Epoch   55 | L1 Loss: 2.972725e+08 | L2 Loss: 7.225563e+00 | Total Loss: 2.972725e+08\n",
            "Epoch   60 | L1 Loss: 2.972685e+08 | L2 Loss: 2.531584e+01 | Total Loss: 2.972685e+08\n",
            "Epoch   65 | L1 Loss: 2.972625e+08 | L2 Loss: 6.821921e+01 | Total Loss: 2.972626e+08\n",
            "Epoch   70 | L1 Loss: 2.972540e+08 | L2 Loss: 1.609245e+02 | Total Loss: 2.972542e+08\n",
            "Epoch   75 | L1 Loss: 2.972428e+08 | L2 Loss: 2.972014e+02 | Total Loss: 2.972431e+08\n",
            "Epoch   80 | L1 Loss: 2.972298e+08 | L2 Loss: 4.237312e+02 | Total Loss: 2.972302e+08\n",
            "Epoch   85 | L1 Loss: 2.972159e+08 | L2 Loss: 6.409291e+02 | Total Loss: 2.972165e+08\n",
            "Epoch   90 | L1 Loss: 2.972021e+08 | L2 Loss: 9.381834e+02 | Total Loss: 2.972030e+08\n",
            "Epoch   95 | L1 Loss: 2.971846e+08 | L2 Loss: 1.457230e+03 | Total Loss: 2.971861e+08\n",
            "Processing from patch 34 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 34\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.865471e+08 | L2 Loss: 7.481155e-08 | Total Loss: 3.865471e+08\n",
            "Epoch   10 | L1 Loss: 3.865471e+08 | L2 Loss: 1.418506e-06 | Total Loss: 3.865471e+08\n",
            "Epoch   15 | L1 Loss: 3.865471e+08 | L2 Loss: 1.008679e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   20 | L1 Loss: 3.865471e+08 | L2 Loss: 6.359349e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   25 | L1 Loss: 3.865471e+08 | L2 Loss: 4.065727e-04 | Total Loss: 3.865471e+08\n",
            "Epoch   30 | L1 Loss: 3.865470e+08 | L2 Loss: 2.265589e-03 | Total Loss: 3.865470e+08\n",
            "Epoch   35 | L1 Loss: 3.865469e+08 | L2 Loss: 1.274637e-02 | Total Loss: 3.865469e+08\n",
            "Epoch   40 | L1 Loss: 3.865467e+08 | L2 Loss: 7.020675e-02 | Total Loss: 3.865467e+08\n",
            "Epoch   45 | L1 Loss: 3.865459e+08 | L2 Loss: 3.225311e-01 | Total Loss: 3.865459e+08\n",
            "Epoch   50 | L1 Loss: 3.865446e+08 | L2 Loss: 1.026777e+00 | Total Loss: 3.865446e+08\n",
            "Epoch   55 | L1 Loss: 3.865426e+08 | L2 Loss: 3.096196e+00 | Total Loss: 3.865426e+08\n",
            "Epoch   60 | L1 Loss: 3.865391e+08 | L2 Loss: 8.329525e+00 | Total Loss: 3.865391e+08\n",
            "Epoch   65 | L1 Loss: 3.865335e+08 | L2 Loss: 2.175269e+01 | Total Loss: 3.865335e+08\n",
            "Epoch   70 | L1 Loss: 3.865254e+08 | L2 Loss: 4.079594e+01 | Total Loss: 3.865255e+08\n",
            "Epoch   75 | L1 Loss: 3.865143e+08 | L2 Loss: 1.498821e+02 | Total Loss: 3.865145e+08\n",
            "Epoch   80 | L1 Loss: 3.864970e+08 | L2 Loss: 1.759049e+02 | Total Loss: 3.864971e+08\n",
            "Epoch   85 | L1 Loss: 3.864746e+08 | L2 Loss: 3.269566e+02 | Total Loss: 3.864749e+08\n",
            "Epoch   90 | L1 Loss: 3.864444e+08 | L2 Loss: 9.390076e+02 | Total Loss: 3.864453e+08\n",
            "Epoch   95 | L1 Loss: 3.864119e+08 | L2 Loss: 8.654091e+02 | Total Loss: 3.864128e+08\n",
            "\n",
            " === Training run 2/5 === Patches: 34\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.865471e+08 | L2 Loss: 7.481155e-08 | Total Loss: 3.865471e+08\n",
            "Epoch   10 | L1 Loss: 3.865471e+08 | L2 Loss: 1.418506e-06 | Total Loss: 3.865471e+08\n",
            "Epoch   15 | L1 Loss: 3.865471e+08 | L2 Loss: 1.008679e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   20 | L1 Loss: 3.865471e+08 | L2 Loss: 6.359349e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   25 | L1 Loss: 3.865471e+08 | L2 Loss: 4.065727e-04 | Total Loss: 3.865471e+08\n",
            "Epoch   30 | L1 Loss: 3.865470e+08 | L2 Loss: 2.265589e-03 | Total Loss: 3.865470e+08\n",
            "Epoch   35 | L1 Loss: 3.865469e+08 | L2 Loss: 1.274637e-02 | Total Loss: 3.865469e+08\n",
            "Epoch   40 | L1 Loss: 3.865467e+08 | L2 Loss: 7.020675e-02 | Total Loss: 3.865467e+08\n",
            "Epoch   45 | L1 Loss: 3.865459e+08 | L2 Loss: 3.225311e-01 | Total Loss: 3.865459e+08\n",
            "Epoch   50 | L1 Loss: 3.865446e+08 | L2 Loss: 1.026777e+00 | Total Loss: 3.865446e+08\n",
            "Epoch   55 | L1 Loss: 3.865426e+08 | L2 Loss: 3.096196e+00 | Total Loss: 3.865426e+08\n",
            "Epoch   60 | L1 Loss: 3.865391e+08 | L2 Loss: 8.329525e+00 | Total Loss: 3.865391e+08\n",
            "Epoch   65 | L1 Loss: 3.865335e+08 | L2 Loss: 2.175269e+01 | Total Loss: 3.865335e+08\n",
            "Epoch   70 | L1 Loss: 3.865254e+08 | L2 Loss: 4.079594e+01 | Total Loss: 3.865255e+08\n",
            "Epoch   75 | L1 Loss: 3.865143e+08 | L2 Loss: 1.498821e+02 | Total Loss: 3.865145e+08\n",
            "Epoch   80 | L1 Loss: 3.864970e+08 | L2 Loss: 1.759049e+02 | Total Loss: 3.864971e+08\n",
            "Epoch   85 | L1 Loss: 3.864746e+08 | L2 Loss: 3.269566e+02 | Total Loss: 3.864749e+08\n",
            "Epoch   90 | L1 Loss: 3.864444e+08 | L2 Loss: 9.390076e+02 | Total Loss: 3.864453e+08\n",
            "Epoch   95 | L1 Loss: 3.864119e+08 | L2 Loss: 8.654091e+02 | Total Loss: 3.864128e+08\n",
            "\n",
            " === Training run 3/5 === Patches: 34\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.865471e+08 | L2 Loss: 7.481155e-08 | Total Loss: 3.865471e+08\n",
            "Epoch   10 | L1 Loss: 3.865471e+08 | L2 Loss: 1.418506e-06 | Total Loss: 3.865471e+08\n",
            "Epoch   15 | L1 Loss: 3.865471e+08 | L2 Loss: 1.008679e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   20 | L1 Loss: 3.865471e+08 | L2 Loss: 6.359349e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   25 | L1 Loss: 3.865471e+08 | L2 Loss: 4.065727e-04 | Total Loss: 3.865471e+08\n",
            "Epoch   30 | L1 Loss: 3.865470e+08 | L2 Loss: 2.265589e-03 | Total Loss: 3.865470e+08\n",
            "Epoch   35 | L1 Loss: 3.865469e+08 | L2 Loss: 1.274637e-02 | Total Loss: 3.865469e+08\n",
            "Epoch   40 | L1 Loss: 3.865467e+08 | L2 Loss: 7.020675e-02 | Total Loss: 3.865467e+08\n",
            "Epoch   45 | L1 Loss: 3.865459e+08 | L2 Loss: 3.225311e-01 | Total Loss: 3.865459e+08\n",
            "Epoch   50 | L1 Loss: 3.865446e+08 | L2 Loss: 1.026777e+00 | Total Loss: 3.865446e+08\n",
            "Epoch   55 | L1 Loss: 3.865426e+08 | L2 Loss: 3.096196e+00 | Total Loss: 3.865426e+08\n",
            "Epoch   60 | L1 Loss: 3.865391e+08 | L2 Loss: 8.329525e+00 | Total Loss: 3.865391e+08\n",
            "Epoch   65 | L1 Loss: 3.865335e+08 | L2 Loss: 2.175269e+01 | Total Loss: 3.865335e+08\n",
            "Epoch   70 | L1 Loss: 3.865254e+08 | L2 Loss: 4.079594e+01 | Total Loss: 3.865255e+08\n",
            "Epoch   75 | L1 Loss: 3.865143e+08 | L2 Loss: 1.498821e+02 | Total Loss: 3.865145e+08\n",
            "Epoch   80 | L1 Loss: 3.864970e+08 | L2 Loss: 1.759049e+02 | Total Loss: 3.864971e+08\n",
            "Epoch   85 | L1 Loss: 3.864746e+08 | L2 Loss: 3.269566e+02 | Total Loss: 3.864749e+08\n",
            "Epoch   90 | L1 Loss: 3.864444e+08 | L2 Loss: 9.390076e+02 | Total Loss: 3.864453e+08\n",
            "Epoch   95 | L1 Loss: 3.864119e+08 | L2 Loss: 8.654091e+02 | Total Loss: 3.864128e+08\n",
            "\n",
            " === Training run 4/5 === Patches: 34\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.865471e+08 | L2 Loss: 7.481155e-08 | Total Loss: 3.865471e+08\n",
            "Epoch   10 | L1 Loss: 3.865471e+08 | L2 Loss: 1.418506e-06 | Total Loss: 3.865471e+08\n",
            "Epoch   15 | L1 Loss: 3.865471e+08 | L2 Loss: 1.008679e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   20 | L1 Loss: 3.865471e+08 | L2 Loss: 6.359349e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   25 | L1 Loss: 3.865471e+08 | L2 Loss: 4.065727e-04 | Total Loss: 3.865471e+08\n",
            "Epoch   30 | L1 Loss: 3.865470e+08 | L2 Loss: 2.265589e-03 | Total Loss: 3.865470e+08\n",
            "Epoch   35 | L1 Loss: 3.865469e+08 | L2 Loss: 1.274637e-02 | Total Loss: 3.865469e+08\n",
            "Epoch   40 | L1 Loss: 3.865467e+08 | L2 Loss: 7.020675e-02 | Total Loss: 3.865467e+08\n",
            "Epoch   45 | L1 Loss: 3.865459e+08 | L2 Loss: 3.225311e-01 | Total Loss: 3.865459e+08\n",
            "Epoch   50 | L1 Loss: 3.865446e+08 | L2 Loss: 1.026777e+00 | Total Loss: 3.865446e+08\n",
            "Epoch   55 | L1 Loss: 3.865426e+08 | L2 Loss: 3.096196e+00 | Total Loss: 3.865426e+08\n",
            "Epoch   60 | L1 Loss: 3.865391e+08 | L2 Loss: 8.329525e+00 | Total Loss: 3.865391e+08\n",
            "Epoch   65 | L1 Loss: 3.865335e+08 | L2 Loss: 2.175269e+01 | Total Loss: 3.865335e+08\n",
            "Epoch   70 | L1 Loss: 3.865254e+08 | L2 Loss: 4.079594e+01 | Total Loss: 3.865255e+08\n",
            "Epoch   75 | L1 Loss: 3.865143e+08 | L2 Loss: 1.498821e+02 | Total Loss: 3.865145e+08\n",
            "Epoch   80 | L1 Loss: 3.864970e+08 | L2 Loss: 1.759049e+02 | Total Loss: 3.864971e+08\n",
            "Epoch   85 | L1 Loss: 3.864746e+08 | L2 Loss: 3.269566e+02 | Total Loss: 3.864749e+08\n",
            "Epoch   90 | L1 Loss: 3.864444e+08 | L2 Loss: 9.390076e+02 | Total Loss: 3.864453e+08\n",
            "Epoch   95 | L1 Loss: 3.864119e+08 | L2 Loss: 8.654091e+02 | Total Loss: 3.864128e+08\n",
            "\n",
            " === Training run 5/5 === Patches: 34\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 3.865471e+08 | L2 Loss: 7.481155e-08 | Total Loss: 3.865471e+08\n",
            "Epoch   10 | L1 Loss: 3.865471e+08 | L2 Loss: 1.418506e-06 | Total Loss: 3.865471e+08\n",
            "Epoch   15 | L1 Loss: 3.865471e+08 | L2 Loss: 1.008679e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   20 | L1 Loss: 3.865471e+08 | L2 Loss: 6.359349e-05 | Total Loss: 3.865471e+08\n",
            "Epoch   25 | L1 Loss: 3.865471e+08 | L2 Loss: 4.065727e-04 | Total Loss: 3.865471e+08\n",
            "Epoch   30 | L1 Loss: 3.865470e+08 | L2 Loss: 2.265589e-03 | Total Loss: 3.865470e+08\n",
            "Epoch   35 | L1 Loss: 3.865469e+08 | L2 Loss: 1.274637e-02 | Total Loss: 3.865469e+08\n",
            "Epoch   40 | L1 Loss: 3.865467e+08 | L2 Loss: 7.020675e-02 | Total Loss: 3.865467e+08\n",
            "Epoch   45 | L1 Loss: 3.865459e+08 | L2 Loss: 3.225311e-01 | Total Loss: 3.865459e+08\n",
            "Epoch   50 | L1 Loss: 3.865446e+08 | L2 Loss: 1.026777e+00 | Total Loss: 3.865446e+08\n",
            "Epoch   55 | L1 Loss: 3.865426e+08 | L2 Loss: 3.096196e+00 | Total Loss: 3.865426e+08\n",
            "Epoch   60 | L1 Loss: 3.865391e+08 | L2 Loss: 8.329525e+00 | Total Loss: 3.865391e+08\n",
            "Epoch   65 | L1 Loss: 3.865335e+08 | L2 Loss: 2.175269e+01 | Total Loss: 3.865335e+08\n",
            "Epoch   70 | L1 Loss: 3.865254e+08 | L2 Loss: 4.079594e+01 | Total Loss: 3.865255e+08\n",
            "Epoch   75 | L1 Loss: 3.865143e+08 | L2 Loss: 1.498821e+02 | Total Loss: 3.865145e+08\n",
            "Epoch   80 | L1 Loss: 3.864970e+08 | L2 Loss: 1.759049e+02 | Total Loss: 3.864971e+08\n",
            "Epoch   85 | L1 Loss: 3.864746e+08 | L2 Loss: 3.269566e+02 | Total Loss: 3.864749e+08\n",
            "Epoch   90 | L1 Loss: 3.864444e+08 | L2 Loss: 9.390076e+02 | Total Loss: 3.864453e+08\n",
            "Epoch   95 | L1 Loss: 3.864119e+08 | L2 Loss: 8.654091e+02 | Total Loss: 3.864128e+08\n",
            "Processing from patch 35 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 35\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 8.617896e+07 | L2 Loss: 5.387243e-08 | Total Loss: 8.617896e+07\n",
            "Epoch   10 | L1 Loss: 8.617896e+07 | L2 Loss: 1.008503e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   15 | L1 Loss: 8.617896e+07 | L2 Loss: 7.291835e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   20 | L1 Loss: 8.617896e+07 | L2 Loss: 4.741213e-05 | Total Loss: 8.617896e+07\n",
            "Epoch   25 | L1 Loss: 8.617895e+07 | L2 Loss: 3.060940e-04 | Total Loss: 8.617895e+07\n",
            "Epoch   30 | L1 Loss: 8.617894e+07 | L2 Loss: 1.706493e-03 | Total Loss: 8.617894e+07\n",
            "Epoch   35 | L1 Loss: 8.617890e+07 | L2 Loss: 9.454632e-03 | Total Loss: 8.617890e+07\n",
            "Epoch   40 | L1 Loss: 8.617882e+07 | L2 Loss: 4.706899e-02 | Total Loss: 8.617882e+07\n",
            "Epoch   45 | L1 Loss: 8.617862e+07 | L2 Loss: 2.243350e-01 | Total Loss: 8.617862e+07\n",
            "Epoch   50 | L1 Loss: 8.617826e+07 | L2 Loss: 8.266299e-01 | Total Loss: 8.617826e+07\n",
            "Epoch   55 | L1 Loss: 8.617762e+07 | L2 Loss: 2.464120e+00 | Total Loss: 8.617762e+07\n",
            "Epoch   60 | L1 Loss: 8.617642e+07 | L2 Loss: 6.773370e+00 | Total Loss: 8.617642e+07\n",
            "Epoch   65 | L1 Loss: 8.617433e+07 | L2 Loss: 1.675970e+01 | Total Loss: 8.617434e+07\n",
            "Epoch   70 | L1 Loss: 8.617053e+07 | L2 Loss: 4.066362e+01 | Total Loss: 8.617057e+07\n",
            "Epoch   75 | L1 Loss: 8.616422e+07 | L2 Loss: 6.697887e+01 | Total Loss: 8.616429e+07\n",
            "Epoch   80 | L1 Loss: 8.615734e+07 | L2 Loss: 9.274345e+01 | Total Loss: 8.615743e+07\n",
            "Epoch   85 | L1 Loss: 8.614714e+07 | L2 Loss: 1.658363e+02 | Total Loss: 8.614730e+07\n",
            "Epoch   90 | L1 Loss: 8.613632e+07 | L2 Loss: 2.414142e+02 | Total Loss: 8.613656e+07\n",
            "Epoch   95 | L1 Loss: 8.611128e+07 | L2 Loss: 9.637928e+02 | Total Loss: 8.611224e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 35\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 8.617896e+07 | L2 Loss: 5.387243e-08 | Total Loss: 8.617896e+07\n",
            "Epoch   10 | L1 Loss: 8.617896e+07 | L2 Loss: 1.008503e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   15 | L1 Loss: 8.617896e+07 | L2 Loss: 7.291835e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   20 | L1 Loss: 8.617896e+07 | L2 Loss: 4.741213e-05 | Total Loss: 8.617896e+07\n",
            "Epoch   25 | L1 Loss: 8.617895e+07 | L2 Loss: 3.060940e-04 | Total Loss: 8.617895e+07\n",
            "Epoch   30 | L1 Loss: 8.617894e+07 | L2 Loss: 1.706493e-03 | Total Loss: 8.617894e+07\n",
            "Epoch   35 | L1 Loss: 8.617890e+07 | L2 Loss: 9.454632e-03 | Total Loss: 8.617890e+07\n",
            "Epoch   40 | L1 Loss: 8.617882e+07 | L2 Loss: 4.706899e-02 | Total Loss: 8.617882e+07\n",
            "Epoch   45 | L1 Loss: 8.617862e+07 | L2 Loss: 2.243350e-01 | Total Loss: 8.617862e+07\n",
            "Epoch   50 | L1 Loss: 8.617826e+07 | L2 Loss: 8.266299e-01 | Total Loss: 8.617826e+07\n",
            "Epoch   55 | L1 Loss: 8.617762e+07 | L2 Loss: 2.464120e+00 | Total Loss: 8.617762e+07\n",
            "Epoch   60 | L1 Loss: 8.617642e+07 | L2 Loss: 6.773370e+00 | Total Loss: 8.617642e+07\n",
            "Epoch   65 | L1 Loss: 8.617433e+07 | L2 Loss: 1.675970e+01 | Total Loss: 8.617434e+07\n",
            "Epoch   70 | L1 Loss: 8.617053e+07 | L2 Loss: 4.066362e+01 | Total Loss: 8.617057e+07\n",
            "Epoch   75 | L1 Loss: 8.616422e+07 | L2 Loss: 6.697887e+01 | Total Loss: 8.616429e+07\n",
            "Epoch   80 | L1 Loss: 8.615734e+07 | L2 Loss: 9.274345e+01 | Total Loss: 8.615743e+07\n",
            "Epoch   85 | L1 Loss: 8.614714e+07 | L2 Loss: 1.658363e+02 | Total Loss: 8.614730e+07\n",
            "Epoch   90 | L1 Loss: 8.613632e+07 | L2 Loss: 2.414142e+02 | Total Loss: 8.613656e+07\n",
            "Epoch   95 | L1 Loss: 8.611128e+07 | L2 Loss: 9.637928e+02 | Total Loss: 8.611224e+07\n",
            "\n",
            " === Training run 3/5 === Patches: 35\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 8.617896e+07 | L2 Loss: 5.387243e-08 | Total Loss: 8.617896e+07\n",
            "Epoch   10 | L1 Loss: 8.617896e+07 | L2 Loss: 1.008503e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   15 | L1 Loss: 8.617896e+07 | L2 Loss: 7.291835e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   20 | L1 Loss: 8.617896e+07 | L2 Loss: 4.741213e-05 | Total Loss: 8.617896e+07\n",
            "Epoch   25 | L1 Loss: 8.617895e+07 | L2 Loss: 3.060940e-04 | Total Loss: 8.617895e+07\n",
            "Epoch   30 | L1 Loss: 8.617894e+07 | L2 Loss: 1.706493e-03 | Total Loss: 8.617894e+07\n",
            "Epoch   35 | L1 Loss: 8.617890e+07 | L2 Loss: 9.454632e-03 | Total Loss: 8.617890e+07\n",
            "Epoch   40 | L1 Loss: 8.617882e+07 | L2 Loss: 4.706899e-02 | Total Loss: 8.617882e+07\n",
            "Epoch   45 | L1 Loss: 8.617862e+07 | L2 Loss: 2.243350e-01 | Total Loss: 8.617862e+07\n",
            "Epoch   50 | L1 Loss: 8.617826e+07 | L2 Loss: 8.266299e-01 | Total Loss: 8.617826e+07\n",
            "Epoch   55 | L1 Loss: 8.617762e+07 | L2 Loss: 2.464120e+00 | Total Loss: 8.617762e+07\n",
            "Epoch   60 | L1 Loss: 8.617642e+07 | L2 Loss: 6.773370e+00 | Total Loss: 8.617642e+07\n",
            "Epoch   65 | L1 Loss: 8.617433e+07 | L2 Loss: 1.675970e+01 | Total Loss: 8.617434e+07\n",
            "Epoch   70 | L1 Loss: 8.617053e+07 | L2 Loss: 4.066362e+01 | Total Loss: 8.617057e+07\n",
            "Epoch   75 | L1 Loss: 8.616422e+07 | L2 Loss: 6.697887e+01 | Total Loss: 8.616429e+07\n",
            "Epoch   80 | L1 Loss: 8.615734e+07 | L2 Loss: 9.274345e+01 | Total Loss: 8.615743e+07\n",
            "Epoch   85 | L1 Loss: 8.614714e+07 | L2 Loss: 1.658363e+02 | Total Loss: 8.614730e+07\n",
            "Epoch   90 | L1 Loss: 8.613632e+07 | L2 Loss: 2.414142e+02 | Total Loss: 8.613656e+07\n",
            "Epoch   95 | L1 Loss: 8.611128e+07 | L2 Loss: 9.637928e+02 | Total Loss: 8.611224e+07\n",
            "\n",
            " === Training run 4/5 === Patches: 35\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 8.617896e+07 | L2 Loss: 5.387243e-08 | Total Loss: 8.617896e+07\n",
            "Epoch   10 | L1 Loss: 8.617896e+07 | L2 Loss: 1.008503e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   15 | L1 Loss: 8.617896e+07 | L2 Loss: 7.291835e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   20 | L1 Loss: 8.617896e+07 | L2 Loss: 4.741213e-05 | Total Loss: 8.617896e+07\n",
            "Epoch   25 | L1 Loss: 8.617895e+07 | L2 Loss: 3.060940e-04 | Total Loss: 8.617895e+07\n",
            "Epoch   30 | L1 Loss: 8.617894e+07 | L2 Loss: 1.706493e-03 | Total Loss: 8.617894e+07\n",
            "Epoch   35 | L1 Loss: 8.617890e+07 | L2 Loss: 9.454632e-03 | Total Loss: 8.617890e+07\n",
            "Epoch   40 | L1 Loss: 8.617882e+07 | L2 Loss: 4.706899e-02 | Total Loss: 8.617882e+07\n",
            "Epoch   45 | L1 Loss: 8.617862e+07 | L2 Loss: 2.243350e-01 | Total Loss: 8.617862e+07\n",
            "Epoch   50 | L1 Loss: 8.617826e+07 | L2 Loss: 8.266299e-01 | Total Loss: 8.617826e+07\n",
            "Epoch   55 | L1 Loss: 8.617762e+07 | L2 Loss: 2.464120e+00 | Total Loss: 8.617762e+07\n",
            "Epoch   60 | L1 Loss: 8.617642e+07 | L2 Loss: 6.773370e+00 | Total Loss: 8.617642e+07\n",
            "Epoch   65 | L1 Loss: 8.617433e+07 | L2 Loss: 1.675970e+01 | Total Loss: 8.617434e+07\n",
            "Epoch   70 | L1 Loss: 8.617053e+07 | L2 Loss: 4.066362e+01 | Total Loss: 8.617057e+07\n",
            "Epoch   75 | L1 Loss: 8.616422e+07 | L2 Loss: 6.697887e+01 | Total Loss: 8.616429e+07\n",
            "Epoch   80 | L1 Loss: 8.615734e+07 | L2 Loss: 9.274345e+01 | Total Loss: 8.615743e+07\n",
            "Epoch   85 | L1 Loss: 8.614714e+07 | L2 Loss: 1.658363e+02 | Total Loss: 8.614730e+07\n",
            "Epoch   90 | L1 Loss: 8.613632e+07 | L2 Loss: 2.414142e+02 | Total Loss: 8.613656e+07\n",
            "Epoch   95 | L1 Loss: 8.611128e+07 | L2 Loss: 9.637928e+02 | Total Loss: 8.611224e+07\n",
            "\n",
            " === Training run 5/5 === Patches: 35\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 8.617896e+07 | L2 Loss: 5.387243e-08 | Total Loss: 8.617896e+07\n",
            "Epoch   10 | L1 Loss: 8.617896e+07 | L2 Loss: 1.008503e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   15 | L1 Loss: 8.617896e+07 | L2 Loss: 7.291835e-06 | Total Loss: 8.617896e+07\n",
            "Epoch   20 | L1 Loss: 8.617896e+07 | L2 Loss: 4.741213e-05 | Total Loss: 8.617896e+07\n",
            "Epoch   25 | L1 Loss: 8.617895e+07 | L2 Loss: 3.060940e-04 | Total Loss: 8.617895e+07\n",
            "Epoch   30 | L1 Loss: 8.617894e+07 | L2 Loss: 1.706493e-03 | Total Loss: 8.617894e+07\n",
            "Epoch   35 | L1 Loss: 8.617890e+07 | L2 Loss: 9.454632e-03 | Total Loss: 8.617890e+07\n",
            "Epoch   40 | L1 Loss: 8.617882e+07 | L2 Loss: 4.706899e-02 | Total Loss: 8.617882e+07\n",
            "Epoch   45 | L1 Loss: 8.617862e+07 | L2 Loss: 2.243350e-01 | Total Loss: 8.617862e+07\n",
            "Epoch   50 | L1 Loss: 8.617826e+07 | L2 Loss: 8.266299e-01 | Total Loss: 8.617826e+07\n",
            "Epoch   55 | L1 Loss: 8.617762e+07 | L2 Loss: 2.464120e+00 | Total Loss: 8.617762e+07\n",
            "Epoch   60 | L1 Loss: 8.617642e+07 | L2 Loss: 6.773370e+00 | Total Loss: 8.617642e+07\n",
            "Epoch   65 | L1 Loss: 8.617433e+07 | L2 Loss: 1.675970e+01 | Total Loss: 8.617434e+07\n",
            "Epoch   70 | L1 Loss: 8.617053e+07 | L2 Loss: 4.066362e+01 | Total Loss: 8.617057e+07\n",
            "Epoch   75 | L1 Loss: 8.616422e+07 | L2 Loss: 6.697887e+01 | Total Loss: 8.616429e+07\n",
            "Epoch   80 | L1 Loss: 8.615734e+07 | L2 Loss: 9.274345e+01 | Total Loss: 8.615743e+07\n",
            "Epoch   85 | L1 Loss: 8.614714e+07 | L2 Loss: 1.658363e+02 | Total Loss: 8.614730e+07\n",
            "Epoch   90 | L1 Loss: 8.613632e+07 | L2 Loss: 2.414142e+02 | Total Loss: 8.613656e+07\n",
            "Epoch   95 | L1 Loss: 8.611128e+07 | L2 Loss: 9.637928e+02 | Total Loss: 8.611224e+07\n",
            "Processing from patch 36 Since the ones before were either equator or pole\n",
            "\n",
            " === Training run 1/5 === Patches: 36\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 9.122007e+07 | L2 Loss: 6.276573e-10 | Total Loss: 9.122007e+07\n",
            "Epoch   10 | L1 Loss: 9.122007e+07 | L2 Loss: 5.815761e-09 | Total Loss: 9.122007e+07\n",
            "Epoch   15 | L1 Loss: 9.122007e+07 | L2 Loss: 2.544926e-07 | Total Loss: 9.122007e+07\n",
            "Epoch   20 | L1 Loss: 9.122007e+07 | L2 Loss: 2.902461e-06 | Total Loss: 9.122007e+07\n",
            "Epoch   25 | L1 Loss: 9.122007e+07 | L2 Loss: 2.152366e-05 | Total Loss: 9.122007e+07\n",
            "Epoch   30 | L1 Loss: 9.122007e+07 | L2 Loss: 1.440002e-04 | Total Loss: 9.122007e+07\n",
            "Epoch   35 | L1 Loss: 9.122006e+07 | L2 Loss: 9.130537e-04 | Total Loss: 9.122006e+07\n",
            "Epoch   40 | L1 Loss: 9.122002e+07 | L2 Loss: 5.052280e-03 | Total Loss: 9.122002e+07\n",
            "Epoch   45 | L1 Loss: 9.121992e+07 | L2 Loss: 2.787413e-02 | Total Loss: 9.121992e+07\n",
            "Epoch   50 | L1 Loss: 9.121969e+07 | L2 Loss: 1.491548e-01 | Total Loss: 9.121969e+07\n",
            "Epoch   55 | L1 Loss: 9.121925e+07 | L2 Loss: 5.972731e-01 | Total Loss: 9.121925e+07\n",
            "Epoch   60 | L1 Loss: 9.121854e+07 | L2 Loss: 2.096810e+00 | Total Loss: 9.121854e+07\n",
            "Epoch   65 | L1 Loss: 9.121752e+07 | L2 Loss: 6.434593e+00 | Total Loss: 9.121753e+07\n",
            "Epoch   70 | L1 Loss: 9.121616e+07 | L2 Loss: 1.794676e+01 | Total Loss: 9.121618e+07\n",
            "Epoch   75 | L1 Loss: 9.121430e+07 | L2 Loss: 5.054203e+01 | Total Loss: 9.121434e+07\n",
            "Epoch   80 | L1 Loss: 9.121107e+07 | L2 Loss: 1.189424e+02 | Total Loss: 9.121119e+07\n",
            "Epoch   85 | L1 Loss: 9.120801e+07 | L2 Loss: 2.089123e+02 | Total Loss: 9.120822e+07\n",
            "Epoch   90 | L1 Loss: 9.120282e+07 | L2 Loss: 3.687430e+02 | Total Loss: 9.120318e+07\n",
            "Epoch   95 | L1 Loss: 9.119683e+07 | L2 Loss: 6.079764e+02 | Total Loss: 9.119744e+07\n",
            "\n",
            " === Training run 2/5 === Patches: 36\n",
            "Epoch    0 | L1 Loss: 0.000000e+00 | L2 Loss: 0.000000e+00 | Total Loss: 0.000000e+00\n",
            "Epoch    5 | L1 Loss: 9.122007e+07 | L2 Loss: 6.276573e-10 | Total Loss: 9.122007e+07\n"
          ]
        }
      ],
      "source": [
        "# I moved everything to the training loop so that for each patch in every iteration, the model receives the correct,\n",
        "# patch-specific input like thetas_nn, phis_nn, Br_nn, dBrdt_nn, dBrdth_nn, dBrdph_nn\n",
        "\n",
        "\n",
        "\n",
        "# Training over all patches\n",
        "num_realizations = 1 # 5 time on each patch\n",
        "epochs = 100\n",
        "λ = 1000\n",
        "\n",
        "patch_centers=[]\n",
        "\n",
        "\n",
        "print(f\"Number of times each patch will run: {num_realizations}\")\n",
        "print(f\"Number of epochs: {epochs}\")\n",
        "\n",
        "\n",
        "'''\n",
        "# for all patches\n",
        "for i1, i2, j1, j2 in patches:\n",
        "    if i1 < 10 or i2 > 170 or (i1 <= 90 <= i2):\n",
        "        continue  # skip poles and equator\n",
        "\n",
        "'''\n",
        "\n",
        "# for less time consumption, I am using 50 patches\n",
        "test_patches = patches[ :40] # you can slice it if u want less, that is why i created a seperate variable\n",
        "\n",
        "print(f\"Number of patches: {len(patches)} but we will consider : {len(test_patches)}\")\n",
        "\n",
        "for patch_idx, (i1, i2, j1, j2) in enumerate(test_patches, start=1):\n",
        "    # Skip poles and equator patches if needed\n",
        "    if i1 < 10 or i2 > 170 or (i1 <= 90 <= i2):\n",
        "      continue\n",
        "\n",
        "\n",
        "    print(f\"Processing from patch {patch_idx} Since the ones before were either equator or pole\") # To know when it is starting and what is getting out of calculation\n",
        "\n",
        "    #slice all Br and inputs\n",
        "    Br_patch = Br[i1:i2, j1:j2, ...]\n",
        "    dBrdt_patch = dBrdt[i1:i2, j1:j2, ...]\n",
        "\n",
        "    #differential or br and sv w.r.t. phi and theta\n",
        "    dBrdth = numpy.gradient(Br, thetas, axis=0)[i1:i2, j1:j2, ...]\n",
        "    dBrdph = numpy.gradient(Br, phis, axis=1)[i1:i2, j1:j2, ...]\n",
        "\n",
        "    thetas_bis = thetas[i1:i2]\n",
        "    phis_bis = phis[j1:j2]\n",
        "\n",
        "    thetas_grid, phis_grid = numpy.meshgrid(thetas_bis, phis_bis, indexing=\"ij\")\n",
        "    thetas_flat = thetas_grid.flatten()\n",
        "    phis_flat = phis_grid.flatten()\n",
        "\n",
        "    thetas_nn = torch.tensor(thetas_flat[:, None], dtype=torch.float32, requires_grad=True)\n",
        "    phis_nn = torch.tensor(phis_flat[:, None], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "\n",
        "    dBrdt_nn = torch.tensor(dBrdt_patch.flatten()[:, None], dtype=torch.float32)\n",
        "    dBrdth_nn = torch.tensor(dBrdth.flatten()[:, None], dtype=torch.float32)\n",
        "    dBrdph_nn = torch.tensor(dBrdph.flatten()[:, None], dtype=torch.float32)\n",
        "\n",
        "    inputs = torch.cat([thetas_nn, phis_nn], dim=1)\n",
        "\n",
        "    loss_history = []\n",
        "    L1_loss_history = []\n",
        "    L2_loss_history = []\n",
        "\n",
        "\n",
        "\n",
        "    for run in range(num_realizations):\n",
        "\n",
        "        print(f\"\\n === Training run {run + 1}/{num_realizations} === Patches: {patch_idx}\")\n",
        "\n",
        "        torch.manual_seed(0)\n",
        "        model = CoreFlowPINN()\n",
        "        with torch.no_grad():\n",
        "            final_layer = model.net[-1]\n",
        "            final_layer.weight.zero_()\n",
        "            final_layer.bias.zero_()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use zeros for dBr/dt at epoch 0\n",
        "            if epoch == 0:\n",
        "                dBrdt_nn_zero = torch.zeros_like(dBrdt_nn)\n",
        "                L1_loss, L2_loss_λ, L3_loss, L4_loss, _, _ = compute_loss(model, inputs, thetas_nn, phis_nn, dBrdt_nn_zero, Br_obs, sigma_i)\n",
        "            else:\n",
        "                L1_loss, L2_loss_λ, L3_loss, L4_loss, _, _ = compute_loss(model, inputs, thetas_nn, phis_nn, dBrdt_nn, Br_obs, sigma_i)\n",
        "\n",
        "            Loss = L1_loss + L2_loss_λ\n",
        "            Loss.backward()\n",
        "\n",
        "            if epoch == 0:\n",
        "                with torch.no_grad():\n",
        "                    final_layer = model.net[-1]\n",
        "                    final_layer.weight += 1e-4 * torch.randn_like(final_layer.weight)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"Epoch {epoch:4d} | L1 Loss: {L1_loss.item():12.6e} | L2 Loss: {L2_loss_λ.item():12.6e} | Total Loss: {Loss.item():12.6e}\", flush=True)\n",
        "\n",
        "            # for graphs\n",
        "            loss_history.append(Loss)\n",
        "            L1_loss_history.append(L1_loss)\n",
        "            L2_loss_history.append(L2_loss_λ)\n",
        "\n",
        "    #mean_loss_map = numpy.mean(loss_history, axis=0)\n",
        "    #loss_grid = mean_loss_map.reshape(len(thetas_bis), len(phis_bis))\n",
        "    patch_centers.append(((i1 + i2) // 2, (j1 + j2) // 2)) # for SV later\n",
        "\n",
        "\n",
        "print(\" Done computing for all patches.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert each tensor in loss_history to a detached NumPy array\n",
        "loss_history_np = [loss.detach().cpu().numpy() if isinstance(loss, torch.Tensor) else loss for loss in loss_history]\n",
        "\n",
        "# If loss_history_np is a 1D list of values, plot directly\n",
        "plt.loglog(loss_history_np)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qA6nnuFjtmJO"
      },
      "id": "qA6nnuFjtmJO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Convert each tensor in loss_history to a detached NumPy array\n",
        "loss_history_np1 = [loss.detach().cpu().numpy() if isinstance(loss, torch.Tensor) else loss for loss in L1_history]\n",
        "loss_history_np2 = [loss.detach().cpu().numpy() if isinstance(loss, torch.Tensor) else loss for loss in L2_history]\n",
        "\n",
        "# Plot both on a log-log scale with different colors and labels\n",
        "plt.loglog(loss_history_np1, label='L1 Loss', color='blue')\n",
        "plt.loglog(loss_history_np2, label='L2 Loss', color='orange')\n",
        "\n",
        "# Add legend and display\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss History (Log-Log Plot)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pnPhQCPp__EL"
      },
      "id": "pnPhQCPp__EL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SV graph"
      ],
      "metadata": {
        "id": "9KgdKAF1qZLk"
      },
      "id": "9KgdKAF1qZLk"
    },
    {
      "cell_type": "code",
      "source": [
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import numpy as np\n",
        "\n",
        "def plot_sv_patch(dBrdt_patch, thetas_bis, phis_bis, patch_id=None):\n",
        "    # Convert from colatitude (theta) and longitude (phi) to lat/lon degrees\n",
        "    theta_grid, phi_grid = np.meshgrid(thetas_bis, phis_bis, indexing='ij')\n",
        "    lat_grid = 90 - np.rad2deg(theta_grid)   # colatitude to latitude\n",
        "    lon_grid = np.rad2deg(phi_grid)          # radians to degrees longitude\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "    ax.coastlines(resolution='110m')\n",
        "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "    ax.gridlines(draw_labels=True)\n",
        "\n",
        "    sc = ax.scatter(lon_grid, lat_grid, c=dBrdt_patch, cmap='seismic', s=20, transform=ccrs.PlateCarree())\n",
        "    cb = plt.colorbar(sc, orientation='vertical', shrink=0.7, pad=0.05)\n",
        "    cb.set_label(r'$\\partial B_r / \\partial t$ (SV)', fontsize=12)\n",
        "\n",
        "    if patch_id is not None:\n",
        "        ax.set_title(f'SV Patch {patch_id}')\n",
        "    else:\n",
        "        ax.set_title('SV Patch')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_sv_patch(dBrdt_patch, thetas_bis, phis_bis, patch_id=len(patch_centers)+1)\n"
      ],
      "metadata": {
        "id": "AH0b8kKYLuTh"
      },
      "id": "AH0b8kKYLuTh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "above: red for positive (increasing\n",
        "𝐵\n",
        "𝑟\n",
        " ), blue for negative (decreasing\n",
        "𝐵\n",
        "𝑟 ).\n",
        "\n"
      ],
      "metadata": {
        "id": "f4uvMUmU35mf"
      },
      "id": "f4uvMUmU35mf"
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,_,_, u_th, u_ph = compute_loss(model, inputs, thetas_nn, phis_nn, dBrdt_nn_zero, Br_obs, sigma_i)\n",
        "# Used patch grid sizes, NOT the full global Br shape or else there is shape error\n",
        "patch_shape = (len(thetas_bis), len(phis_bis))\n",
        "\n",
        "u_th_map = u_th.reshape(patch_shape).detach().cpu().numpy()\n",
        "u_ph_map = u_ph.reshape(patch_shape).detach().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "ngJ7R3UlLXt6"
      },
      "id": "ngJ7R3UlLXt6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cmocean\n",
        "import cartopy.crs as ccrs\n",
        "\n",
        "u_th_full = numpy.zeros((thetas.size, phis.size))\n",
        "u_th_full[i1:i2,j1:j2] = u_th_map\n",
        "\n",
        "thetas_bis_deg = numpy.rad2deg(thetas)\n",
        "phis_bis_deg = numpy.rad2deg(phis)\n",
        "\n",
        "latitudes = pygeo.convertThetasToLatitudes(thetas)\n",
        "longitudes = pygeo.convertPhisToLongitudes(phis)\n",
        "\n",
        "lat_grid, lon_grid = numpy.meshgrid(latitudes, longitudes, indexing=\"ij\")\n",
        "\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "# Set the projection to Hammer and add the axes\n",
        "ax = fig.add_subplot(1, 1, 1, projection=ccrs.Aitoff())\n",
        "\n",
        "u_max = numpy.max(numpy.abs(u_th_full))\n",
        "\n",
        "# Use `pcolormesh` to project the data onto the map\n",
        "pcol = ax.pcolormesh(lon_grid, lat_grid, u_th_full, transform=ccrs.PlateCarree(), cmap=cmocean.cm.balance, vmin=-u_max, vmax=u_max)\n",
        "\n",
        "# Add coastlines for context\n",
        "ax.coastlines()\n",
        "\n",
        "plt.colorbar(pcol)"
      ],
      "metadata": {
        "id": "cPyqHgs7LanX"
      },
      "id": "cPyqHgs7LanX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
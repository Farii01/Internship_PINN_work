{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de392b6b",
   "metadata": {},
   "source": [
    "### Retrieving the core flow from geomagnetic measures using _physics-informed neural networks_ (PiNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86170f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygeodyntools --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pygeotools\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6e044",
   "metadata": {},
   "source": [
    "### 0) Useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_study: int = 2020       # Time of the study\n",
    "\n",
    "number_of_inputs: int = 2       # Number of input variables (θ and φ angles)\n",
    "number_of_outputs: int = 3      # Number of output variables (uθ, uφ, br)\n",
    "\n",
    "number_of_layers: int = 1       # Number of layers\n",
    "number_of_neurons: int = 16     # Number of neurons\n",
    "\n",
    "number_epochs: int = 10000      # Number of epochs\n",
    "\n",
    "fine_tune: bool = True          # Fine-tuning the model using L-BFGS?\n",
    "number_steps: int = 500         # Number of steps for the fine tuning\n",
    "\n",
    "grid_size: int = 3              # Using 3deg per 3deg grid\n",
    "\n",
    "patch: tuple = (5, 85, 5, 85)   # Defining the patch\n",
    "\n",
    "use_synthetic_sv: bool = False   # Do we use a synthetic SV?\n",
    "\n",
    "estimate_small_scale_br: bool = True # Do we estimate the small-scales of Br?\n",
    "\n",
    "λ_dbrdt = 1\n",
    "λ_br = 0\n",
    "λ_tg = 1e4\n",
    "λ_tor = 0\n",
    "λ_wn = 1e-5\n",
    "λ_sn = 0\n",
    "\n",
    "losses: dict = {                # The different losses to use (available: misfit_dbrdt, misfit_br, tangential_geostrophy, weak_norm, strong_norm, vorticity)\n",
    "    \"misfit_dbrdt\": λ_dbrdt,\n",
    "    \"misfit_br\": λ_br,\n",
    "    \"tangential_geostrophy\": λ_tg,\n",
    "    # \"toroidal\": λ_tor,\n",
    "    \"weak_norm\": λ_wn,\n",
    "    # \"strong_norm\": λ_sn\n",
    "    # \"vorticity\": 1e4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ee5a4",
   "metadata": {},
   "source": [
    "### 1) Retrieving the data from geodynamo simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the library\n",
    "pygeo = pygeotools.pygeotools()\n",
    "\n",
    "# Defining the path and the name of the model\n",
    "model_path = \"geodynamo.hdf5\"\n",
    "model_name = \"model_geodynamo\"\n",
    "\n",
    "# Loading the model\n",
    "pygeo.loadModel(model_name, \"pygeodyn_hdf5\", model_path)\n",
    "\n",
    "# Defining the grid for computing the fields\n",
    "# It is 3°x3° angular grid\n",
    "if not pygeo.isGrid(f\"{grid_size}deg\"):\n",
    "    pygeo.addGrid(f\"{grid_size}deg\", grid_size, grid_size)\n",
    "\n",
    "# Setting the grid\n",
    "pygeo.setGrid(f\"{grid_size}deg\")\n",
    "\n",
    "# Retrieving the angles\n",
    "_, (thetas, phis) = pygeo.getCurrentGrid()\n",
    "\n",
    "# Defining the context for computing the SV and MF, and U\n",
    "context_sv_mf = {\"lmax\": 13, \"r\": pygeo.constants[\"rCore\"]}\n",
    "context_u = {\"lmax\": 18, \"r\": pygeo.constants[\"rCore\"]}\n",
    "\n",
    "# Computing the fields\n",
    "SV = pygeo.addMeasure(model_name, \"SV\", context_sv_mf)\n",
    "MF = pygeo.addMeasure(model_name, \"MF\", context_sv_mf)\n",
    "U = pygeo.addMeasure(model_name, \"U\", context_u)\n",
    "\n",
    "# Retrieving the times\n",
    "times = pygeo.getQuantity(model_name, \"times\")\n",
    "\n",
    "# Retrieving the associated times index\n",
    "idx_times = numpy.argmin(numpy.abs(times - time_of_study))\n",
    "\n",
    "# Retrieving the components of the fields\n",
    "dbrdt_obs = SV[idx_times, ..., 0]\n",
    "br_obs = MF[idx_times, ..., 0]\n",
    "dbrdth_obs = numpy.gradient(br_obs, thetas, axis=0)\n",
    "dbrdph_obs = numpy.gradient(br_obs, phis, axis=1)\n",
    "uθ_obs = U[idx_times, ..., 1]\n",
    "uφ_obs = U[idx_times, ..., 2]\n",
    "\n",
    "# Storing into the observations dict\n",
    "observations = {\n",
    "    \"dbrdt\": dbrdt_obs, \"br\": br_obs, \"uth\": uθ_obs, \"uph\": uφ_obs, \"dbrdth\": dbrdth_obs, \"dbrdph\": dbrdph_obs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff07ed3",
   "metadata": {},
   "source": [
    "### 1 bis) Generating synthetic SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_synthetic_sv:\n",
    "\n",
    "    # Creating synthetic data\n",
    "    θ = thetas\n",
    "    φ = phis\n",
    "\n",
    "    θ_grid, φ_grid = numpy.meshgrid(θ, φ, indexing=\"ij\")\n",
    "\n",
    "    sinθ = numpy.sin(θ_grid).clip(1e-1, 1)\n",
    "    cosθ = numpy.cos(θ_grid)\n",
    "\n",
    "    dθ = numpy.gradient(θ).mean()\n",
    "    dφ = numpy.gradient(φ).mean()\n",
    "\n",
    "    dΩ = sinθ * dθ * dφ\n",
    "\n",
    "    r = 3480.\n",
    "\n",
    "    duθdθ = numpy.gradient(uθ_obs, θ, axis=0)\n",
    "    duθdφ = numpy.gradient(uθ_obs, φ, axis=1)\n",
    "    duφdθ = numpy.gradient(uφ_obs, θ, axis=0)\n",
    "    duφdφ = numpy.gradient(uφ_obs, φ, axis=1)\n",
    "\n",
    "    divh_uh= (1 / (r * sinθ)) * (duθdθ * sinθ + uθ_obs * cosθ + duφdφ)\n",
    "\n",
    "    dbrdth = numpy.gradient(br_obs, θ, axis=0)\n",
    "    dbrdph = numpy.gradient(br_obs, φ, axis=1)\n",
    "\n",
    "    observations[\"dbrdth\"] = dbrdth\n",
    "    observations[\"dbrdph\"] = dbrdph\n",
    "\n",
    "    gradθbr = dbrdth / r\n",
    "    gradφbr = dbrdph / (r * sinθ)\n",
    "\n",
    "    br_divh_uh = br_obs * divh_uh\n",
    "    uh_dot_gradbr = uθ_obs * gradθbr + uφ_obs * gradφbr\n",
    "\n",
    "    dbrdt = -(br_divh_uh + uh_dot_gradbr)\n",
    "\n",
    "    # Overwriting the dbrdt observation\n",
    "    observations[\"dbrdt\"] = dbrdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25e3fd",
   "metadata": {},
   "source": [
    "### 2) Defining the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b108ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nb_input: int, nb_output: int, nb_layer: int, nb_neuron: int) -> None:\n",
    "        \n",
    "        # Initializing the torch module\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # Initializing the layers\n",
    "        layers = []\n",
    "\n",
    "        # Creating the first layer\n",
    "        layers.append(torch.nn.Linear(nb_input, nb_neuron))\n",
    "        layers.append(torch.nn.Tanh())\n",
    "\n",
    "        # Creating the hidden layers\n",
    "        for _ in range(nb_layer):\n",
    "            layers.append(torch.nn.Linear(nb_neuron, nb_neuron))\n",
    "            layers.append(torch.nn.Tanh())\n",
    "\n",
    "        # Creating the last layer\n",
    "        layers.append(torch.nn.Linear(nb_neuron, nb_output))\n",
    "\n",
    "        # Creating the network\n",
    "        self.net: torch.nn.Sequential = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network instance\n",
    "network_instance = Network(number_of_inputs, number_of_outputs, number_of_layers, number_of_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748c463",
   "metadata": {},
   "source": [
    "### 3) Creating the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a tensor from a numpy array\n",
    "def as_tensor(numpy_array: numpy.ndarray, requires_grad: bool = False) -> torch.Tensor:\n",
    "    return torch.tensor(numpy_array, dtype=torch.float32, requires_grad=requires_grad)\n",
    "\n",
    "# Function to create an autograd tensor from a tensor\n",
    "def as_grad(tensor: torch.Tensor, input: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.autograd.grad(tensor, input, torch.ones_like(tensor), retain_graph=True, create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting back the angles in degrees\n",
    "thetas_deg: numpy.ndarray = numpy.round(numpy.rad2deg(thetas), 3)\n",
    "phis_deg: numpy.ndarray = numpy.round(numpy.rad2deg(phis), 3)\n",
    "\n",
    "# Retrieving the corresponding indexes for the patch\n",
    "idx_theta_min = numpy.argmin(numpy.abs(thetas_deg - patch[0]))\n",
    "idx_theta_max = numpy.argmin(numpy.abs(thetas_deg - patch[1]))\n",
    "idx_phi_min = numpy.argmin(numpy.abs(phis_deg - patch[2]))\n",
    "idx_phi_max = numpy.argmin(numpy.abs(phis_deg - patch[3]))\n",
    "\n",
    "# Avoiding conflict with the slice end\n",
    "idx_theta_max = None if idx_theta_max + 1 == len(thetas_deg) else idx_theta_max + 1\n",
    "idx_phi_max = None if idx_phi_max + 1 == len(phis_deg) else idx_phi_max + 1\n",
    "\n",
    "# Defining the slices (to ease the notation afterwards)\n",
    "slice_thetas = slice(idx_theta_min, idx_theta_max, 1)\n",
    "slice_phis = slice(idx_phi_min, idx_phi_max, 1)\n",
    "\n",
    "# Creating the grid\n",
    "thetas_grid, phis_grid = numpy.meshgrid(thetas, phis, indexing=\"ij\")\n",
    "\n",
    "# Defining the tensors from the observations\n",
    "tensors = {}\n",
    "\n",
    "# Looping over all observations\n",
    "for name in observations:\n",
    "    field = observations[name][slice_thetas, slice_phis].reshape(-1, 1)\n",
    "    tensors[name] = torch.tensor(field, dtype=torch.float32)\n",
    "\n",
    "# Defining the angular grid tensor\n",
    "thetas_tensor = as_tensor(thetas_grid[slice_thetas, slice_phis].reshape(-1, 1), requires_grad=True)\n",
    "phis_tensor = as_tensor(phis_grid[slice_thetas, slice_phis].reshape(-1, 1), requires_grad=True)\n",
    "\n",
    "# Updating the list of tensors\n",
    "tensors.update({\n",
    "    \"thetas\": thetas_tensor, \"phis\": phis_tensor\n",
    "})\n",
    "\n",
    "# Defining the input tensor\n",
    "input_tensor = torch.concatenate([thetas_tensor, phis_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7518d",
   "metadata": {},
   "source": [
    "### 4)Compute loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing the loss function\n",
    "def compute_loss(input: torch.Tensor, return_fields: bool = False) -> tuple | torch.Tensor:\n",
    "\n",
    "    # Defining the outer core radius\n",
    "    r = torch.tensor(pygeo.constants[\"rCore\"], dtype=torch.float32)\n",
    "    \n",
    "    # Retrieving the predictions from the network\n",
    "    predictions = network_instance(input)\n",
    "\n",
    "    # Separating between the flow components and br\n",
    "    # Here, the flow components are toroidal and poloidal scalar fields, t and s\n",
    "    t: torch.Tensor = predictions[...,0:1]\n",
    "    s: torch.Tensor = predictions[...,1:2]\n",
    "    br: torch.Tensor = predictions[...,2:3]\n",
    "\n",
    "    # Retrieving the observations\n",
    "    dbrdt_obs: torch.Tensor = tensors[\"dbrdt\"]\n",
    "    br_obs: torch.Tensor = tensors[\"br\"]\n",
    "\n",
    "    # Computing the derivatives of the observed Br (in we are estimating the small-scales)\n",
    "    if estimate_small_scale_br:\n",
    "        dbrdθ_obs = tensors[\"dbrdth\"]\n",
    "        dbrdφ_obs = tensors[\"dbrdph\"]\n",
    "\n",
    "    # Retrieving angles (ensuring gradient calculations)\n",
    "    θ: torch.Tensor = tensors[\"thetas\"]\n",
    "    φ: torch.Tensor = tensors[\"phis\"]\n",
    "\n",
    "    # Defining useful quantities\n",
    "    sinθ: torch.Tensor = torch.sin(θ).clamp(1e-1, 1)\n",
    "    cosθ: torch.Tensor = torch.cos(θ)\n",
    "\n",
    "    # Defining the derivatives of t and s\n",
    "    dtdθ: torch.Tensor = as_grad(t, θ)\n",
    "    dtdφ: torch.Tensor = as_grad(t, φ)\n",
    "    dsdθ: torch.Tensor = as_grad(s, θ)\n",
    "    dsdφ: torch.Tensor = as_grad(s, φ)\n",
    "\n",
    "    # Retrieving uθ and uφ\n",
    "    uθ: torch.Tensor = (1 / sinθ) * dtdφ + dsdθ\n",
    "    uφ: torch.Tensor = -dtdθ + (1 / sinθ) * dsdφ\n",
    "\n",
    "    # 1) Computing the SV\n",
    "\n",
    "    # Defining the equation of induction\n",
    "    # We have: dbr / dt = -divh(uh br) = -br divh(uh) - uh ⋅  gradh(br)\n",
    "\n",
    "    # Computing the derivatives of uθ and uφ\n",
    "    duθdθ: torch.Tensor = as_grad(uθ, θ)\n",
    "    duθdφ: torch.Tensor = as_grad(uθ, φ)\n",
    "    duφdθ: torch.Tensor = as_grad(uφ, θ)\n",
    "    duφdφ: torch.Tensor = as_grad(uφ, φ)\n",
    "\n",
    "    # Computing the derivatives of br\n",
    "    dbrdθ: torch.Tensor = as_grad(br, θ)\n",
    "    dbrdφ: torch.Tensor = as_grad(br, φ)\n",
    "\n",
    "    # Computing divh(uh)\n",
    "    divh_uh: torch.Tensor = (1 / (r * sinθ)) * (duθdθ * sinθ + uθ * cosθ + duφdφ)\n",
    "\n",
    "    # Computing gradh(br)\n",
    "    if estimate_small_scale_br:\n",
    "        gradθ_br: torch.Tensor = (1 / r) * (dbrdθ + dbrdθ_obs)\n",
    "        gradφ_br: torch.Tensor = (1 / (r * sinθ)) * (dbrdφ + dbrdφ_obs)\n",
    "\n",
    "        dbrdt = -((br + br_obs) * divh_uh + gradθ_br * uθ + gradφ_br * uφ)\n",
    "    else:\n",
    "        gradθ_br: torch.Tensor = (1 / r) * dbrdθ\n",
    "        gradφ_br: torch.Tensor = (1 / (r * sinθ)) * dbrdφ\n",
    "\n",
    "        dbrdt = -(br * divh_uh + gradθ_br * uθ + gradφ_br * uφ)\n",
    "\n",
    "    # 2) Computing the loss functions\n",
    "\n",
    "    # Defining the total loss function\n",
    "    total_loss: torch.Tensor = torch.tensor(0, dtype=torch.float32)\n",
    "\n",
    "    if \"misfit_dbrdt\" in losses:\n",
    "        misfit_dbrdt: torch.Tensor = (dbrdt - dbrdt_obs).pow(2).mean() / dbrdt_obs.pow(2).mean()\n",
    "        loss = losses[\"misfit_dbrdt\"] * misfit_dbrdt\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"misfit_dbrdt\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "    if \"misfit_br\" in losses:\n",
    "        misfit_br: torch.Tensor = (br - br_obs).pow(2).mean() / br_obs.pow(2).mean()\n",
    "        loss = losses[\"misfit_br\"] * misfit_br\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"misfit_br\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "    if \"tangential_geostrophy\" in losses:\n",
    "        tangential_geostrophy: torch.Tensor = (cosθ * divh_uh - sinθ * uθ / r).pow(2).mean()\n",
    "        loss = losses[\"tangential_geostrophy\"] * tangential_geostrophy\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"tangential_geostrophy\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "    if \"toroidal\" in losses:\n",
    "        toroidal: torch.Tensor = divh_uh.pow(2).mean()\n",
    "        loss = losses[\"toroidal\"] * toroidal\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"toroidal\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "    if \"weak_norm\" in losses:\n",
    "        dθ = numpy.gradient(thetas).mean()\n",
    "        dφ = numpy.gradient(phis).mean()\n",
    "        dΩ = sinθ * dθ * dφ\n",
    "\n",
    "        weak_norm: torch.Tensor = torch.sum((uθ**2 + uφ**2) * dΩ) / torch.sum(dΩ)\n",
    "\n",
    "        loss = losses[\"weak_norm\"] * weak_norm\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"weak_norm\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "    if \"strong_norm\" in losses:\n",
    "        dθ = numpy.gradient(thetas).mean()\n",
    "        dφ = numpy.gradient(phis).mean()\n",
    "        dΩ = sinθ * dθ * dφ\n",
    "\n",
    "        d2uθdθ: torch.Tensor = as_grad(duθdθ, θ)\n",
    "        d2uθdφ: torch.Tensor = as_grad(duθdφ, φ)\n",
    "        d2uφdθ: torch.Tensor = as_grad(duφdθ, θ)\n",
    "        d2uφdφ: torch.Tensor = as_grad(duφdφ, φ)\n",
    "\n",
    "        Δuθ = (1 / (r**2 * sinθ)) * (cosθ * duθdθ + sinθ * d2uθdθ) + (1 / (r * sinθ)**2) * d2uθdφ\n",
    "        Δuφ = (1 / (r**2 * sinθ)) * (cosθ * duφdθ + sinθ * d2uφdθ) + (1 / (r * sinθ)**2) * d2uφdφ\n",
    "\n",
    "        strong_norm: torch.Tensor = torch.sum((Δuθ**2 + Δuφ**2) * dΩ) / torch.sum(dΩ)\n",
    "\n",
    "        loss = losses[\"strong_norm\"] * strong_norm\n",
    "        total_loss += loss\n",
    "\n",
    "        saved_losses[\"strong_norm\"].append(loss.clone().detach().numpy())\n",
    "\n",
    "\n",
    "    # 3) Returning the results\n",
    "\n",
    "    if return_fields:\n",
    "        return dbrdt.clone().detach().numpy(), \\\n",
    "            uθ.clone().detach().numpy(), uφ.clone().detach().numpy(), br.clone().detach().numpy()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60deb9a",
   "metadata": {},
   "source": [
    "### 5) Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the losses\n",
    "saved_losses = {name: [] for name in losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the optimizer\n",
    "optimizer = torch.optim.Adam(network_instance.parameters())\n",
    "\n",
    "# Initializing the layers\n",
    "with torch.no_grad():\n",
    "    outputs_layer: torch.nn.Linear = network_instance.net[-1]\n",
    "    torch.nn.init.xavier_normal_(outputs_layer.weight)\n",
    "    outputs_layer.weight[2].data.fill_(1e5)\n",
    "\n",
    "# The lower loss so far\n",
    "lower_loss = 1e99\n",
    "\n",
    "# Starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Creating the bar format for the progress bar\n",
    "leftmost_bar = \"\\r{desc}: {percentage:3.0f}% | \"\n",
    "rightmost_bar = \" | {n_fmt}/{total_fmt} [{remaining} remaining, {rate_fmt}] ; Loss = {postfix[0]:.2E}\"\n",
    "center_bar: str = \"{bar:10}\"\n",
    "\n",
    "network_instance.train()\n",
    "\n",
    "# Starting the training\n",
    "with tqdm.tqdm(total=number_epochs, bar_format=f\"{leftmost_bar}{center_bar}{rightmost_bar}\", postfix=[lower_loss]) as progress:\n",
    "\n",
    "    # Looping over all epochs\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(input_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # If the current loss is lower, we are saving the model\n",
    "        if loss < lower_loss:\n",
    "            lower_loss = float(loss.detach().clone())\n",
    "            progress.postfix[0] = lower_loss\n",
    "            torch.save(network_instance.state_dict(), \"best_model.pt\")\n",
    "\n",
    "        # Updating the progress bar\n",
    "        progress.update()\n",
    "\n",
    "# Ending time\n",
    "end_time = time.time()\n",
    "\n",
    "# At the end of the training, we are retrieving the best model\n",
    "network_instance.load_state_dict(torch.load(\"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50656a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some fine-tuning with L-BFGS\n",
    "if fine_tune:\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(network_instance.parameters())\n",
    "\n",
    "    # Defining the closure function (mandatory for L-BFGS)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(input_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Starting the training\n",
    "    with tqdm.tqdm(total=number_steps, bar_format=f\"{leftmost_bar}{center_bar}{rightmost_bar}\", postfix=[lower_loss]) as progress:\n",
    "\n",
    "        # Reload the best model\n",
    "        network_instance.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "        # Looping over all epochs\n",
    "        for epoch in range(number_steps):\n",
    "\n",
    "            # Optimization step\n",
    "            loss = optimizer.step(closure)\n",
    "            \n",
    "            # If the current loss is lower, we are saving the model\n",
    "            if loss < lower_loss:\n",
    "                lower_loss = float(loss.detach().clone())\n",
    "                progress.postfix[0] = lower_loss\n",
    "                torch.save(network_instance.state_dict(), \"best_model.pt\")\n",
    "\n",
    "            # Updating the progress bar\n",
    "            progress.update()\n",
    "\n",
    "# At the end of the training, we are retrieving the best model\n",
    "network_instance.load_state_dict(torch.load(\"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4368cd",
   "metadata": {},
   "source": [
    "### 6) Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f97c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The training took {end_time - start_time} seconds.\")\n",
    "print(f\"The best model has a loss = {lower_loss:.2E}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144a98f",
   "metadata": {},
   "source": [
    "### 7) Retrieving the predicted fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the observations\n",
    "dbrdt_obs = observations[\"dbrdt\"][slice_thetas, slice_phis]\n",
    "uθ_obs = observations[\"uth\"][slice_thetas, slice_phis]\n",
    "uφ_obs = observations[\"uph\"][slice_thetas, slice_phis]\n",
    "br_obs = observations[\"br\"][slice_thetas, slice_phis]\n",
    "\n",
    "network_instance.eval()\n",
    "\n",
    "# Retrieving the predictions\n",
    "dbrdt_tensor_pred, \\\n",
    "uθ_tensor_pred, \\\n",
    "uφ_tensor_pred, \\\n",
    "br_tensor_pred = compute_loss(input_tensor, True)\n",
    "\n",
    "# Reshaping the fields\n",
    "dbrdt_pred = dbrdt_tensor_pred.reshape(dbrdt_obs.shape)\n",
    "uθ_pred = uθ_tensor_pred.reshape(uθ_obs.shape)\n",
    "uφ_pred = uφ_tensor_pred.reshape(uθ_obs.shape)\n",
    "br_pred = br_tensor_pred.reshape(uθ_obs.shape)\n",
    "\n",
    "# Saving\n",
    "predictions = {\n",
    "    \"dbrdt\": dbrdt_pred, \"uth\": uθ_pred, \"uph\": uφ_pred, \"br\": br_pred\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a714ec3",
   "metadata": {},
   "source": [
    "### 8) Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "base_out = r\"E:\\INTERNSHIP\\PINN\\Code\\pygeopinn-dev\\Outputs\"\n",
    "\n",
    "i = 1\n",
    "i=i+1\n",
    "\n",
    "\n",
    "folder = os.path.join(base_out, f\"{i}_Real_SV_{timestamp}\")\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "print(\"Saving everything to:\", folder)\n",
    "\n",
    "fig_dir   = os.path.join(folder, \"figures\")\n",
    "ckpt_dir  = os.path.join(folder, \"checkpoints\")\n",
    "logs_dir  = os.path.join(folder, \"logs\")\n",
    "\n",
    "for d in (fig_dir, ckpt_dir, logs_dir):\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the quantity we want to map\n",
    "name = \"uth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0030932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid in longitudes and latitudes\n",
    "latitudes = pygeo.convertThetasToLatitudes(thetas)\n",
    "longitudes = pygeo.convertPhisToLongitudes(phis)\n",
    "\n",
    "lat_grid_full, long_grid_full = numpy.meshgrid(latitudes, longitudes, indexing=\"ij\")\n",
    "\n",
    "lat_grid = lat_grid_full[slice_thetas, slice_phis]\n",
    "long_grid = long_grid_full[slice_thetas, slice_phis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titles, suptitles and units\n",
    "titles = [\"Observation\", \"Prediction\", \"Difference\"]\n",
    "units = {\"dbrdt\": \"nT / yr\", \"br\": \"nT\", \"uth\": \"km / yr\", \"uph\": \"km / yr\"}\n",
    "suptitles = {\"dbrdt\": \"Secular variation\", \"br\": \"Magnetic field\", \"uth\": \"Azimuthal flow $u_\\\\theta$\", \"uph\": \"Polar flow $u_\\\\phi$\"}\n",
    "\n",
    "for name in [\"dbrdt\", \"br\", \"uth\", \"uph\"]:\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, subplot_kw={'projection': ccrs.PlateCarree()}, sharex=True, sharey=True, figsize=(10, 4), dpi=150)\n",
    "\n",
    "    obs = observations[name][slice_thetas, slice_phis]\n",
    "    pred = predictions[name]\n",
    "\n",
    "    # Maximum value took by the observations\n",
    "    vmax = numpy.max(numpy.abs(obs))\n",
    "\n",
    "    # Arguments for the plot\n",
    "    kwargs = {\"cmap\": cmocean.cm.balance, \"vmin\": -vmax, \"vmax\": vmax, \"s\": 15, \"marker\": \"o\"}\n",
    "\n",
    "    cb1 = axes[0].scatter(long_grid, lat_grid, c=obs, **kwargs)\n",
    "    cb2 = axes[1].scatter(long_grid, lat_grid, c=pred, **kwargs)\n",
    "    cb3 = axes[2].scatter(long_grid, lat_grid, c=obs - pred, **kwargs)\n",
    "\n",
    "    axes[0].contour(long_grid, lat_grid, obs, colors=['black'], levels=10, linewidths=1, linestyles=\"solid\")\n",
    "    axes[1].contour(long_grid, lat_grid, pred, colors=['black'], levels=10, linewidths=1, linestyles=\"solid\")\n",
    "\n",
    "    for n, ax in enumerate(axes):\n",
    "        ax.coastlines(resolution='110m')\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, alpha=0.3)\n",
    "        ax.set_title(titles[n])\n",
    "\n",
    "    cbar = plt.colorbar(cb1, orientation=\"horizontal\", shrink=0.5, extend=\"both\")\n",
    "    cbar2 = plt.colorbar(cb2, orientation=\"horizontal\", shrink=0.5, extend=\"both\")\n",
    "    cbar3 = plt.colorbar(cb3, orientation=\"horizontal\", shrink=0.5, extend=\"both\")\n",
    "\n",
    "    cbar.set_label(units[name])\n",
    "    cbar2.set_label(units[name])\n",
    "    cbar3.set_label(units[name])\n",
    "\n",
    "    plt.suptitle(suptitles[name])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(fig_dir, f\"{name}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99813e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = [\"dbrdt\", \"br\", \"uth\", \"uph\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(maps), subplot_kw={'projection': ccrs.PlateCarree()}, sharex=True, sharey=True, figsize=(15, 4), dpi=150)\n",
    "\n",
    "for n, map in enumerate(maps):\n",
    "    pred = predictions[map]\n",
    "\n",
    "    # Maximum value took by the observations\n",
    "    vmax = numpy.max(numpy.abs(pred))\n",
    "\n",
    "    # Arguments for the plot\n",
    "    kwargs = {\"cmap\": cmocean.cm.balance, \"vmin\": -vmax, \"vmax\": vmax, \"s\": 15, \"marker\": \"o\"}\n",
    "\n",
    "    cb = axes[n].scatter(long_grid, lat_grid, c=pred, **kwargs)\n",
    "\n",
    "    axes[n].contour(long_grid, lat_grid, pred, colors=['black'], levels=10, linewidths=1, linestyles=\"solid\")\n",
    "\n",
    "    axes[n].coastlines(resolution='110m')\n",
    "    axes[n].add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    axes[n].gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, alpha=0.3)\n",
    "    axes[n].set_title(suptitles[map])\n",
    "\n",
    "    cbar = plt.colorbar(cb, orientation=\"horizontal\", shrink=0.5, extend=\"both\")\n",
    "\n",
    "    cbar.set_label(units[map])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(fig_dir, f\"{name}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29ce05",
   "metadata": {},
   "source": [
    "### 9) Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"misfit_dbrdt\": \"Misfit to the SV\",\n",
    "    \"misfit_br\": \"Misfit to the MF\",\n",
    "    \"tangential_geostrophy\": \"Tangential geostrophy constraint\",\n",
    "    \"toroidal\": \"Toroidality constraint\",\n",
    "    \"weak_norm\": \"Weak-norm regularization\",\n",
    "    \"strong_norm\": \"Strong-norm regularization\",\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=150)\n",
    "\n",
    "for name in saved_losses:\n",
    "    ax.loglog(saved_losses[name], label=labels[name])\n",
    "\n",
    "ax.grid(alpha=0.2)\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(os.path.join(fig_dir, \"losses.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134c935",
   "metadata": {},
   "source": [
    "### 10) Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"uth\"\n",
    "\n",
    "obs = observations[name][slice_thetas, slice_phis]\n",
    "pred = predictions[name]\n",
    "Δ = numpy.abs(obs - pred)\n",
    "\n",
    "rmse = numpy.sqrt(numpy.mean(Δ**2))\n",
    "re_max = 100 * Δ.mean() / numpy.abs(obs).max()\n",
    "\n",
    "print(f\"RMSE = {rmse:.2E} {units[name]}\")\n",
    "print(f\"Relative error (% of max) = {re_max:.2E}\")\n",
    "\n",
    "print(f\"{name}\\t{rmse:.2E} {units[name]}\\t{round(re_max, 2)} % of max.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "uθ_pred, uφ_pred = predictions[\"uth\"], predictions[\"uph\"]\n",
    "uθ_obs, uφ_obs = observations[\"uth\"][slice_thetas, slice_phis], observations[\"uph\"][slice_thetas, slice_phis]\n",
    "\n",
    "dθ = numpy.gradient(thetas).mean()\n",
    "dφ = numpy.gradient(phis).mean()\n",
    "\n",
    "sinθ = numpy.sin(thetas_grid[slice_thetas, slice_phis])\n",
    "\n",
    "dΩ = sinθ * dθ * dφ\n",
    "\n",
    "Corr_obs_obs = numpy.sum((uθ_obs**2 + uφ_obs**2) * dΩ)\n",
    "Corr_pred_pred = numpy.sum((uθ_pred**2 + uφ_pred**2) * dΩ)\n",
    "Corr_obs_pred = numpy.sum((uθ_obs * uθ_pred + uφ_obs * uφ_pred) * dΩ)\n",
    "\n",
    "Corr = Corr_obs_pred / numpy.sqrt(Corr_obs_obs * Corr_pred_pred)\n",
    "\n",
    "print(f\"{round(Corr, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the complexity on the observed field\n",
    "uθ = predictions[\"uth\"]\n",
    "uφ = predictions[\"uph\"]\n",
    "\n",
    "θ = thetas[slice_thetas]\n",
    "φ = phis[slice_phis]\n",
    "\n",
    "sinθ = numpy.sin(thetas_grid[slice_thetas, slice_phis]).clip(1e-2, 1)\n",
    "cosθ = numpy.cos(thetas_grid[slice_thetas, slice_phis])\n",
    "\n",
    "dΩ = sinθ * dθ * dφ\n",
    "\n",
    "duθdθ = numpy.gradient(uθ, θ, axis=0)\n",
    "duθdφ = numpy.gradient(uθ, φ, axis=1)\n",
    "duφdθ = numpy.gradient(uφ, θ, axis=0)\n",
    "duφdφ = numpy.gradient(uφ, φ, axis=1)\n",
    "\n",
    "d2uθdθ: torch.Tensor = numpy.gradient(duθdθ, θ, axis=0)\n",
    "d2uθdφ: torch.Tensor = numpy.gradient(duθdφ, φ, axis=1)\n",
    "d2uφdθ: torch.Tensor = numpy.gradient(duφdθ, θ, axis=0)\n",
    "d2uφdφ: torch.Tensor = numpy.gradient(duφdφ, φ, axis=1)\n",
    "\n",
    "Δuθ = (1 / (r**2 * sinθ)) * (cosθ * duθdθ + sinθ * d2uθdθ) + (1 / (r * sinθ)**2) * d2uθdφ\n",
    "Δuφ = (1 / (r**2 * sinθ)) * (cosθ * duφdθ + sinθ * d2uφdθ) + (1 / (r * sinθ)**2) * d2uφdφ\n",
    "\n",
    "area = numpy.sum(dΩ)\n",
    "\n",
    "divh_uh= (1 / (r * sinθ)) * (duθdθ * sinθ + uθ * cosθ + duφdφ)\n",
    "\n",
    "weak_norm = numpy.sum((uθ**2 + uφ**2) * dΩ) / area\n",
    "strong_norm = numpy.sum((Δuθ**2 + Δuφ**2) * dΩ) / area\n",
    "tangential_geostrophy = cosθ * divh_uh - uθ * sinθ / r\n",
    "toroidality = divh_uh\n",
    "\n",
    "# Misfit\n",
    "dbrdt_obs = observations[\"dbrdt\"][slice_thetas, slice_phis]\n",
    "dbrdt_pred = predictions[\"dbrdt\"]\n",
    "L1 = ((dbrdt_obs - dbrdt_pred)**2).mean() / (dbrdt_obs**2).mean()\n",
    "\n",
    "print(f\"Correlation: {Corr:.2E}\")\n",
    "print(f\"Weak norm: {weak_norm:.2E}\")\n",
    "print(f\"Strong norm: {strong_norm:.2E}\")\n",
    "print(\"---\")\n",
    "print(f\"Tangential geostrophy: {tangential_geostrophy.mean():.2E}\")\n",
    "print(f\"Toroidality: {toroidality.mean():.2E}\")\n",
    "print(\"---\")\n",
    "print(f\"Misfit: {L1:.2E}\")\n",
    "\n",
    "print(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
